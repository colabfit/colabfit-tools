{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from itertools import chain, islice\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import dateutil.parser\n",
    "import findspark\n",
    "import lmdb\n",
    "import numpy as np\n",
    "import pyspark\n",
    "from ase.atoms import Atoms\n",
    "\n",
    "from functools import partial\n",
    "from ase.io.cfg import read_cfg\n",
    "from dotenv import load_dotenv\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    BooleanType,\n",
    "    DoubleType,\n",
    "    FloatType,\n",
    "    IntegerType,\n",
    "    LongType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    "    TimestampType,\n",
    ")\n",
    "\n",
    "from colabfit.tools.configuration import AtomicConfiguration\n",
    "\n",
    "from colabfit.tools.database import DataManager, PGDataLoader\n",
    "from colabfit.tools.property import Property\n",
    "from colabfit.tools.property_definitions import (\n",
    "    atomic_forces_pd,\n",
    "    cauchy_stress_pd,\n",
    "    potential_energy_pd,\n",
    ")\n",
    "\n",
    "findspark.init()\n",
    "format = \"jdbc\"\n",
    "load_dotenv(\"./.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up MTPU and Carolina Materials readers and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_stress(keys, stress):\n",
    "    stresses = {k: s for k, s in zip(keys, stress)}\n",
    "    return [\n",
    "        [stresses[\"xx\"], stresses[\"xy\"], stresses[\"xz\"]],\n",
    "        [stresses[\"xy\"], stresses[\"yy\"], stresses[\"yz\"]],\n",
    "        [stresses[\"xz\"], stresses[\"yz\"], stresses[\"zz\"]],\n",
    "    ]\n",
    "\n",
    "\n",
    "SYMBOL_DICT = {\"0\": \"Si\", \"1\": \"O\"}\n",
    "\n",
    "\n",
    "def reader(filepath):\n",
    "    with open(filepath, \"rt\") as f:\n",
    "        energy = None\n",
    "        forces = None\n",
    "        coords = []\n",
    "        cell = []\n",
    "        symbols = []\n",
    "        config_count = 0\n",
    "        for line in f:\n",
    "            if line.strip().startswith(\"Size\"):\n",
    "                size = int(f.readline().strip())\n",
    "            elif line.strip().lower().startswith(\"supercell\"):\n",
    "                cell.append([float(x) for x in f.readline().strip().split()])\n",
    "                cell.append([float(x) for x in f.readline().strip().split()])\n",
    "                cell.append([float(x) for x in f.readline().strip().split()])\n",
    "            elif line.strip().startswith(\"Energy\"):\n",
    "                energy = float(f.readline().strip())\n",
    "            elif line.strip().startswith(\"PlusStress\"):\n",
    "                stress_keys = line.strip().split()[-6:]\n",
    "                stress = [float(x) for x in f.readline().strip().split()]\n",
    "                stress = convert_stress(stress_keys, stress)\n",
    "            elif line.strip().startswith(\"AtomData:\"):\n",
    "                keys = line.strip().split()[1:]\n",
    "                if \"fx\" in keys:\n",
    "                    forces = []\n",
    "                for i in range(size):\n",
    "                    li = {\n",
    "                        key: val for key, val in zip(keys, f.readline().strip().split())\n",
    "                    }\n",
    "                    symbols.append(SYMBOL_DICT[li[\"type\"]])\n",
    "                    if \"cartes_x\" in keys:\n",
    "                        coords.append(\n",
    "                            [\n",
    "                                float(c)\n",
    "                                for c in [\n",
    "                                    li[\"cartes_x\"],\n",
    "                                    li[\"cartes_y\"],\n",
    "                                    li[\"cartes_z\"],\n",
    "                                ]\n",
    "                            ]\n",
    "                        )\n",
    "                    elif \"direct_x\" in keys:\n",
    "                        coords.append(\n",
    "                            [\n",
    "                                float(c)\n",
    "                                for c in [\n",
    "                                    li[\"direct_x\"],\n",
    "                                    li[\"direct_y\"],\n",
    "                                    li[\"direct_z\"],\n",
    "                                ]\n",
    "                            ]\n",
    "                        )\n",
    "\n",
    "                    if \"fx\" in keys:\n",
    "                        forces.append(\n",
    "                            [float(f) for f in [li[\"fx\"], li[\"fy\"], li[\"fz\"]]]\n",
    "                        )\n",
    "\n",
    "            elif line.startswith(\"END_CFG\"):\n",
    "                if \"cartes_x\" in keys:\n",
    "                    config = AtomicConfiguration(\n",
    "                        positions=coords, symbols=symbols, cell=cell\n",
    "                    )\n",
    "                elif \"direct_x\" in keys:\n",
    "                    config = AtomicConfiguration(\n",
    "                        scaled_positions=coords, symbols=symbols, cell=cell\n",
    "                    )\n",
    "                config.info[\"energy\"] = energy\n",
    "                if forces:\n",
    "                    config.info[\"forces\"] = forces\n",
    "                config.info[\"stress\"] = stress\n",
    "\n",
    "                if \"Si\" in symbols and \"O\" in symbols:\n",
    "                    config.info[\"input\"] = {\n",
    "                        \"kpoint-scheme\": \"Monkhorst-Pack\",\n",
    "                        \"kpoints\": \"11x11x11\",\n",
    "                        \"kinetic-energy-cutoff\": {\n",
    "                            \"val\": 1224,\n",
    "                            \"units\": \"eV\",\n",
    "                        },\n",
    "                    }\n",
    "                    config.info[\"_name\"] = f\"{filepath.stem}_SiO2_{config_count}\"\n",
    "                elif \"Si\" in symbols:\n",
    "                    config.info[\"input\"] = {\n",
    "                        \"kpoint-scheme\": \"Monkhorst-Pack\",\n",
    "                        \"kpoints\": \"8x8x8\",\n",
    "                        \"kinetic-energy-cutoff\": {\n",
    "                            \"val\": 884,\n",
    "                            \"units\": \"eV\",\n",
    "                        },\n",
    "                    }\n",
    "                    config.info[\"_name\"] = f\"{filepath.stem}_Si_{config_count}\"\n",
    "                elif \"O\" in symbols:\n",
    "                    config.info[\"input\"] = {\n",
    "                        \"kpoint-scheme\": \"Monkhorst-Pack\",\n",
    "                        \"kpoints\": \"gamma-point\",\n",
    "                        \"kinetic-energy-cutoff\": {\n",
    "                            \"val\": 1224,\n",
    "                            \"units\": \"eV\",\n",
    "                        },\n",
    "                    }\n",
    "                    config.info[\"_name\"] = f\"{filepath.stem}_O_{config_count}\"\n",
    "                config_count += 1\n",
    "                yield config\n",
    "                forces = None\n",
    "                stress = []\n",
    "                coords = []\n",
    "                cell = []\n",
    "                symbols = []\n",
    "                energy = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nsites': 4,\n",
       " 'elements': ['Si'],\n",
       " 'nelements': 1,\n",
       " 'elements_ratios': [1.0],\n",
       " 'chemical_formula_anonymous': 'A',\n",
       " 'chemical_formula_reduced': 'Si',\n",
       " 'chemical_formula_hill': 'Si4',\n",
       " 'dimension_types': [0, 0, 0],\n",
       " 'nperiodic_dimensions': 0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtpu_configs = reader(Path(\"data/mtpu_2023/Unified_training_set.cfg\"))\n",
    "data = [x for x in mtpu_configs]\n",
    "data[0].configuration_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOFTWARE = \"VASP\"\n",
    "METHODS = \"DFT-PBE\"\n",
    "CM_PI_METADATA = {\n",
    "    \"software\": {\"value\": SOFTWARE},\n",
    "    \"method\": {\"value\": METHODS},\n",
    "    \"input\": {\"value\": {\"IBRION\": 6, \"NFREE\": 4}},\n",
    "}\n",
    "\n",
    "CM_PROPERTY_MAP = {\n",
    "    \"formation-energy\": [\n",
    "        {\n",
    "            \"energy\": {\"field\": \"energy\", \"units\": \"eV\"},\n",
    "            \"per-atom\": {\"value\": False, \"units\": None},\n",
    "        }\n",
    "    ],\n",
    "    \"_metadata\": CM_PI_METADATA,\n",
    "}\n",
    "CO_MD = {\n",
    "    key: {\"field\": key}\n",
    "    for key in [\n",
    "        \"_symmetry_space_group_name_H-M\",\n",
    "        \"_symmetry_Int_Tables_number\",\n",
    "        \"_chemical_formula_structural\",\n",
    "        \"_chemical_formula_sum\",\n",
    "        \"_cell_volume\",\n",
    "        \"_cell_formula_units_Z\",\n",
    "        \"symmetry_dict\",\n",
    "        \"formula_pretty\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "def load_row(txn, row):\n",
    "    try:\n",
    "        data = pickle.loads(txn.get(f\"{row}\".encode(\"ascii\")))\n",
    "        return data\n",
    "    except TypeError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def config_from_row(row: dict, row_num: int):\n",
    "    coords = row.pop(\"cart_coords\")\n",
    "    a_num = row.pop(\"atomic_numbers\")\n",
    "    cell = [\n",
    "        row.pop(x)\n",
    "        for x in [\n",
    "            \"_cell_length_a\",\n",
    "            \"_cell_length_b\",\n",
    "            \"_cell_length_c\",\n",
    "            \"_cell_angle_alpha\",\n",
    "            \"_cell_angle_beta\",\n",
    "            \"_cell_angle_gamma\",\n",
    "        ]\n",
    "    ]\n",
    "    config = Atoms(scaled_positions=coords, numbers=a_num, cell=cell)\n",
    "    symmetry_dict = {str(key): val for key, val in row.pop(\"symmetry_dict\").items()}\n",
    "    for key in symmetry_dict:\n",
    "        key = str(key)\n",
    "    config.info = row\n",
    "    config.info[\"symmetry_dict\"] = symmetry_dict\n",
    "    config.info[\"name\"] = f\"carolina_materials_{row_num}\"\n",
    "    return config\n",
    "\n",
    "\n",
    "def carmat_reader(fp: Path):\n",
    "    parent = fp.parent\n",
    "    env = lmdb.open(str(parent))\n",
    "    txn = env.begin()\n",
    "    row_num = 0\n",
    "    while row_num <= 10000:\n",
    "        row = load_row(txn, row_num)\n",
    "        if row is False:\n",
    "            env.close()\n",
    "            break\n",
    "        yield config_from_row(row, row_num)\n",
    "        row_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "carmat_configs = list(carmat_reader(Path(\"data/carolina_matdb/base/all/data.mdb\")))\n",
    "PI_METADATA = {\n",
    "    \"software\": {\"value\": \"Quantum ESPRESSO\"},\n",
    "    \"method\": {\"value\": \"DFT-PBE\"},\n",
    "    \"input\": {\"field\": \"input\"},\n",
    "}\n",
    "\n",
    "PROPERTY_MAP = {\n",
    "    \"potential-energy\": [\n",
    "        {\n",
    "            \"energy\": {\"field\": \"energy\", \"units\": \"eV\"},\n",
    "            \"per-atom\": {\"value\": False, \"units\": None},\n",
    "            # \"_metadata\": PI_METADATA,\n",
    "        }\n",
    "    ],\n",
    "    \"atomic-forces\": [\n",
    "        {\n",
    "            \"forces\": {\"field\": \"forces\", \"units\": \"eV/angstrom\"},\n",
    "            # \"_metadata\": PI_METADATA,\n",
    "        },\n",
    "    ],\n",
    "    \"cauchy-stress\": [\n",
    "        {\n",
    "            \"stress\": {\"field\": \"stress\", \"units\": \"GPa\"},\n",
    "            \"volume-normalized\": {\"value\": True, \"units\": None},\n",
    "        }\n",
    "    ],\n",
    "    \"_metadata\": PI_METADATA,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100001"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carmat_aconfigs = [\n",
    "    AtomicConfiguration(names=f\"{i}\").from_ase(c) for i, c in enumerate(carmat_configs)\n",
    "]\n",
    "for c in carmat_aconfigs:\n",
    "    c.set_metadata(CO_MD)\n",
    "len(carmat_aconfigs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions and schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_schema = StructType(\n",
    "    [\n",
    "        StructField(\"id\", StringType(), False),\n",
    "        StructField(\"hash\", StringType(), False),\n",
    "        StructField(\"last_modified\", TimestampType(), False),\n",
    "        StructField(\"dataset_ids\", StringType(), True),  # ArrayType(StringType())\n",
    "        StructField(\"metadata\", StringType(), True),\n",
    "        StructField(\"chemical_formula_hill\", StringType(), True),\n",
    "        StructField(\"chemical_formula_reduced\", StringType(), True),\n",
    "        StructField(\"chemical_formula_anonymous\", StringType(), True),\n",
    "        StructField(\"elements\", StringType(), True),  # ArrayType(StringType())\n",
    "        StructField(\"elements_ratios\", StringType(), True),  # ArrayType(IntegerType())\n",
    "        StructField(\"atomic_numbers\", StringType(), True),  # ArrayType(IntegerType())\n",
    "        StructField(\"nsites\", IntegerType(), True),\n",
    "        StructField(\"nelements\", IntegerType(), True),\n",
    "        StructField(\"nperiodic_dimensions\", IntegerType(), True),\n",
    "        StructField(\"cell\", StringType(), True),  # ArrayType(ArrayType(DoubleType()))\n",
    "        StructField(\"dimension_types\", StringType(), True),  # ArrayType(IntegerType())\n",
    "        StructField(\"pbc\", StringType(), True),  # ArrayType(IntegerType())\n",
    "        StructField(\n",
    "            \"positions\", StringType(), True\n",
    "        ),  # ArrayType(ArrayType(DoubleType()))\n",
    "        StructField(\"names\", StringType(), True),  # ArrayType(StringType())\n",
    "    ]\n",
    ")\n",
    "property_object_schema = StructType(\n",
    "    [\n",
    "        StructField(\"id\", StringType(), False),\n",
    "        StructField(\"hash\", StringType(), False),\n",
    "        StructField(\"last_modified\", TimestampType(), False),\n",
    "        StructField(\"configuration_ids\", StringType(), True),  # ArrayType(StringType())\n",
    "        StructField(\"dataset_ids\", StringType(), True),  # ArrayType(StringType())\n",
    "        StructField(\"metadata\", StringType(), True),\n",
    "        StructField(\"chemical_formula_hill\", StringType(), True),\n",
    "        StructField(\"potential_energy\", DoubleType(), True),\n",
    "        StructField(\"potential_energy_unit\", StringType(), True),\n",
    "        StructField(\"potential_energy_per_atom\", BooleanType(), True),\n",
    "        StructField(\"potential_energy_reference\", DoubleType(), True),\n",
    "        StructField(\"potential_energy_reference_unit\", StringType(), True),\n",
    "        StructField(\"potential_energy_property_id\", StringType(), True),\n",
    "        StructField(\n",
    "            \"atomic_forces\", StringType(), True\n",
    "        ),  # ArrayType(ArrayType(DoubleType()))\n",
    "        StructField(\"atomic_forces_unit\", StringType(), True),\n",
    "        StructField(\"atomic_forces_property_id\", StringType(), True),\n",
    "        StructField(\n",
    "            \"cauchy_stress\", StringType(), True\n",
    "        ),  # ArrayType(ArrayType(DoubleType()))\n",
    "        StructField(\"cauchy_stress_unit\", StringType(), True),\n",
    "        StructField(\"cauchy_stress_volume_normalized\", BooleanType(), True),\n",
    "        StructField(\"cauchy_stress_property_id\", StringType(), True),\n",
    "        StructField(\"free_energy\", DoubleType(), True),\n",
    "        StructField(\"free_energy_unit\", StringType(), True),\n",
    "        StructField(\"free_energy_per_atom\", BooleanType(), True),\n",
    "        StructField(\"free_energy_reference\", DoubleType(), True),\n",
    "        StructField(\"free_energy_reference_unit\", StringType(), True),\n",
    "        StructField(\"free_energy_property_id\", StringType(), True),\n",
    "        StructField(\"band_gap\", DoubleType(), True),\n",
    "        StructField(\"band_gap_unit\", StringType(), True),\n",
    "        StructField(\"band_gap_property_id\", StringType(), True),\n",
    "        StructField(\"formation_energy\", DoubleType(), True),\n",
    "        StructField(\"formation_energy_unit\", StringType(), True),\n",
    "        StructField(\"formation_energy_per_atom\", BooleanType(), True),\n",
    "        StructField(\"formation_energy_reference\", DoubleType(), True),\n",
    "        StructField(\"formation_energy_reference_unit\", StringType(), True),\n",
    "        StructField(\"formation_energy_property_id\", StringType(), True),\n",
    "        StructField(\"adsorption_energy\", DoubleType(), True),\n",
    "        StructField(\"adsorption_energy_unit\", StringType(), True),\n",
    "        StructField(\"adsorption_energy_per_atom\", BooleanType(), True),\n",
    "        StructField(\"adsorption_energy_reference\", DoubleType(), True),\n",
    "        StructField(\"adsorption_energy_reference_unit\", StringType(), True),\n",
    "        StructField(\"adsorption_energy_property_id\", StringType(), True),\n",
    "        StructField(\"atomization_energy\", DoubleType(), True),\n",
    "        StructField(\"atomization_energy_unit\", StringType(), True),\n",
    "        StructField(\"atomization_energy_per_atom\", BooleanType(), True),\n",
    "        StructField(\"atomization_energy_reference\", DoubleType(), True),\n",
    "        StructField(\"atomization_energy_reference_unit\", StringType(), True),\n",
    "        StructField(\"atomization_energy_property_id\", StringType(), True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringify_lists(row_dict):\n",
    "    \"\"\"\n",
    "    Replace list/tuple fields with comma-separated strings.\n",
    "    Spark and Vast both support array columns, but the connector does not,\n",
    "    so keeping cell values in list format crashes the table.\n",
    "    TODO: Remove when no longer necessary\n",
    "    \"\"\"\n",
    "    for key, val in row_dict.items():\n",
    "        if (\n",
    "            isinstance(val, np.ndarray)\n",
    "            or isinstance(val, list)\n",
    "            or isinstance(val, tuple)\n",
    "            or isinstance(val, dict)\n",
    "        ):\n",
    "            row_dict[key] = str(val)\n",
    "    return row_dict\n",
    "\n",
    "\n",
    "def _empty_dict_from_schema(schema):\n",
    "    empty_dict = {}\n",
    "    for field in schema:\n",
    "        empty_dict[field.name] = None\n",
    "    return empty_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "_CONFIGS_COLLECTION = \"configurations\"\n",
    "_PROPOBJECT_COLLECTION = \"property_objects\"\n",
    "\n",
    "\n",
    "class PGDataLoader:\n",
    "    \"\"\"\n",
    "    Class to load data from files to ColabFit PostgreSQL database\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        appname=\"colabfit\",\n",
    "        url=\"jdbc:postgresql://localhost:5432/colabfit\",\n",
    "        env=\"./.env\",\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # self.spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "        JARFILE = os.environ.get(\"CLASSPATH\")\n",
    "        self.spark = (\n",
    "            SparkSession.builder.appName(appname)\n",
    "            .config(\"spark.jars\", JARFILE)\n",
    "            .getOrCreate()\n",
    "        )\n",
    "\n",
    "        user = os.environ.get(\"PGS_USER\")\n",
    "        password = os.environ.get(\"PGS_PASS\")\n",
    "        driver = os.environ.get(\"PGS_DRIVER\")\n",
    "        self.properties = {\n",
    "            \"user\": user,\n",
    "            \"password\": password,\n",
    "            \"driver\": driver,\n",
    "        }\n",
    "        self.url = url\n",
    "        self.database_name = None\n",
    "        findspark.init()\n",
    "\n",
    "        # format = \"jdbc\"  # for postgres local\n",
    "        load_dotenv(env)\n",
    "\n",
    "    def get_spark(self):\n",
    "        return self.spark\n",
    "\n",
    "    def get_spark_context(self):\n",
    "        return self.spark.sparkContext\n",
    "\n",
    "    def write_table(self, spark_rows: list[dict], table_name: str, schema: StructType):\n",
    "        po_df = self.spark.createDataFrame(spark_rows, schema=schema)\n",
    "        po_df.write.jdbc(\n",
    "            url=self.url,\n",
    "            table=table_name,\n",
    "            mode=\"append\",\n",
    "            properties=self.properties,\n",
    "        )\n",
    "\n",
    "\n",
    "def batched(iterable, n):\n",
    "    \"Batch data into tuples of length n. The last batch may be shorter.\"\n",
    "    it = iter(iterable)\n",
    "    while batch := tuple(islice(it, n)):\n",
    "        yield batch\n",
    "\n",
    "\n",
    "class DataManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        nprocs: int = 2,\n",
    "        configs: list[AtomicConfiguration] = None,\n",
    "        prop_defs: list[dict] = None,\n",
    "        prop_map: dict = None,\n",
    "    ):\n",
    "        self.configs = configs\n",
    "        if isinstance(prop_defs, dict):\n",
    "            prop_defs = [prop_defs]\n",
    "        self.prop_defs = prop_defs\n",
    "        self.prop_map = prop_map\n",
    "        self.nprocs = nprocs\n",
    "\n",
    "    def _gather_co_do_rows(self, prop_defs, prop_map, configs):\n",
    "        \"\"\"Convert COs and DOs to Spark rows.\"\"\"\n",
    "        configs = list(configs)\n",
    "        co_po_rows = []\n",
    "        for config in configs:\n",
    "            co_po_rows.append(\n",
    "                (\n",
    "                    config.to_spark_row(),\n",
    "                    Property.from_definition(\n",
    "                        prop_defs,\n",
    "                        configuration=config,\n",
    "                        property_map=prop_map,\n",
    "                    ).to_spark_row(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return co_po_rows\n",
    "\n",
    "    def gather_co_do_rows_pool(self):\n",
    "        \"\"\"Convert COs and DOs to Spark rows using multiprocessing.\n",
    "        Returns batches of tuples of (configuration_row, property_row).\"\"\"\n",
    "        chunk_size = 1000\n",
    "\n",
    "        with Pool(self.nprocs) as p:\n",
    "            config_chunks = batched(self.configs, chunk_size)\n",
    "            print(\"config chunks obj\", config_chunks)\n",
    "            part_gather = partial(\n",
    "                self._gather_co_do_rows,\n",
    "                self.prop_defs,\n",
    "                self.prop_map,\n",
    "            )\n",
    "            pimap = p.imap(part_gather, config_chunks)\n",
    "            while batch := tuple(islice(pimap, chunk_size)):\n",
    "                yield batch\n",
    "\n",
    "        # For running without multiprocessing on notebook\n",
    "        # part_gather = partial(\n",
    "        #     self._gather_co_do_rows,\n",
    "        #     self.prop_defs,\n",
    "        #     self.prop_map,\n",
    "        # )\n",
    "        # while batch := tuple(islice(part_gather(self.configs), chunk_size)):\n",
    "        #     yield batch\n",
    "        #     break\n",
    "\n",
    "    def load_data_to_pg_in_batches(self, loader: PGDataLoader):\n",
    "        \"\"\"Load data to PostgreSQL database in batches.\"\"\"\n",
    "        co_po_rows = self.gather_co_do_rows_pool()\n",
    "        for co_po_batch in co_po_rows:\n",
    "            co_rows, po_rows = list(zip(*co_po_batch))\n",
    "\n",
    "            loader.write_table(\n",
    "                co_rows,\n",
    "                _CONFIGS_COLLECTION,\n",
    "                config_schema,\n",
    "            )\n",
    "            loader.write_table(\n",
    "                po_rows,\n",
    "                _PROPOBJECT_COLLECTION,\n",
    "                property_object_schema,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to DB and run loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/15 11:25:20 WARN Utils: Your hostname, arktos resolves to a loopback address: 127.0.1.1; using 172.24.21.25 instead (on interface enp5s0)\n",
      "24/04/15 11:25:20 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/04/15 11:25:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "JARFILE = os.environ.get(\"CLASSPATH\")\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"PostgreSQL Connection with PySpark\")\n",
    "    .config(\"spark.jars\", JARFILE)\n",
    "    .getOrCreate()\n",
    ")\n",
    "url = \"jdbc:postgresql://localhost:5432/colabfit\"\n",
    "user = os.environ.get(\"PGS_USER\")\n",
    "password = os.environ.get(\"PGS_PASS\")\n",
    "properties = {\n",
    "    \"user\": user,\n",
    "    \"password\": password,\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "}\n",
    "loader = PGDataLoader(appname=\"colabfit\", env=\"./.env\")\n",
    "with open(\"formation_energy.json\", \"r\") as f:\n",
    "    formation_energy_pd = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataManager' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dm \u001b[38;5;241m=\u001b[39m \u001b[43mDataManager\u001b[49m(\n\u001b[1;32m      2\u001b[0m     nprocs\u001b[38;5;241m=\u001b[39mcpu_count() \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m      3\u001b[0m     configs\u001b[38;5;241m=\u001b[39mcarmat_aconfigs,\n\u001b[1;32m      4\u001b[0m     prop_defs\u001b[38;5;241m=\u001b[39m[formation_energy_pd],\n\u001b[1;32m      5\u001b[0m     prop_map\u001b[38;5;241m=\u001b[39mCM_PROPERTY_MAP,\n\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataManager' is not defined"
     ]
    }
   ],
   "source": [
    "dm = DataManager(\n",
    "    nprocs=cpu_count() - 2,\n",
    "    configs=carmat_aconfigs,\n",
    "    prop_defs=[formation_energy_pd],\n",
    "    prop_map=CM_PROPERTY_MAP,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "# FIX the iterator\n",
    "dat1 = dm._gather_co_do_rows(\n",
    "    [formation_energy_pd], CM_PROPERTY_MAP, carmat_aconfigs[:1000]\n",
    ")\n",
    "print(len(dat1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gatherer = dm.gather_co_do_rows_pool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "g0 = next(gatherer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.load_data_to_pg_in_batches(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = chain.from_iterable(dm.gather_co_do_rows_pool())\n",
    "co_rows, po_rows = list(zip(*data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_PROPOBJECT_COLLECTION = \"property_objects\"\n",
    "sess.write_table(co_rows, _PROPOBJECT_COLLECTION, property_object_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Can we make the configuration and the property instance/data object at the same time?\n",
    "In this way, we would only have to pass through the data one time.\n",
    "\n",
    "Workflow:\n",
    "create database access object\n",
    "create data reader as function? of the database access object\n",
    "reader returns ase.Atoms-style objects (AtomicConfiguration)\n",
    "DOs and PIs are now one object\n",
    "These DOs point to a configuration\n",
    "The configuration may already exist in the database, so we keep track of the hash added to the DO\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = json.load(Path(\"sample_db/co_ds1.json\").open(\"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(\"sample_db/co_ds1.json\"), \"r\") as f:\n",
    "    co_json = spark.sparkContext.parallelize(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = co_json.map(_parse_config).map(stringify_lists)\n",
    "co_df = spark.createDataFrame(co, config_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_configs(co_path, spark):\n",
    "    with open(co_path, \"r\") as f:\n",
    "        co_json = spark.sparkContext.parallelize(json.load(f))\n",
    "    co = co_json.map(_parse_config).map(stringify_lists)\n",
    "    co_df = spark.createDataFrame(co, config_schema)\n",
    "    return co_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_configs(\"sample_db/co_ds1.json\", spark).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"co\"\n",
    "\n",
    "mode = \"append\"\n",
    "url = \"jdbc:postgresql://localhost:5432/colabfit\"\n",
    "properties = {\"user\": user, \"password\": password, \"driver\": \"org.postgresql.Driver\"}\n",
    "co_df.write.jdbc(url=url, table=table_name, mode=mode, properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# co_df.write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cf-dev-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
