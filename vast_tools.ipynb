{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from functools import partial\n",
    "# from itertools import chain, islice\n",
    "# from multiprocessing import Pool, cpu_count\n",
    "from pathlib import Path\n",
    "\n",
    "# from pprint import pprint\n",
    "\n",
    "import dateutil.parser\n",
    "import findspark\n",
    "import lmdb\n",
    "import numpy as np\n",
    "import psycopg\n",
    "import pyspark.sql.functions as sf\n",
    "from ase.atoms import Atoms\n",
    "from ase.io.cfg import read_cfg\n",
    "from dotenv import load_dotenv\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    ArrayType,\n",
    "    BooleanType,\n",
    "    DoubleType,\n",
    "    FloatType,\n",
    "    IntegerType,\n",
    "    LongType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    "    TimestampType,\n",
    ")\n",
    "from colabfit.tools.schema import (\n",
    "    property_object_schema,\n",
    "    config_df_schema,\n",
    "    config_schema,\n",
    "    property_object_df_schema,\n",
    ")\n",
    "from colabfit.tools.configuration import AtomicConfiguration, config_schema\n",
    "from colabfit.tools.database import DataManager, PGDataLoader\n",
    "from colabfit.tools.dataset import Dataset, dataset_schema\n",
    "from colabfit.tools.property import Property, property_object_schema\n",
    "from colabfit.tools.property_definitions import (\n",
    "    atomic_forces_pd,\n",
    "    cauchy_stress_pd,\n",
    "    potential_energy_pd,\n",
    ")\n",
    "from colabfit.tools.schema import configuration_set_schema\n",
    "import pyarrow as pa\n",
    "\n",
    "with open(\"formation_energy.json\", \"r\") as f:\n",
    "    formation_energy_pd = json.load(f)\n",
    "findspark.init()\n",
    "format = \"jdbc\"\n",
    "load_dotenv(\"./.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up MTPU and Carolina Materials readers and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MTPU data\n",
    "\n",
    "\n",
    "def convert_stress(keys, stress):\n",
    "    stresses = {k: s for k, s in zip(keys, stress)}\n",
    "    return [\n",
    "        [stresses[\"xx\"], stresses[\"xy\"], stresses[\"xz\"]],\n",
    "        [stresses[\"xy\"], stresses[\"yy\"], stresses[\"yz\"]],\n",
    "        [stresses[\"xz\"], stresses[\"yz\"], stresses[\"zz\"]],\n",
    "    ]\n",
    "\n",
    "\n",
    "SYMBOL_DICT = {\"0\": \"Si\", \"1\": \"O\"}\n",
    "\n",
    "\n",
    "def mtpu_reader(filepath):\n",
    "    with open(filepath, \"rt\") as f:\n",
    "        energy = None\n",
    "        forces = None\n",
    "        coords = []\n",
    "        cell = []\n",
    "        symbols = []\n",
    "        config_count = 0\n",
    "        info = dict()\n",
    "        for line in f:\n",
    "            if line.strip().startswith(\"Size\"):\n",
    "                size = int(f.readline().strip())\n",
    "            elif line.strip().lower().startswith(\"supercell\"):\n",
    "                cell.append([float(x) for x in f.readline().strip().split()])\n",
    "                cell.append([float(x) for x in f.readline().strip().split()])\n",
    "                cell.append([float(x) for x in f.readline().strip().split()])\n",
    "            elif line.strip().startswith(\"Energy\"):\n",
    "                energy = float(f.readline().strip())\n",
    "            elif line.strip().startswith(\"PlusStress\"):\n",
    "                stress_keys = line.strip().split()[-6:]\n",
    "                stress = [float(x) for x in f.readline().strip().split()]\n",
    "                stress = convert_stress(stress_keys, stress)\n",
    "            elif line.strip().startswith(\"AtomData:\"):\n",
    "                keys = line.strip().split()[1:]\n",
    "                if \"fx\" in keys:\n",
    "                    forces = []\n",
    "                for i in range(size):\n",
    "                    li = {\n",
    "                        key: val for key, val in zip(keys, f.readline().strip().split())\n",
    "                    }\n",
    "                    symbols.append(SYMBOL_DICT[li[\"type\"]])\n",
    "                    if \"cartes_x\" in keys:\n",
    "                        coords.append(\n",
    "                            [\n",
    "                                float(c)\n",
    "                                for c in [\n",
    "                                    li[\"cartes_x\"],\n",
    "                                    li[\"cartes_y\"],\n",
    "                                    li[\"cartes_z\"],\n",
    "                                ]\n",
    "                            ]\n",
    "                        )\n",
    "                    elif \"direct_x\" in keys:\n",
    "                        coords.append(\n",
    "                            [\n",
    "                                float(c)\n",
    "                                for c in [\n",
    "                                    li[\"direct_x\"],\n",
    "                                    li[\"direct_y\"],\n",
    "                                    li[\"direct_z\"],\n",
    "                                ]\n",
    "                            ]\n",
    "                        )\n",
    "\n",
    "                    if \"fx\" in keys:\n",
    "                        forces.append(\n",
    "                            [float(f) for f in [li[\"fx\"], li[\"fy\"], li[\"fz\"]]]\n",
    "                        )\n",
    "\n",
    "            elif line.startswith(\"END_CFG\"):\n",
    "\n",
    "                info[\"energy\"] = energy\n",
    "                if forces:\n",
    "                    info[\"forces\"] = forces\n",
    "                info[\"stress\"] = stress\n",
    "\n",
    "                if \"Si\" in symbols and \"O\" in symbols:\n",
    "                    info[\"input\"] = {\n",
    "                        \"kpoint-scheme\": \"Monkhorst-Pack\",\n",
    "                        \"kpoints\": \"11x11x11\",\n",
    "                        \"kinetic-energy-cutoff\": {\n",
    "                            \"val\": 1224,\n",
    "                            \"units\": \"eV\",\n",
    "                        },\n",
    "                    }\n",
    "                    info[\"_name\"] = f\"{filepath.stem}_SiO2_{config_count}\"\n",
    "                elif \"Si\" in symbols:\n",
    "                    info[\"input\"] = {\n",
    "                        \"kpoint-scheme\": \"Monkhorst-Pack\",\n",
    "                        \"kpoints\": \"8x8x8\",\n",
    "                        \"kinetic-energy-cutoff\": {\n",
    "                            \"val\": 884,\n",
    "                            \"units\": \"eV\",\n",
    "                        },\n",
    "                    }\n",
    "                    info[\"_name\"] = f\"{filepath.stem}_Si_{config_count}\"\n",
    "                elif \"O\" in symbols:\n",
    "                    info[\"input\"] = {\n",
    "                        \"kpoint-scheme\": \"Monkhorst-Pack\",\n",
    "                        \"kpoints\": \"gamma-point\",\n",
    "                        \"kinetic-energy-cutoff\": {\n",
    "                            \"val\": 1224,\n",
    "                            \"units\": \"eV\",\n",
    "                        },\n",
    "                    }\n",
    "                    info[\"_name\"] = f\"{filepath.stem}_O_{config_count}\"\n",
    "                if \"cartes_x\" in keys:\n",
    "                    config = AtomicConfiguration(\n",
    "                        positions=coords, symbols=symbols, cell=cell, info=info\n",
    "                    )\n",
    "                elif \"direct_x\" in keys:\n",
    "                    config = AtomicConfiguration(\n",
    "                        scaled_positions=coords, symbols=symbols, cell=cell, info=info\n",
    "                    )\n",
    "                config_count += 1\n",
    "                yield config\n",
    "                forces = None\n",
    "                stress = []\n",
    "                coords = []\n",
    "                cell = []\n",
    "                symbols = []\n",
    "                energy = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtpu_configs = mtpu_reader(Path(\"data/mtpu_2023/Unified_training_set.cfg\"))\n",
    "data = list(mtpu_configs)\n",
    "# data = [x for x in mtpu_configs]\n",
    "# data[0].configuration_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import colabfit.tools.configuration\n",
    "from importlib import reload\n",
    "\n",
    "reload(colabfit.tools.configuration)\n",
    "AtomicConfiguration = colabfit.tools.configuration.AtomicConfiguration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carolina Materials data\n",
    "\n",
    "SOFTWARE = \"VASP\"\n",
    "METHODS = \"DFT-PBE\"\n",
    "CM_PI_METADATA = {\n",
    "    \"software\": {\"value\": SOFTWARE},\n",
    "    \"method\": {\"value\": METHODS},\n",
    "    \"input\": {\"value\": {\"IBRION\": 6, \"NFREE\": 4}},\n",
    "}\n",
    "\n",
    "CM_PROPERTY_MAP = {\n",
    "    \"formation-energy\": [\n",
    "        {\n",
    "            \"energy\": {\"field\": \"energy\", \"units\": \"eV\"},\n",
    "            \"per-atom\": {\"value\": False, \"units\": None},\n",
    "        }\n",
    "    ],\n",
    "    \"_metadata\": CM_PI_METADATA,\n",
    "}\n",
    "CO_MD = {\n",
    "    key: {\"field\": key}\n",
    "    for key in [\n",
    "        \"_symmetry_space_group_name_H-M\",\n",
    "        \"_symmetry_Int_Tables_number\",\n",
    "        \"_chemical_formula_structural\",\n",
    "        \"_chemical_formula_sum\",\n",
    "        \"_cell_volume\",\n",
    "        \"_cell_formula_units_Z\",\n",
    "        \"symmetry_dict\",\n",
    "        \"formula_pretty\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "def load_row(txn, row):\n",
    "    try:\n",
    "        data = pickle.loads(txn.get(f\"{row}\".encode(\"ascii\")))\n",
    "        return data\n",
    "    except TypeError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def config_from_row(row: dict, row_num: int):\n",
    "    coords = row.pop(\"cart_coords\")\n",
    "    a_num = row.pop(\"atomic_numbers\")\n",
    "    cell = [\n",
    "        row.pop(x)\n",
    "        for x in [\n",
    "            \"_cell_length_a\",\n",
    "            \"_cell_length_b\",\n",
    "            \"_cell_length_c\",\n",
    "            \"_cell_angle_alpha\",\n",
    "            \"_cell_angle_beta\",\n",
    "            \"_cell_angle_gamma\",\n",
    "        ]\n",
    "    ]\n",
    "    symmetry_dict = {str(key): val for key, val in row.pop(\"symmetry_dict\").items()}\n",
    "    for key in symmetry_dict:\n",
    "        key = str(key)\n",
    "    info = {}\n",
    "    info = row\n",
    "    info[\"symmetry_dict\"] = symmetry_dict\n",
    "    info[\"_name\"] = f\"carolina_materials_{row_num}\"\n",
    "    if row_num % 10 == 0:\n",
    "        info[\"_labels\"] = [row_num % 10, \"bcc\"]\n",
    "    else:\n",
    "        info[\"_labels\"] = [row_num % 10, \"fcc\"]\n",
    "    config = AtomicConfiguration(\n",
    "        scaled_positions=coords,\n",
    "        numbers=a_num,\n",
    "        cell=cell,\n",
    "        info=info,\n",
    "    )\n",
    "    return config\n",
    "    # return AtomicConfiguration.from_ase(config)\n",
    "\n",
    "\n",
    "def carmat_reader(fp: Path):\n",
    "    parent = fp.parent\n",
    "    env = lmdb.open(str(parent))\n",
    "    txn = env.begin()\n",
    "    row_num = 0\n",
    "    rows = []\n",
    "    while row_num <= 100000:\n",
    "        row = load_row(txn, row_num)\n",
    "        if row is False:\n",
    "            env.close()\n",
    "            break\n",
    "        rows.append(row)\n",
    "        yield config_from_row(row, row_num)\n",
    "        row_num += 1\n",
    "    env.close()\n",
    "    return False\n",
    "    # return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "PI_METADATA = {\n",
    "    \"software\": {\"value\": \"Quantum ESPRESSO\"},\n",
    "    \"method\": {\"value\": \"DFT-PBE\"},\n",
    "    \"input\": {\"field\": \"input\"},\n",
    "}\n",
    "PROPERTY_MAP = {\n",
    "    \"potential-energy\": [\n",
    "        {\n",
    "            \"energy\": {\"field\": \"energy\", \"units\": \"eV\"},\n",
    "            \"per-atom\": {\"value\": False, \"units\": None},\n",
    "            # \"_metadata\": PI_METADATA,\n",
    "        }\n",
    "    ],\n",
    "    \"atomic-forces\": [\n",
    "        {\n",
    "            \"forces\": {\"field\": \"forces\", \"units\": \"eV/angstrom\"},\n",
    "            # \"_metadata\": PI_METADATA,\n",
    "        },\n",
    "    ],\n",
    "    \"cauchy-stress\": [\n",
    "        {\n",
    "            \"stress\": {\"field\": \"stress\", \"units\": \"GPa\"},\n",
    "            \"volume-normalized\": {\"value\": True, \"units\": None},\n",
    "        }\n",
    "    ],\n",
    "    \"_metadata\": PI_METADATA,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to DB and run loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/21 14:49:53 WARN Utils: Your hostname, arktos resolves to a loopback address: 127.0.1.1; using 172.24.21.25 instead (on interface enp5s0)\n",
      "24/05/21 14:49:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/05/21 14:49:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/21 14:49:55 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "JARFILE = os.environ.get(\"CLASSPATH\")\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"PostgreSQL Connection with PySpark\")\n",
    "    .config(\"spark.jars\", JARFILE)\n",
    "    .getOrCreate()\n",
    ")\n",
    "url = \"jdbc:postgresql://localhost:5432/colabfit\"\n",
    "user = os.environ.get(\"PGS_USER\")\n",
    "password = os.environ.get(\"PGS_PASS\")\n",
    "properties = {\n",
    "    \"user\": user,\n",
    "    \"password\": password,\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "}\n",
    "loader = PGDataLoader(appname=\"colabfit\", env=\"./.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/20 17:14:41 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AtomicConfiguration(name=Unified_training_set_SiO2_1061, symbols='Si4', pbc=False, cell=[[3.85085, 0.0, 0.077017], [-1.925425, 3.334933, -0.038508], [0.127258, 0.0, 6.362934]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1062/1062 [00:00<00:00, 2495.36it/s]\n"
     ]
    }
   ],
   "source": [
    "mtpu_configs = mtpu_reader(Path(\"data/mtpu_2023/Unified_training_set.cfg\"))\n",
    "\n",
    "PI_METADATA = {\n",
    "    \"software\": {\"value\": \"Quantum ESPRESSO\"},\n",
    "    \"method\": {\"value\": \"DFT-PBE\"},\n",
    "    \"input\": {\"field\": \"input\"},\n",
    "}\n",
    "PROPERTY_MAP = {\n",
    "    \"potential-energy\": [\n",
    "        {\n",
    "            \"energy\": {\"field\": \"energy\", \"units\": \"eV\"},\n",
    "            \"per-atom\": {\"value\": False, \"units\": None},\n",
    "            # \"_metadata\": PI_METADATA,\n",
    "        }\n",
    "    ],\n",
    "    \"atomic-forces\": [\n",
    "        {\n",
    "            \"forces\": {\"field\": \"forces\", \"units\": \"eV/angstrom\"},\n",
    "            # \"_metadata\": PI_METADATA,\n",
    "        },\n",
    "    ],\n",
    "    \"cauchy-stress\": [\n",
    "        {\n",
    "            \"stress\": {\"field\": \"stress\", \"units\": \"GPa\"},\n",
    "            \"volume-normalized\": {\"value\": True, \"units\": None},\n",
    "        }\n",
    "    ],\n",
    "    \"_metadata\": PI_METADATA,\n",
    "}\n",
    "spark = SparkSession.builder.appName(\"ColabfitIngestData\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "# loader = SparkDataLoader(table_prefix=\"ndb.colabfit.dev\")\n",
    "# print(loader.spark)\n",
    "mtpu_ds_id = \"DS_y7nrdsjtuwom_0\"\n",
    "mtpu_configs = list(mtpu_configs)\n",
    "print(mtpu_configs[0])\n",
    "co_po_rows = []\n",
    "for config in tqdm(mtpu_configs):\n",
    "    config.set_dataset_id(mtpu_ds_id)\n",
    "    co_po_rows.append(\n",
    "        (\n",
    "            config.spark_row,\n",
    "            Property.from_definition(\n",
    "                [potential_energy_pd, atomic_forces_pd, cauchy_stress_pd],\n",
    "                configuration=config,\n",
    "                property_map=PROPERTY_MAP,\n",
    "            ).spark_row,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'CO_47706510123393079',\n",
       " 'hash': 47706510123393079,\n",
       " 'last_modified': datetime.datetime(2024, 5, 20, 17, 14, 41),\n",
       " 'dataset_ids': \"['DS_y7nrdsjtuwom_0']\",\n",
       " 'metadata': None,\n",
       " 'chemical_formula_hill': 'Si4',\n",
       " 'chemical_formula_reduced': 'Si',\n",
       " 'chemical_formula_anonymous': 'A',\n",
       " 'elements': \"['Si']\",\n",
       " 'elements_ratios': '[1.0]',\n",
       " 'atomic_numbers': '[14, 14, 14, 14]',\n",
       " 'nsites': 4,\n",
       " 'nelements': 1,\n",
       " 'nperiodic_dimensions': 0,\n",
       " 'cell': '[[3.85085, 0.0, 0.077017], [-1.925425, 3.334933, -0.038508], [0.127258, 0.0, 6.362934]]',\n",
       " 'dimension_types': '[0, 0, 0]',\n",
       " 'pbc': '[False, False, False]',\n",
       " 'positions': '[[1.892001, 1.11132, 0.400465], [1.955509, -1.11132, 3.581973], [1.895339, -1.11132, -0.400508], [1.958847, 1.11132, 2.781]]',\n",
       " 'names': \"['Unified_training_set_Si_0']\",\n",
       " 'labels': None,\n",
       " 'configuration_set_ids': None}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co_po_rows[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2024, 5, 20, 17, 10, 24)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dateutil.parser.parse(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"making co po rows\")\n",
    "    co_rows, po_rows = list(zip(*co_po_rows))\n",
    "    print(\"making cos dataframes...\")\n",
    "    cos_dataframe = spark.createDataFrame(co_rows, schema=config_schema)\n",
    "    print(\"Done!\")\n",
    "    print(cos_dataframe.show(1, False))\n",
    "    print(\"making pos dataframes...\")\n",
    "    pos_dataframe = spark.createDataFrame(po_rows, schema=property_object_schema)\n",
    "    print(\"Done!\")\n",
    "    pos_dataframe.show(1, False)\n",
    "    try:\n",
    "        # loader.write_table(\n",
    "        #     co_rows,\n",
    "        #     loader.config_table,\n",
    "        #     config_schema,\n",
    "        # )\n",
    "        # loader.write_table(\n",
    "        #     po_rows,\n",
    "        #     loader.prop_object_table,\n",
    "        #     property_object_schema,\n",
    "        # )\n",
    "        print(loader.config_table)\n",
    "        cos_dataframe.write.mode(\"append\").saveAsTable(loader.config_table)\n",
    "        print(loader.prop_object_table)\n",
    "        pos_dataframe.write.mode(\"append\").saveAsTable(loader.prop_object_table)\n",
    "    except:\n",
    "        print(\"loader write failed\")\n",
    "except:\n",
    "    print(\"error getting df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_df = loader.spark.read.jdbc(\n",
    "    url=url, table=\"configurations\", properties=properties\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_elem(col_array, elem):\n",
    "    print(col_array)\n",
    "    unstrung = eval(col_array)\n",
    "    unstrung.append(elem)\n",
    "    unstrung = list(set(unstrung))\n",
    "    return str(unstrung)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-------------------+-------------------+---------------------+--------+---------------------+------------------------+--------------------------+----------------+-----------------+----------------------------+------+---------+--------------------+-------------------------------------------------------------------------------------------------+---------------+---------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------+----------+---------------------+\n",
      "|id                    |hash               |last_modified      |dataset_ids          |metadata|chemical_formula_hill|chemical_formula_reduced|chemical_formula_anonymous|elements        |elements_ratios  |atomic_numbers              |nsites|nelements|nperiodic_dimensions|cell                                                                                             |dimension_types|pbc                  |positions                                                                                                                                                                                                                                                                                                                                                                                                            |names                      |labels    |configuration_set_ids|\n",
      "+----------------------+-------------------+-------------------+---------------------+--------+---------------------+------------------------+--------------------------+----------------+-----------------+----------------------------+------+---------+--------------------+-------------------------------------------------------------------------------------------------+---------------+---------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------+----------+---------------------+\n",
      "|CO_1000272601413209564|1000272601413209564|2024-05-03 15:10:43|['DS_y7nrdsjtuw0g_0']|NULL    |In2N4Y2              |InN2Y                   |A2BC                      |['In', 'N', 'Y']|[0.25, 0.5, 0.25]|[39, 39, 49, 49, 7, 7, 7, 7]|8     |3        |0                   |[[3.42184625, 0.0, 0.0], [-1.7109231249999994, 2.9634057803445177, 0.0], [0.0, 0.0, 11.13673602]]|[0, 0, 0]      |[False, False, False]|[[0.0, 0.0, 0.0], [0.0, 0.0, 5.56836801], [-1.710923098524749e-08, 1.9756038634410311, 2.784184005], [1.7109231421092315, 0.9878019169034866, 8.352552015], [-1.710923098524749e-08, 1.9756038634410311, 6.919232818060203], [1.7109231421092315, 0.9878019169034866, 4.217503201939798], [1.7109231421092315, 0.9878019169034866, 1.350864808060202], [-1.710923098524749e-08, 1.9756038634410311, 9.7858712119398]]|['carolina_materials_4263']|[3, 'fcc']|NULL                 |\n",
      "+----------------------+-------------------+-------------------+---------------------+--------+---------------------+------------------------+--------------------------+----------------+-----------------+----------------------------+------+---------+--------------------+-------------------------------------------------------------------------------------------------+---------------+---------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------+----------+---------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config_df.show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEXT: find a way to check wehther cs-ids is null and handle the column =+ cs_id as array\n",
    "# OR find a way to use the lambda function to use the user-defined function (append element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"fcc\", 6]\n",
    "config_set_id = \"test_config_set_id\"\n",
    "config_df.withColumn(\"filter_labels\", sf.lit(labels)).withColumn(\n",
    "    \"labels_unstrung\", sf.from_json(sf.col(\"labels\"), ArrayType(StringType()))\n",
    ").withColumn(\n",
    "    \"has_labels\",\n",
    "    sf.forall(\n",
    "        \"filter_labels\",\n",
    "        lambda x: sf.array_contains(col=sf.col(\"labels_unstrung\"), value=x),\n",
    "    ),\n",
    ").withColumn(\n",
    "    \"new_cs_id\", sf.lit([config_set_id])\n",
    ").withColumn(\n",
    "    \"configuration_set_ids\",\n",
    "    sf.when(\n",
    "        condition=sf.col(\"has_labels\") == True,\n",
    "        value=sf.array_union(config_df[\"configuration_set_ids\"], sf.col(\"dataset_ids\")),\n",
    "    ),\n",
    "    # .otherwise(config_df[\"configuration_set_ids\"]),\n",
    ")\n",
    "# .withColumn(\n",
    "#     \"configuration_set_ids\",\n",
    "#     sf.transform_values(\n",
    "#         \"configuration_set_ids\", lambda k, cs_ids: append_elem(cs_ids, config_set_id)\n",
    "#     ),\n",
    "# )\n",
    "# .show(\n",
    "#     10, False\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.rdd  \n",
    "you can only parallelize one time so don't try to do a dataframe select from an rdd  \n",
    "updating to sdk 5.1 in a couple weeks  \n",
    "boto3 and s3 are the amazon file system interactions, mostly for adding metadata TO FILES (not to the database) and interacting with the files as FileExistsError. \n",
    "Make sure to spark.stop() at end of  python file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'carmat_reader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m carmat_config_gen \u001b[38;5;241m=\u001b[39m \u001b[43mcarmat_reader\u001b[49m(Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/carolina_matdb/base/all/data.mdb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      2\u001b[0m carmat_ds_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDS_y7nrdsjtuw0g_0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# carmat_ds_id2 = \"duplicate_ds_id\"\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'carmat_reader' is not defined"
     ]
    }
   ],
   "source": [
    "carmat_config_gen = carmat_reader(Path(\"data/carolina_matdb/base/all/data.mdb\"))\n",
    "carmat_ds_id = \"DS_y7nrdsjtuw0g_0\"\n",
    "# carmat_ds_id2 = \"duplicate_ds_id\"\n",
    "dm = DataManager(\n",
    "    nprocs=4,\n",
    "    configs=carmat_config_gen,\n",
    "    prop_defs=[formation_energy_pd],\n",
    "    prop_map=CM_PROPERTY_MAP,\n",
    "    dataset_id=carmat_ds_id,\n",
    ")\n",
    "# dm_dup = DataManager(\n",
    "#     nprocs=4,\n",
    "#     configs=carmat_config_gen,\n",
    "#     prop_defs=[formation_energy_pd],\n",
    "#     prop_map=CM_PROPERTY_MAP,\n",
    "#     dataset_id=carmat_ds_id2,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ID: DS_y7nrdsjtuwom_0\n"
     ]
    }
   ],
   "source": [
    "mtpu_ds_id = \"DS_y7nrdsjtuwom_0\"\n",
    "mtpu_configs = mtpu_reader(Path(\"data/mtpu_2023/Unified_training_set.cfg\"))\n",
    "dm2 = DataManager(\n",
    "    nprocs=4,\n",
    "    configs=mtpu_configs,\n",
    "    prop_defs=[potential_energy_pd, atomic_forces_pd, cauchy_stress_pd],\n",
    "    prop_map=PROPERTY_MAP,\n",
    "    dataset_id=mtpu_ds_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(dm.gather_co_po_in_batches())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = [x[0] for x in batch]\n",
    "cos_dataframe = loader.spark.createDataFrame(cos, schema=config_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rows = loader.spark.createDataFrame(cos, schema=config_schema).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "\n",
    "\n",
    "def spark_to_arrow_type(spark_type):\n",
    "    type_name = spark_type.dataType.typeName()\n",
    "    if type_name in data_type_map:\n",
    "        print(data_type_map[type_name])\n",
    "        return pa.field(spark_type.name, data_type_map[type_name])\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported PySpark data type: {spark_type}\")\n",
    "\n",
    "\n",
    "data_type_map = {\n",
    "    \"string\": pa.string(),\n",
    "    \"integer\": pa.int32(),\n",
    "    \"float\": pa.float64(),\n",
    "    # DoubleType: pa.float64(),\n",
    "    # BooleanType: pa.bool_(),\n",
    "    \"timestamp\": pa.timestamp(\"ns\"),\n",
    "    # DateType: pa.date32(),\n",
    "    # ArrayType: lambda dt: pa.list_(convert_type(dt)),\n",
    "    \"struct\": lambda st: pa.struct([spark_to_arrow_type(f) for f in st.fields]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_set_query = (\n",
    "    cos_dataframe.withColumn(\n",
    "        \"names_unstrung\", sf.from_json(sf.col(\"names\"), ArrayType(StringType()))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"labels_unstrung\",\n",
    "        sf.from_json(sf.col(\"labels\"), ArrayType(StringType())),\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"dataset_ids_unstrung\", sf.from_json(\"dataset_ids\", ArrayType(StringType()))\n",
    "    )\n",
    "    .drop(\"names\", \"labels\", \"dataset_ids\")\n",
    "    .withColumnRenamed(\"names_unstrung\", \"names\")\n",
    "    .withColumnRenamed(\"labels_unstrung\", \"labels\")\n",
    "    .withColumnRenamed(\"dataset_ids_unstrung\", \"dataset_ids\")\n",
    "    .filter(sf.array_contains(sf.col(\"dataset_ids\"), carmat_ds_id))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-------------------+-------------------+--------+---------------------+------------------------+--------------------------+-----------------------+--------------------+----------------------------------+------+---------+--------------------+-----------------------------------------------------------------------------------------------------------------------------------+---------------+---------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------+----------------------+--------+-------------------+\n",
      "|id                    |hash               |last_modified      |metadata|chemical_formula_hill|chemical_formula_reduced|chemical_formula_anonymous|elements               |elements_ratios     |atomic_numbers                    |nsites|nelements|nperiodic_dimensions|cell                                                                                                                               |dimension_types|pbc                  |positions                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |configuration_set_ids|names                 |labels  |dataset_ids        |\n",
      "+----------------------+-------------------+-------------------+--------+---------------------+------------------------+--------------------------+-----------------------+--------------------+----------------------------------+------+---------+--------------------+-----------------------------------------------------------------------------------------------------------------------------------+---------------+---------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------+----------------------+--------+-------------------+\n",
      "|CO_2035392092515548233|2035392092515548233|2024-05-08 15:42:38|NULL    |H6BrCaRh2            |BrCaH6Rh2               |A6B2CD                    |['Br', 'Ca', 'H', 'Rh']|[0.1, 0.1, 0.6, 0.2]|[35, 20, 1, 1, 1, 1, 1, 1, 45, 45]|10    |4        |0                   |[[5.39874426, 0.0, 0.0], [2.6993721300000004, 4.67544967769542, 0.0], [2.6993721300000004, 1.5584832258984738, 4.4080562295931855]]|[0, 0, 0]      |[False, False, False]|[[0.0, 0.0, 0.0], [5.398744260000001, 3.116966451796947, 2.2040281147965928], [5.398744260000001, 4.84493341938597, 3.4258852752451454], [5.39874426, 1.3889994842079243, 0.9821709543480398], [6.895207550832455, 3.980949935591458, 0.9821709543480398], [3.902280969167545, 2.252982968002436, 3.4258852752451454], [6.895207550832455, 2.252982968002436, 3.4258852752451454], [3.902280969167545, 3.980949935591458, 0.9821709543480398], [8.098116390000001, 4.67544967769542, 3.306042172194889], [2.6993721300000004, 1.5584832258984735, 1.1020140573982964]]|NULL                 |[carolina_materials_0]|[0, bcc]|[DS_y7nrdsjtuw0g_0]|\n",
      "+----------------------+-------------------+-------------------+--------+---------------------+------------------------+--------------------------+-----------------------+--------------------+----------------------------------+------+---------+--------------------+-----------------------------------------------------------------------------------------------------------------------------------+---------------+---------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------+----------------------+--------+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config_set_query.show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+------------------+-------------------+--------+---------------------+------------------------+--------------------------+------------------------+--------------------+----------------------------------------+------+---------+--------------------+--------------------------------------------------------------------------------------------------------------------------------+---------------+---------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------+--------+-------------------+---------------+----------------------+\n",
      "|id                   |hash              |last_modified      |metadata|chemical_formula_hill|chemical_formula_reduced|chemical_formula_anonymous|elements                |elements_ratios     |atomic_numbers                          |nsites|nelements|nperiodic_dimensions|cell                                                                                                                            |dimension_types|pbc                  |positions                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |configuration_set_ids|labels  |dataset_ids        |labels_exploded|names_exploded        |\n",
      "+---------------------+------------------+-------------------+--------+---------------------+------------------------+--------------------------+------------------------+--------------------+----------------------------------------+------+---------+--------------------+--------------------------------------------------------------------------------------------------------------------------------+---------------+---------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------+--------+-------------------+---------------+----------------------+\n",
      "|CO_623768866906050502|623768866906050502|2024-05-08 15:17:24|NULL    |Cd6MgSeTe2           |Cd6MgSeTe2              |A6B2CD                    |['Cd', 'Mg', 'Se', 'Te']|[0.6, 0.1, 0.1, 0.2]|[48, 48, 48, 48, 48, 48, 12, 34, 52, 52]|10    |4        |0                   |[[8.10975373, 0.0, 0.0], [4.054876866668527, 7.023251338917589, 0.0], [4.054875643331443, 2.341084015070624, 6.621585529535167]]|[0, 0, 0]      |[False, False, False]|[[2.0274378216657216, 1.170542007535312, 3.3107927647675837], [6.082314686665722, 1.170542007535312, 3.3107927647675837], [6.082315298334263, 3.5116256694587946, 0.0], [2.0274384333342637, 3.5116256694587946, 0.0], [4.054876254999986, 4.682167676994107, 3.3107927647675837], [4.054876865, 0.0, 0.0], [0.0, 0.0, 0.0], [8.109753119999985, 4.682167676994107, 3.3107927647675837], [12.164629679999976, 7.02325151549116, 4.9661891471513755], [4.054876559999992, 2.3410838384970534, 1.6553963823837918]]|NULL                 |[1, fcc]|[DS_y7nrdsjtuw0g_0]|1              |carolina_materials_101|\n",
      "+---------------------+------------------+-------------------+--------+---------------------+------------------------+--------------------------+------------------------+--------------------+----------------------------------------+------+---------+--------------------+--------------------------------------------------------------------------------------------------------------------------------+---------------+---------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------+--------+-------------------+---------------+----------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_match = \"fcc|1\"\n",
    "names_match = \"materials_10\"\n",
    "if names_match:\n",
    "    config_set_query = (\n",
    "        config_set_query.withColumn(\"labels_exploded\", sf.explode(sf.col(\"labels\")))\n",
    "        .withColumn(\"names_exploded\", sf.explode(sf.col(\"names\")))\n",
    "        .drop(\"names\")\n",
    "        .filter(sf.regexp_like(sf.col(\"names_exploded\"), sf.lit(rf\"{names_match}\")))\n",
    "    )\n",
    "if label_match:\n",
    "    config_set_query = config_set_query.filter(\n",
    "        sf.regexp_like(sf.col(\"labels_exploded\"), sf.lit(rf\"{label_match}\"))\n",
    "    )\n",
    "config_set_query.show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import colabfit.tools.configuration_set\n",
    "\n",
    "reload(colabfit.tools.configuration_set)\n",
    "ConfigurationSet = colabfit.tools.configuration_set.ConfigurationSet\n",
    "reload(colabfit.tools.database)\n",
    "from colabfit.tools.database import DataManager, PGDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_ids = [x[0] for x in config_set_query.select(\"id\").distinct().collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[Row(element='Ga', ratio=0.012887595268940222), Row(element='I', ratio=0.016706765146289347), Row(element='Pt', ratio=0.012332385551087706), Row(element='Se', ratio=0.016555344314147753), Row(element='Tl', ratio=0.006023184211854569), Row(element='Ni', ratio=0.017144203105809514), Row(element='Os', ratio=0.011339737873715026), Row(element='Co', ratio=0.014014839241549877), Row(element='Fe', ratio=0.016269327186769184), Row(element='Ru', ratio=0.012214613792755354), Row(element='Mg', ratio=0.010919124451099483), Row(element='Ti', ratio=0.02444605212241533), Row(element='Ag', ratio=0.008883355485640258), Row(element='H', ratio=0.07577771421841614), Row(element='Te', ratio=0.012988542490367953), Row(element='Al', ratio=0.018389218836751518), Row(element='C', ratio=0.01552904756296583), Row(element='S', ratio=0.027171627100964046), Row(element='Li', ratio=0.021552231774820397), Row(element='Ca', ratio=0.007991655029695307), Row(element='Zr', ratio=0.010077897605868398), Row(element='Cd', ratio=0.011104194357050323), Row(element='B', ratio=0.01663946699867086), Row(element='K', ratio=0.012231438329659975), Row(element='Nb', ratio=0.018456516984370005), Row(element='Tc', ratio=0.014418628127260797), Row(element='P', ratio=0.018254622541514543), Row(element='Rb', ratio=0.006309201339233138), Row(element='F', ratio=0.029426115046183355), Row(element='Cl', ratio=0.02343657990813803), Row(element='Hf', ratio=0.009775055941585209), Row(element='Br', ratio=0.024496525733129194), Row(element='Be', ratio=0.02158588084862964), Row(element='Cs', ratio=0.005013711997577267), Row(element='O', ratio=0.024412403048606087), Row(element='V', ratio=0.02474889378669852), Row(element='Y', ratio=0.008008479566599929), Row(element='In', ratio=0.009724582330871343), Row(element='N', ratio=0.0200043743795952), Row(element='Sb', ratio=0.013425980449888117), Row(element='Ir', ratio=0.01228191194037384), Row(element='Sn', ratio=0.011541632316570486), Row(element='Rh', ratio=0.01810320170937295), Row(element='Pb', ratio=0.006410148560660868), Row(element='Hg', ratio=0.009724582330871343), Row(element='Ta', ratio=0.009522687888015883), Row(element='Mn', ratio=0.014805592476067097), Row(element='Cr', ratio=0.015512223026061207), Row(element='Cu', ratio=0.011205141578478052), Row(element='Sc', ratio=0.010902299914194861), Row(element='Ge', ratio=0.014216733684405337), Row(element='Re', ratio=0.013762471187980551), Row(element='Ba', ratio=0.004929589313054158), Row(element='Na', ratio=0.029762605784275788), Row(element='As', ratio=0.02067735585578007), Row(element='Bi', ratio=0.008630987432070933), Row(element='Si', ratio=0.041876272355603414), Row(element='Mo', ratio=0.01157528139037973), Row(element='Au', ratio=0.011794000370139812), Row(element='Zn', ratio=0.012618402678466275), Row(element='Pd', ratio=0.011861298517758299), Row(element='W', ratio=0.01366152396655282), Row(element='Sr', ratio=0.003936941635681478)]\n",
      "[0.008883355485640258, 0.018389218836751518, 0.02067735585578007, 0.011794000370139812, 0.01663946699867086, 0.004929589313054158, 0.02158588084862964, 0.008630987432070933, 0.024496525733129194, 0.01552904756296583, 0.007991655029695307, 0.011104194357050323, 0.02343657990813803, 0.014014839241549877, 0.015512223026061207, 0.005013711997577267, 0.011205141578478052, 0.029426115046183355, 0.016269327186769184, 0.012887595268940222, 0.014216733684405337, 0.07577771421841614, 0.009775055941585209, 0.009724582330871343, 0.016706765146289347, 0.009724582330871343, 0.01228191194037384, 0.012231438329659975, 0.021552231774820397, 0.010919124451099483, 0.014805592476067097, 0.01157528139037973, 0.0200043743795952, 0.029762605784275788, 0.018456516984370005, 0.017144203105809514, 0.024412403048606087, 0.011339737873715026, 0.018254622541514543, 0.006410148560660868, 0.011861298517758299, 0.012332385551087706, 0.006309201339233138, 0.013762471187980551, 0.01810320170937295, 0.012214613792755354, 0.027171627100964046, 0.013425980449888117, 0.010902299914194861, 0.016555344314147753, 0.041876272355603414, 0.011541632316570486, 0.003936941635681478, 0.009522687888015883, 0.014418628127260797, 0.012988542490367953, 0.02444605212241533, 0.006023184211854569, 0.02474889378669852, 0.01366152396655282, 0.008008479566599929, 0.012618402678466275, 0.010077897605868398]\n",
      "[0.008883355485640258, 0.018389218836751518, 0.02067735585578007, 0.011794000370139812, 0.01663946699867086, 0.004929589313054158, 0.02158588084862964, 0.008630987432070933, 0.024496525733129194, 0.01552904756296583, 0.007991655029695307, 0.011104194357050323, 0.02343657990813803, 0.014014839241549877, 0.015512223026061207, 0.005013711997577267, 0.011205141578478052, 0.029426115046183355, 0.016269327186769184, 0.012887595268940222, 0.014216733684405337, 0.07577771421841614, 0.009775055941585209, 0.009724582330871343, 0.016706765146289347, 0.009724582330871343, 0.01228191194037384, 0.012231438329659975, 0.021552231774820397, 0.010919124451099483, 0.014805592476067097, 0.01157528139037973, 0.0200043743795952, 0.029762605784275788, 0.018456516984370005, 0.017144203105809514, 0.024412403048606087, 0.011339737873715026, 0.018254622541514543, 0.006410148560660868, 0.011861298517758299, 0.012332385551087706, 0.006309201339233138, 0.013762471187980551, 0.01810320170937295, 0.012214613792755354, 0.027171627100964046, 0.013425980449888117, 0.010902299914194861, 0.016555344314147753, 0.041876272355603414, 0.011541632316570486, 0.003936941635681478, 0.009522687888015883, 0.014418628127260797, 0.012988542490367953, 0.02444605212241533, 0.006023184211854569, 0.02474889378669852, 0.01366152396655282, 0.008008479566599929, 0.012618402678466275, 0.010077897605868398]\n"
     ]
    }
   ],
   "source": [
    "CS = ConfigurationSet(\n",
    "    name=\"test\", config_df=config_set_query, description=\"test description\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_co_rows_cs_id(self, co_ids: list[str], cs_id: str):\n",
    "    with psycopg.connect(\n",
    "        \"\"\"dbname=colabfit user=%s password=%s host=localhost port=5432\"\"\"\n",
    "        % (\n",
    "            user,\n",
    "            password,\n",
    "        )\n",
    "    ) as conn:\n",
    "        # dbname=self.database_name,\n",
    "        # user=self.properties[\"user\"],\n",
    "        # password=self.properties[\"password\"],\n",
    "        # host=\"localhost\",\n",
    "        # port=\"5432\",\n",
    "        cur = conn.execute(\n",
    "            \"\"\"UPDATE configurations\n",
    "                SET configuration_set_ids = \n",
    "            \"\"\"\n",
    "        )\n",
    "        cur = conn.execute(\n",
    "            \"\"\"UPDATE configurations\n",
    "                SET configuration_set_ids = concat(%s::text, \n",
    "                rtrim(ltrim(replace(configuration_set_ids,%s,''), \n",
    "                \n",
    "                '['),']') || ', ', %s::text)\n",
    "            WHERE id = ANY(%s)\"\"\",\n",
    "            (\"[\", f\"{cs_id}\", f\"{cs_id}]\", co_ids),\n",
    "            # (\"[\", f\", {cs_id}\", f\", {cs_id}]\"),\n",
    "        )\n",
    "        # cur.fetchall()\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You were trying to get  postgresql to recognize the WHERE id = ANY() array syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_co_rows_cs_id(loader, co_ids, CS.spark_row[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colabfit.tools.utilities import _empty_dict_from_schema\n",
    "from colabfit.tools.schema import configuration_set_schema\n",
    "import dateutil.parser\n",
    "\n",
    "cs = _empty_dict_from_schema(configuration_set_schema)\n",
    "cs[\"nconfigurations\"] = 200\n",
    "cs[\"dataset_id\"] = carmat_ds_id\n",
    "cs[\"name\"] = \"test\"\n",
    "cs[\"description\"] = \"test description for test\"\n",
    "cs[\"nelements\"] = 25\n",
    "cs[\"last_modified\"] = dateutil.parser.parse(\n",
    "    datetime.datetime.now(tz=datetime.timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    ")\n",
    "cs[\"id\"] = \"CS_y7nrdsjtuw0g_0\"\n",
    "cs[\"hash\"] = hash(cs[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader.write_table([cs], \"configuration_sets\", configuration_set_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_ids = [x[0] for x in config_set_query.select(\"id\").distinct().collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.load_data_to_pg_in_batches(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "with psycopg.connect(\n",
    "    dbname=\"colabfit\",\n",
    "    user=os.environ.get(\"PGS_USER\"),\n",
    "    password=os.environ.get(\"PGS_PASS\"),\n",
    "    host=\"localhost\",\n",
    ") as conn:\n",
    "    with conn.cursor() as cur:\n",
    "\n",
    "        # cur.execute(\n",
    "        #     \"UPDATE configurations SET configuration_set_ids = configuration_set_ids || %(cs_id)s WHERE id = ANY(%(co_ids)s)\",\n",
    "        #     {\"cs_id\": cs[\"id\"], \"co_ids\": co_ids},\n",
    "        # )\n",
    "        # data = cur.fetchall()\n",
    "        cur.execute(\n",
    "            \"SELECT * FROM public.configurations WHERE id = ANY(%s)\",\n",
    "            [co_ids],\n",
    "        )\n",
    "        data2 = cur.fetchall()\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_ids.append(\"CO_215290934057753943\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm2.load_data_to_pg_in_batches(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upsert appears to be this for postgres:\n",
    "```\n",
    "update the_table\n",
    "    set id = id || array[5,6]\n",
    "where id = 4;\n",
    "```\n",
    "* ~~Check for upsert function from pyspark to concatenate lists of relationships instead of primary key id collision~~\n",
    "* There is no pyspark-upsert function. Will have to manage this possibly through a different sql-based library\n",
    "* Written: find duplicates, but convert to access database, not download full dataframe\n",
    "* I see this being used with batches of hashes during upload: something like\n",
    "    ``` for batch in batches:\n",
    "            hash_duplicates = find_duplicates(batch, loader/database)\n",
    "            hash_duplicates.make_change_to_append_dataset-ids\n",
    "            hash_duplicates.write-to-database\n",
    "* Where would be the best place to catch duplicates? Keeping in mind that this might be a bulk operation (i.e. on the order of millions, like with ANI1/ANI2x variations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/30 09:52:06 WARN Utils: Your hostname, arktos resolves to a loopback address: 127.0.1.1; using 172.24.21.25 instead (on interface enp5s0)\n",
      "24/05/30 09:52:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/05/30 09:52:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/30 09:52:08 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "JARFILE = os.environ.get(\"CLASSPATH\")\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"PostgreSQL Connection with PySpark\")\n",
    "    .config(\"spark.jars\", JARFILE)\n",
    "    .getOrCreate()\n",
    ")\n",
    "url = \"jdbc:postgresql://localhost:5432/colabfit\"\n",
    "user = os.environ.get(\"PGS_USER\")\n",
    "password = os.environ.get(\"PGS_PASS\")\n",
    "properties = {\n",
    "    \"user\": user,\n",
    "    \"password\": password,\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "}\n",
    "loader = PGDataLoader(appname=\"colabfit\", env=\"./.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtpu_ds_id = \"DS_y7nrdsjtuwom_0\"\n",
    "mtpu_configs = mtpu_reader(Path(\"data/mtpu_2023/Unified_training_set.cfg\"))\n",
    "dm2 = DataManager(\n",
    "    nprocs=4,\n",
    "    configs=mtpu_configs,\n",
    "    prop_defs=[potential_energy_pd, atomic_forces_pd, cauchy_stress_pd],\n",
    "    prop_map=PROPERTY_MAP,\n",
    "    dataset_id=mtpu_ds_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = dm2\n",
    "\n",
    "\n",
    "def find_existing_rows_append_elem(\n",
    "    self,\n",
    "    table_name: str,\n",
    "    ids: list[str],\n",
    "    cols: list[str],\n",
    "    elems: list[str],\n",
    "    edit_schema: StructType,\n",
    "    write_schema: StructType,\n",
    "):\n",
    "    if isinstance(cols, str):\n",
    "        cols = [cols]\n",
    "    if isinstance(elems, str):\n",
    "        elems = [elems]\n",
    "    col_types = {\"id\": StringType(), \"$row_id\": IntegerType()}\n",
    "    edit_col_types = {\"id\": StringType(), \"$row_id\": IntegerType()}\n",
    "    for col in cols:\n",
    "        col_types[col] = get_spark_field_type(write_schema, col)\n",
    "        edit_col_types[col] = get_spark_field_type(edit_schema, col)\n",
    "    update_cols = [col for col in col_types if col != \"id\"]\n",
    "    query_schema = StructType(\n",
    "        [\n",
    "            StructField(col, col_types[col], False)\n",
    "            for i, col in enumerate(cols + [\"id\", \"$row_id\"])\n",
    "        ]\n",
    "    )\n",
    "    edit_schema = StructType(\n",
    "        [\n",
    "            StructField(col, edit_col_types[col], False)\n",
    "            for i, col in enumerate(cols + [\"id\", \"$row_id\"])\n",
    "        ]\n",
    "    )\n",
    "    partial_batch_to_rdd = partial(arrow_record_batch_to_rdd, query_schema)\n",
    "    with self.session.transaction() as tx:\n",
    "        # string would be 'ndb.colabfit.dev.[table name]'\n",
    "        table_path = table_name.split(\".\")\n",
    "        table = tx.bucket(table_path[1]).schema(table_path[2]).table(table_path[3])\n",
    "        rec_batch = table.select(\n",
    "            predicate=_.id.isin(ids), columns=cols + [\"id\"], internal_row_id=True\n",
    "        )\n",
    "        rdd = self.spark.sparkContext.parallelize([])\n",
    "        for batch in rec_batch:\n",
    "            rdd = rdd.union(\n",
    "                self.spark.sparkContext.parallelize(list(partial_batch_to_rdd(batch)))\n",
    "            )\n",
    "    rdd = rdd.map(unstringify_row_dict)\n",
    "\n",
    "    def add_elem_to_row_dict(col, elem, row_dict):\n",
    "        val = row_dict.get(col, [])\n",
    "        row_dict[col] = list(set(val + [elem]))\n",
    "        return row_dict\n",
    "\n",
    "    for col, elem in zip(cols, elems):\n",
    "        partial_add = partial(add_elem_to_row_dict, col, elem)\n",
    "        rdd = rdd.map(partial_add)\n",
    "    update_ids = rdd.map(lambda x: x[\"id\"]).collect()\n",
    "    new_ids = [id for id in ids if id not in update_ids]\n",
    "    rdd = rdd.map(stringify_row_dict)\n",
    "    rdd_collect = rdd.map(lambda x: [x[col] for col in update_cols]).collect()\n",
    "    update_schema = StructType(\n",
    "        [StructField(col, col_types[col], False) for col in update_cols]\n",
    "    )\n",
    "    arrow_schema = spark_schema_to_arrow_schema(update_schema)\n",
    "    update_table = pa.table(\n",
    "        [pa.array(col) for col in zip(*rdd_collect)], schema=arrow_schema\n",
    "    )\n",
    "    with self.session.transaction() as tx:\n",
    "        table = tx.bucket(table_path[1]).schema(table_path[2]).table(table_path[3])\n",
    "        table.update(rows=update_table)\n",
    "    return (new_ids, update_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader.spark.sql(f\"drop table if exists {loader.config_table}\")\n",
    "loader.spark.sql(f\"drop table if exists {loader.prop_object_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_list = list(mtpu_reader(Path(\"data/mtpu_2023/Unified_training_set.cfg\")))\n",
    "dm2.configs = config_list[:50]\n",
    "dm2.load_co_po_to_vastdb(loader)\n",
    "dm2.configs = config_list[25:]\n",
    "dm2.load_co_po_to_vastdb(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "\n",
    "import colabfit.tools.dataset\n",
    "import colabfit.tools.database\n",
    "\n",
    "reload(colabfit.tools.dataset)\n",
    "reload(colabfit.tools.database)\n",
    "DataManager = colabfit.tools.database.DataManager\n",
    "\n",
    "# Dataset = colabfit.tools.dataset.Dataset\n",
    "##############\n",
    "\n",
    "import json\n",
    "import lmdb\n",
    "import pickle\n",
    "from colabfit.tools.database import DataManager, SparkDataLoader\n",
    "\n",
    "carmat_config_gen = carmat_reader(Path(\"data/carolina_matdb/base/all/data.mdb\"))\n",
    "carmat_ds_id = \"DS_y7nrdsjtuw0g_0\"\n",
    "loader = SparkDataLoader(table_prefix=\"ndb.colabfit.dev\")\n",
    "load_dotenv()\n",
    "access_key = os.getenv(\"SPARK_ID\")\n",
    "access_secret = os.getenv(\"SPARK_KEY\")\n",
    "endpoint = os.getenv(\"SPARK_ENDPOINT\")\n",
    "loader.set_vastdb_session(\n",
    "    endpoint=endpoint, access_key=access_key, access_secret=access_secret\n",
    ")\n",
    "\n",
    "with open(\"formation_energy.json\", \"r\") as f:\n",
    "    formation_energy_pd = json.load(f)\n",
    "\n",
    "dm = DataManager(\n",
    "    nprocs=1,\n",
    "    configs=carmat_config_gen,\n",
    "    prop_defs=[formation_energy_pd],\n",
    "    prop_map=CM_PROPERTY_MAP,\n",
    "    dataset_id=carmat_ds_id,\n",
    ")\n",
    "dm.configs = carmat_reader(Path(\"data/carolina_matdb/base/all/data.mdb\"))\n",
    "dm.load_co_po_to_vastdb(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = dm2.gather_co_po_in_batches()\n",
    "batch = next(batches)\n",
    "cos, pos = zip(*batch)\n",
    "rdd = loader.spark.sparkContext.parallelize(cos)\n",
    "ids_coll = rdd.map(lambda x: x[\"id\"]).collect()\n",
    "loader.spark.read.table(loader.config_table).select(sf.col(\"id\")).filter(\n",
    "    sf.col(\"id\").isin(broadcast_ids.value)\n",
    ").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = mtpu_ds_id\n",
    "name_label_match = [\n",
    "    (\".*Si.*3.*\", \"None\", \"All_si_with_zero\", \"All Si with zero description\"),\n",
    "    (\".*Si.*4.*\", \"None\", \"All_si_with_two\", \"All Si with two description\"),\n",
    "]\n",
    "find_existing_rows_append_elem(\n",
    "    loader,\n",
    "    table_name=loader.config_table,\n",
    "    ids=co_ids,\n",
    "    cols=\"configuration_set_ids\",\n",
    "    elems=\"test1_cs-id\",\n",
    "    edit_schema=config_df_schema,\n",
    "    write_schema=config_schema,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_configuration_set(\n",
    "    self,\n",
    "    loader,\n",
    "    # below args in order:\n",
    "    # [config-name-regex-pattern], [config-label-regex-pattern], \\\n",
    "    # [config-set-name], [config-set-description]\n",
    "    name_label_match: list[tuple],\n",
    "    dataset_id: str,\n",
    "):\n",
    "    # Load unstrung dataframe of configs, filter for just includes ds-id\n",
    "    config_df = loader.read_table(table_name=loader.config_table, unstring=True)\n",
    "    config_df = config_df.filter(sf.array_contains(sf.col(\"dataset_ids\"), dataset_id))\n",
    "    # for each set of name-label matches, filter the config_df\n",
    "    # find the secret row id for the matching columns\n",
    "    # use vastdb-sdk to update configuration_set_ids column with new configuration set id\n",
    "    # Should be able to use current \"find_dups_append_elem_sdk\" function, but should rename this\n",
    "    # to reflect general update usage, rather than find duplicates only,\n",
    "    # or if necessary write very similar function that only updates, but assumes existence of row\n",
    "    # in the table, since we're checking for that here already.\n",
    "    for i, (names_match, label_match, cs_name, cs_desc) in enumerate(name_label_match):\n",
    "        print(\n",
    "            f\"names match: {names_match}, label {label_match}, cs_name {cs_name}, cs_desc {cs_desc}\"\n",
    "        )\n",
    "        if names_match:\n",
    "            config_set_query = config_df.withColumn(\n",
    "                \"names_exploded\", sf.explode(sf.col(\"names\"))\n",
    "            ).filter(sf.col(\"names_exploded\").rlike(names_match))\n",
    "        # Currently an AND operation on labels: labels col contains x AND y\n",
    "        # if label_match:\n",
    "        #     if isinstance(label_match, str):\n",
    "        #         label_match = [label_match]\n",
    "        #     for label in label_match:\n",
    "        #         config_set_query = config_set_query.filter(\n",
    "        #             sf.array_contains(sf.col(\"labels\"), label)\n",
    "        #         )\n",
    "    co_ids = [x[\"id\"] for x in config_set_query.select(\"id\").distinct().collect()]\n",
    "    find_existing_rows_append_elem(\n",
    "        loader,\n",
    "        table_name=loader.config_table,\n",
    "        ids=co_ids,\n",
    "        cols=\"configuration_set_ids\",\n",
    "        elems=cs_name,\n",
    "        edit_schema=config_df_schema,\n",
    "        write_schema=config_schema,\n",
    "    )\n",
    "    config_set = ConfigurationSet(\n",
    "        name=cs_name,\n",
    "        description=cs_desc,\n",
    "        config_df=config_set_query,\n",
    "    )\n",
    "    row = config_set.spark_row\n",
    "    loader.write_table([row], loader.config_set_table, schema=configuration_set_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_configuration_set(dm2, loader, name_label_match, dataset_id)\n",
    "\"\"\"  File \"<stdin>\", line 37, in find_existing_rows_append_elem\n",
    "  File \"/ext3/miniconda3/lib/python3.12/site-packages/vastdb/table.py\", line 351, in select\n",
    "    query_data_request = internal_commands.build_query_data_request(\n",
    "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/ext3/miniconda3/lib/python3.12/site-packages/vastdb/internal_commands.py\", line 2084, in build_query_data_request\n",
    "    filter_obj = predicate.serialize(builder)\n",
    "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/ext3/miniconda3/lib/python3.12/site-packages/vastdb/internal_commands.py\", line 245, in serialize\n",
    "    raise NotImplementedError(self.expr)  # an empty OR is equivalent to a 'FALSE' literal\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "\n",
    "import colabfit.tools.dataset\n",
    "import colabfit.tools.database\n",
    "\n",
    "reload(colabfit.tools.dataset)\n",
    "reload(colabfit.tools.database)\n",
    "DataManager = colabfit.tools.database.DataManager\n",
    "\n",
    "Dataset = colabfit.tools.dataset.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_duplicate_hash(spark_rows: dict, loader):\n",
    "    # hashes = loader.spark.createDataFrame([x[\"hash\"] for x in spark_rows])\n",
    "    hashes = [x[\"hash\"] for x in spark_rows]\n",
    "    duplicates = loader.spark.read.jdbc(\n",
    "        url=url,\n",
    "        table=\"configurations\",\n",
    "        properties=properties,\n",
    "    ).filter(sf.col(\"hash\").isin(hashes))\n",
    "    # dupl_hashes = df.filter(df.hash.isin(hashes)).select(\"hash\").collect()\n",
    "    return duplicates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cf-dev-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
