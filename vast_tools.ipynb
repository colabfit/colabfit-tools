{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "import boto3\n",
    "\n",
    "\n",
    "def create_s3_writer(bucket_name, access_key, secret_key, endpoint_url=None):\n",
    "    s3_client = boto3.client(\n",
    "        \"s3\",\n",
    "        aws_access_key_id=access_key,\n",
    "        aws_secret_access_key=secret_key,\n",
    "        endpoint_url=endpoint_url,\n",
    "    )\n",
    "\n",
    "    def write_to_s3(content, file_key):\n",
    "        try:\n",
    "            s3_client.put_object(Bucket=bucket_name, Key=file_key, Body=content)\n",
    "            return f\"s3://{bucket_name}/{file_key}\"\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "\n",
    "    return write_to_s3\n",
    "\n",
    "\n",
    "# Create the S3 writer function\n",
    "s3_writer = create_s3_writer(\n",
    "    bucket_name=\"your-bucket-name\",\n",
    "    access_key=\"your-access-key\",\n",
    "    secret_key=\"your-secret-key\",\n",
    "    endpoint_url=\"your-endpoint-url\",  # Optional, remove if using standard S3\n",
    ")\n",
    "\n",
    "\n",
    "# Create a UDF that uses the S3 writer\n",
    "@udf(returnType=StringType())\n",
    "def write_content_to_s3(content, id):\n",
    "    if content is None:\n",
    "        return None\n",
    "    file_key = f\"path/to/files/{id}.txt\"\n",
    "    return s3_writer(content, file_key)\n",
    "\n",
    "\n",
    "# Apply the UDF to your DataFrame\n",
    "df = df.withColumn(\"file_path\", write_content_to_s3(df.content_column, df.id))\n",
    "\n",
    "\n",
    "loader.access_key = os.getenv(\"SPARK_ID\")\n",
    "loader.access_secret = os.getenv(\"SPARK_KEY\")\n",
    "loader.endpoint = os.getenv(\"SPARK_ENDPOINT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from colabfit.tools.schema import *\n",
    "from pyspark import StorageLevel\n",
    "from ibis import _\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import pyspark.sql.functions as sf\n",
    "from colabfit.tools.utilities import unstring_df_val\n",
    "\n",
    "\n",
    "def get_config_ds_data(self, dataset_id, table_name):\n",
    "    elements_counts = defaultdict(lambda x: 0)\n",
    "    total_elements = 0\n",
    "    elements = set()\n",
    "    dimension_types = set()\n",
    "    nperiodic_dimensions = set()\n",
    "    nsites = 0\n",
    "    predicate = _.dataset_ids.contains(dataset_id)\n",
    "    bucket_name, schema_name, table_n = self._get_table_split(table_name)\n",
    "    with self.session.transaction() as tx:\n",
    "        table = tx.bucket(bucket_name).schema(schema_name).table(table_n)\n",
    "        rec_batch_reader = table.select(predicate=predicate, internal_row_id=False)\n",
    "        # spark_df: DataFrame = None\n",
    "        nconfigurations = 0\n",
    "        df_schema = config_schema\n",
    "        unstr_schema = config_df_schema\n",
    "        schema_type_dict = {f.name: f.dataType for f in unstr_schema}\n",
    "        string_cols = [f.name for f in unstr_schema if f.dataType.typeName() == \"array\"]\n",
    "        for rec_batch in tqdm(rec_batch_reader):\n",
    "            print(\"reading next batch\")\n",
    "            if rec_batch is None or rec_batch.num_rows == 0:\n",
    "                break\n",
    "            print(f\"Read {rec_batch.num_rows} rows\")\n",
    "            total_rows = rec_batch.num_rows\n",
    "            nconfigurations += total_rows\n",
    "            print(f\"Total rows read: {nconfigurations}\")\n",
    "            chunk_size = 10000\n",
    "            for i in range(0, total_rows, chunk_size):\n",
    "                batch_chunk = rec_batch.slice(i, min(chunk_size, total_rows - i))\n",
    "                pandas_chunk_df = batch_chunk.to_pandas()\n",
    "                chunk_spark_df = self.spark.createDataFrame(\n",
    "                    pandas_chunk_df, schema=df_schema\n",
    "                )\n",
    "                for col in string_cols:\n",
    "                    string_col_udf = sf.udf(unstring_df_val, schema_type_dict[col])\n",
    "                    chunk_spark_df = chunk_spark_df.withColumn(\n",
    "                        col, string_col_udf(sf.col(col))\n",
    "                    )\n",
    "                nsites += chunk_spark_df.agg({\"nsites\": \"sum\"}).first()[0]\n",
    "                batch_elements = sorted(\n",
    "                    chunk_spark_df.withColumn(\n",
    "                        \"exploded_elements\", sf.explode(\"elements\")\n",
    "                    )\n",
    "                    .agg(sf.collect_set(\"exploded_elements\").alias(\"exploded_elements\"))\n",
    "                    .select(\"exploded_elements\")\n",
    "                    .take(1)[0][0]\n",
    "                )\n",
    "                elements.update(batch_elements)\n",
    "                atomic_ratios_df = chunk_spark_df.select(\"atomic_numbers\").withColumn(\n",
    "                    \"single_element\", sf.explode(\"atomic_numbers\")\n",
    "                )\n",
    "                total_elements += atomic_ratios_df.count()\n",
    "                atomic_ratios_df = atomic_ratios_df.groupBy(\"single_element\").count()\n",
    "                dimension_types.update(\n",
    "                    chunk_spark_df.agg(sf.collect_set(\"dimension_types\")).collect()[0][0]\n",
    "                )\n",
    "                nperiodic_dimensions.update(\n",
    "                    chunk_spark_df.agg(sf.collect_set(\"nperiodic_dimensions\")).collect()[\n",
    "                        0\n",
    "                    ][0]\n",
    "                )\n",
    "                for row in atomic_ratios_df.collect():\n",
    "                    elements_counts[row[\"single_element\"]] += row[\"count\"]\n",
    "\n",
    "    nelements = len(list(elements))\n",
    "\n",
    "    config_data = {\n",
    "        \"elements\": None,\n",
    "        \"elements_ratios\": None,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colabfit.tools.utilities import *\n",
    "from colabfit.tools.schema import *\n",
    "import pyspark.sql.functions as sf\n",
    "\n",
    "cos = spark.table(\"ndb.colabfit.dev.co_oodcat\")\n",
    "cos = cos.filter(sf.col(\"dataset_ids\").contains(\"DS_wmgdq06mzdys_0\"))\n",
    "unstr_schema = config_df_schema\n",
    "schema_type_dict = {f.name: f.dataType for f in unstr_schema}\n",
    "array_cols = [f.name for f in unstr_schema if f.dataType.typeName() == \"array\"]\n",
    "df_schema = config_schema\n",
    "for col in array_cols:\n",
    "    string_col_udf = sf.udf(unstring_df_val, schema_type_dict[col])\n",
    "    cos = cos.withColumn(col, string_col_udf(sf.col(col)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = loader\n",
    "predicate = _.dataset_id == dm.dataset_id\n",
    "\n",
    "read_batches_to_spark(\n",
    "    loader, \"colabfit\", \"dev\", \"po_oodcat\", predicate, False, property_object_schema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "import sys\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "access_key = os.getenv(\"SPARK_ID\")\n",
    "access_secret = os.getenv(\"SPARK_KEY\")\n",
    "endpoint = os.getenv(\"SPARK_ENDPOINT\")\n",
    "spark_conf = {\n",
    "    \"access_key\": access_key,\n",
    "    \"access_secret\": access_secret,\n",
    "    \"endpoint\": endpoint,\n",
    "}\n",
    "\n",
    "\n",
    "def get_s3_client(spark_conf):\n",
    "    return boto3.client(\n",
    "        \"s3\",\n",
    "        use_ssl=False,\n",
    "        endpoint_url=spark_conf[\"endpoint\"],\n",
    "        aws_access_key_id=spark_conf[\"access_key\"],\n",
    "        aws_secret_access_key=spark_conf[\"access_secret\"],\n",
    "        region_name=\"fake-region\",\n",
    "        config=boto3.session.Config(\n",
    "            signature_version=\"s3v4\", s3={\"addressing_style\": \"path\"}\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "bucket_name = \"colabfit-data\"\n",
    "key = \"gpw_METADATA/test.txt\"\n",
    "content = \"Hello, world!\"\n",
    "file_path = f\"/vdev/{bucket_name}/{key}\"\n",
    "s3_client = get_s3_client(spark_conf)\n",
    "s3_client.put_object(Bucket=bucket_name, Key=key, Body=content)\n",
    "response = s3_client.head_object(Bucket=bucket_name, Key=key)\n",
    "\n",
    "\n",
    "class S3FileManager:\n",
    "    def __init__(self, bucket_name, access_id, secret_key, endpoint_url=None):\n",
    "        self.bucket_name = bucket_name\n",
    "        self.access_id = access_id\n",
    "        self.secret_key = secret_key\n",
    "        self.endpoint_url = endpoint_url\n",
    "        self.s3_client = None\n",
    "\n",
    "    def __getstate__(self):\n",
    "        # Don't pickle the client\n",
    "        state = self.__dict__.copy()\n",
    "        del state[\"s3_client\"]\n",
    "        return state\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        # Reconstruct the client on unpickling\n",
    "        self.__dict__.update(state)\n",
    "        self.s3_client = None\n",
    "\n",
    "    def get_client(self):\n",
    "        if self.s3_client is None:\n",
    "            self.s3_client = boto3.client(\n",
    "                \"s3\",\n",
    "                use_ssl=False,\n",
    "                endpoint_url=self.endpoint_url,\n",
    "                aws_access_key_id=self.access_id,\n",
    "                aws_secret_access_key=self.secret_key,\n",
    "                region_name=\"fake-region\",\n",
    "                config=boto3.session.Config(\n",
    "                    signature_version=\"s3v4\", s3={\"addressing_style\": \"path\"}\n",
    "                ),\n",
    "            )\n",
    "        return self.s3_client\n",
    "\n",
    "    def write_file(self, content, file_key):\n",
    "        try:\n",
    "            client = self.get_client()\n",
    "            client.put_object(Bucket=self.bucket_name, Key=file_key, Body=content)\n",
    "            return (f\"/vdev/{self.bucket_name}/{file_key}\", sys.getsizeof(content))\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "\n",
    "    def read_file(self, file_key):\n",
    "        try:\n",
    "            client = self.get_client()\n",
    "            response = client.get_object(Bucket=self.bucket_name, Key=file_key)\n",
    "            return response[\"Body\"].read().decode(\"utf-8\")\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "\n",
    "\n",
    "s3m = S3FileManager(\n",
    "    bucket_name=bucket_name,\n",
    "    access_key=access_key,\n",
    "    access_secret=access_secret,\n",
    "    endpoint_url=endpoint,\n",
    ")\n",
    "s3m.get_client()\n",
    "s3m.write_file(content=content, file_key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import json\n",
    "from time import time\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from functools import partial\n",
    "# from itertools import chain, islice\n",
    "# from multiprocessing import Pool, cpu_count\n",
    "from pathlib import Path\n",
    "\n",
    "# from pprint import pprint\n",
    "\n",
    "import dateutil.parser\n",
    "import findspark\n",
    "import lmdb\n",
    "import numpy as np\n",
    "import psycopg\n",
    "import pyspark.sql.functions as sf\n",
    "from ase.atoms import Atoms\n",
    "from ase.io.cfg import read_cfg\n",
    "from dotenv import load_dotenv\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    ArrayType,\n",
    "    BooleanType,\n",
    "    DoubleType,\n",
    "    FloatType,\n",
    "    IntegerType,\n",
    "    LongType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    "    TimestampType,\n",
    ")\n",
    "from colabfit.tools.schema import (\n",
    "    property_object_schema,\n",
    "    config_df_schema,\n",
    "    config_schema,\n",
    "    property_object_df_schema,\n",
    ")\n",
    "from colabfit.tools.configuration import AtomicConfiguration, config_schema\n",
    "from colabfit.tools.database import DataManager, PGDataLoader\n",
    "from colabfit.tools.dataset import Dataset, dataset_schema\n",
    "from colabfit.tools.property import Property, property_object_schema\n",
    "from colabfit.tools.property_definitions import (\n",
    "    atomic_forces_pd,\n",
    "    cauchy_stress_pd,\n",
    "    potential_energy_pd,\n",
    ")\n",
    "from colabfit.tools.schema import configuration_set_schema\n",
    "import pyarrow as pa\n",
    "\n",
    "with open(\"formation_energy.json\", \"r\") as f:\n",
    "    formation_energy_pd = json.load(f)\n",
    "findspark.init()\n",
    "format = \"jdbc\"\n",
    "load_dotenv(\"./.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to DB and run loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/21 14:49:53 WARN Utils: Your hostname, arktos resolves to a loopback address: 127.0.1.1; using 172.24.21.25 instead (on interface enp5s0)\n",
      "24/05/21 14:49:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/05/21 14:49:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/21 14:49:55 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "JARFILE = os.environ.get(\"CLASSPATH\")\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"PostgreSQL Connection with PySpark\")\n",
    "    .config(\"spark.jars\", JARFILE)\n",
    "    .getOrCreate()\n",
    ")\n",
    "url = \"jdbc:postgresql://localhost:5432/colabfit\"\n",
    "user = os.environ.get(\"PGS_USER\")\n",
    "password = os.environ.get(\"PGS_PASS\")\n",
    "properties = {\n",
    "    \"user\": user,\n",
    "    \"password\": password,\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "}\n",
    "loader = PGDataLoader(appname=\"colabfit\", env=\"./.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "from ase.io import iread\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "from colabfit.tools.configuration import AtomicConfiguration\n",
    "from colabfit.tools.database import DataManager, SparkDataLoader\n",
    "from colabfit.tools.property_definitions import (\n",
    "    atomic_forces_pd,\n",
    "    free_energy_pd,\n",
    "    potential_energy_pd,\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "loader = SparkDataLoader(table_prefix=\"ndb.colabfit.dev\")\n",
    "access_key = os.getenv(\"SPARK_ID\")\n",
    "access_secret = os.getenv(\"SPARK_KEY\")\n",
    "endpoint = os.getenv(\"SPARK_ENDPOINT\")\n",
    "# loader.set_vastdb_session(\n",
    "PKL_FP = Path(\"data/oc20_data_mapping.pkl\")\n",
    "with open(PKL_FP, \"rb\") as f:\n",
    "    OC20_MAP = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.rdd  \n",
    "you can only parallelize one time so don't try to do a dataframe select from an rdd  \n",
    "updating to sdk 5.1 in a couple weeks  \n",
    "boto3 and s3 are the amazon file system interactions, mostly for adding metadata TO FILES (not to the database) and interacting with the files as FileExistsError. \n",
    "Make sure to spark.stop() at end of  python file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_co_rows_cs_id(self, co_ids: list[str], cs_id: str):\n",
    "    with psycopg.connect(\n",
    "        \"\"\"dbname=colabfit user=%s password=%s host=localhost port=5432\"\"\"\n",
    "        % (\n",
    "            user,\n",
    "            password,\n",
    "        )\n",
    "    ) as conn:\n",
    "        # dbname=self.database_name,\n",
    "        # user=self.properties[\"user\"],\n",
    "        # password=self.properties[\"password\"],\n",
    "        # host=\"localhost\",\n",
    "        # port=\"5432\",\n",
    "        cur = conn.execute(\n",
    "            \"\"\"UPDATE configurations\n",
    "                SET configuration_set_ids = \n",
    "            \"\"\"\n",
    "        )\n",
    "        cur = conn.execute(\n",
    "            \"\"\"UPDATE configurations\n",
    "                SET configuration_set_ids = concat(%s::text, \n",
    "                rtrim(ltrim(replace(configuration_set_ids,%s,''), \n",
    "                \n",
    "                '['),']') || ', ', %s::text)\n",
    "            WHERE id = ANY(%s)\"\"\",\n",
    "            (\"[\", f\"{cs_id}\", f\"{cs_id}]\", co_ids),\n",
    "            # (\"[\", f\", {cs_id}\", f\", {cs_id}]\"),\n",
    "        )\n",
    "        # cur.fetchall()\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You were trying to get  postgresql to recognize the WHERE id = ANY() array syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "with psycopg.connect(\n",
    "    dbname=\"colabfit\",\n",
    "    user=os.environ.get(\"PGS_USER\"),\n",
    "    password=os.environ.get(\"PGS_PASS\"),\n",
    "    host=\"localhost\",\n",
    ") as conn:\n",
    "    with conn.cursor() as cur:\n",
    "\n",
    "        # cur.execute(\n",
    "        #     \"UPDATE configurations SET configuration_set_ids = configuration_set_ids || %(cs_id)s WHERE id = ANY(%(co_ids)s)\",\n",
    "        #     {\"cs_id\": cs[\"id\"], \"co_ids\": co_ids},\n",
    "        # )\n",
    "        # data = cur.fetchall()\n",
    "        cur.execute(\n",
    "            \"SELECT * FROM public.configurations WHERE id = ANY(%s)\",\n",
    "            [co_ids],\n",
    "        )\n",
    "        data2 = cur.fetchall()\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upsert appears to be this for postgres:\n",
    "```\n",
    "update the_table\n",
    "    set id = id || array[5,6]\n",
    "where id = 4;\n",
    "```\n",
    "* ~~Check for upsert function from pyspark to concatenate lists of relationships instead of primary key id collision~~\n",
    "* There is no pyspark-upsert function. Will have to manage this possibly through a different sql-based library\n",
    "* Written: find duplicates, but convert to access database, not download full dataframe\n",
    "* I see this being used with batches of hashes during upload: something like\n",
    "    ``` for batch in batches:\n",
    "            hash_duplicates = find_duplicates(batch, loader/database)\n",
    "            hash_duplicates.make_change_to_append_dataset-ids\n",
    "            hash_duplicates.write-to-database\n",
    "* Where would be the best place to catch duplicates? Keeping in mind that this might be a bulk operation (i.e. on the order of millions, like with ANI1/ANI2x variations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/30 09:52:06 WARN Utils: Your hostname, arktos resolves to a loopback address: 127.0.1.1; using 172.24.21.25 instead (on interface enp5s0)\n",
      "24/05/30 09:52:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/05/30 09:52:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/30 09:52:08 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "JARFILE = os.environ.get(\"CLASSPATH\")\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"PostgreSQL Connection with PySpark\")\n",
    "    .config(\"spark.jars\", JARFILE)\n",
    "    .getOrCreate()\n",
    ")\n",
    "url = \"jdbc:postgresql://localhost:5432/colabfit\"\n",
    "user = os.environ.get(\"PGS_USER\")\n",
    "password = os.environ.get(\"PGS_PASS\")\n",
    "properties = {\n",
    "    \"user\": user,\n",
    "    \"password\": password,\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "}\n",
    "loader = PGDataLoader(appname=\"colabfit\", env=\"./.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtpu_ds_id = \"DS_y7nrdsjtuwom_0\"\n",
    "mtpu_configs = mtpu_reader(Path(\"data/mtpu_2023/Unified_training_set.cfg\"))\n",
    "dm2 = DataManager(\n",
    "    nprocs=4,\n",
    "    configs=mtpu_configs,\n",
    "    prop_defs=[potential_energy_pd, atomic_forces_pd, cauchy_stress_pd],\n",
    "    prop_map=PROPERTY_MAP,\n",
    "    dataset_id=mtpu_ds_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colabfit.tools.schema import *\n",
    "import pyspark.sql.functions as sf\n",
    "\n",
    "rows = dm._gather_co_po_rows(dm.prop_defs, dm.prop_map, dm.dataset_id, dm.configs)\n",
    "co_rows, po_rows = list(zip(*rows))\n",
    "spark_df = loader.spark.createDataFrame(po_rows, schema=property_object_df_schema)\n",
    "table_name = loader.prop_object_table\n",
    "\n",
    "\n",
    "def write_table(\n",
    "    self,\n",
    "    spark_df,\n",
    "    table_name: str,\n",
    "    ids_filter: list[str] = None,\n",
    "    check_length_col: str = None,\n",
    "):\n",
    "    \"\"\"Include self.table_prefix in the table name when passed to this function\"\"\"\n",
    "\n",
    "\n",
    "if ids_filter is not None:\n",
    "    spark_df = spark_df.filter(sf.col(\"id\").isin(ids_filter))\n",
    "ids = [x[\"id\"] for x in spark_df.select(\"id\").collect()]\n",
    "all_unique = self.check_unique_ids(table_name, ids)\n",
    "if not all_unique:\n",
    "    raise ValueError(\"Duplicate IDs found in table. Not writing.\")\n",
    "table_split = table_name.split(\".\")\n",
    "string_cols = [f.name for f in spark_df.schema if f.dataType.typeName() == \"array\"]\n",
    "string_col_udf = sf.udf(stringify_df_val, StringType())\n",
    "for col in string_cols:\n",
    "    spark_df = spark_df.withColumn(col, string_col_udf(sf.col(col)))\n",
    "arrow_schema = spark_schema_to_arrow_schema(spark_df.schema)\n",
    "if not self.spark.catalog.tableExists(table_name):\n",
    "    print(f\"Creating table {table_name}\")\n",
    "    with self.session.transaction() as tx:\n",
    "        schema = tx.bucket(table_split[1]).schema(table_split[2])\n",
    "        schema.create_table(table_split[3], arrow_schema)\n",
    "arrow_rec_batch = pa.table(\n",
    "    [pa.array(col) for col in zip(*spark_df.collect())],\n",
    "    schema=arrow_schema,\n",
    ").to_batches()[0]\n",
    "with self.session.transaction() as tx:\n",
    "    table = tx.bucket(table_split[1]).schema(table_split[2]).table(table_split[3])\n",
    "    table.insert(arrow_rec_batch)\n",
    "\n",
    "\n",
    "write_table(loader, spark_df, table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = loader\n",
    "\n",
    "dataset_id = \"DS_y7nrdsjtuwom_0\"\n",
    "\n",
    "\n",
    "spark_schema = StructType(\n",
    "    [\n",
    "        StructField(\"id\", StringType(), False),\n",
    "        StructField(\"multiplicity\", IntegerType(), True),\n",
    "        StructField(\"last_modified\", TimestampType(), False),\n",
    "        StructField(\"$row_id\", IntegerType(), False),\n",
    "    ]\n",
    ")\n",
    "with self.session.transaction() as tx:\n",
    "    table_name = self.prop_object_table\n",
    "    table_path = table_name.split(\".\")\n",
    "    table = tx.bucket(table_path[1]).schema(table_path[2]).table(table_path[3])\n",
    "    rec_batches = table.select(\n",
    "        predicate=table[\"dataset_id\"] == dataset_id,\n",
    "        columns=[\"id\", \"multiplicity\", \"last_modified\"],\n",
    "        internal_row_id=True,\n",
    "    )\n",
    "    for rec_batch in rec_batches:\n",
    "        df = self.spark.createDataFrame(\n",
    "            rec_batch.to_struct_array().to_pandas(), schema=spark_schema\n",
    "        )\n",
    "        print(f\"length of df: {df.count()}\")\n",
    "        df = df.withColumn(\"multiplicity\", sf.lit(0))\n",
    "        update_time = dateutil.parser.parse(\n",
    "            datetime.datetime.now(tz=datetime.timezone.utc).strftime(\n",
    "                \"%Y-%m-%dT%H:%M:%SZ\"\n",
    "            )\n",
    "        )\n",
    "        df = df.withColumn(\"last_modified\", sf.lit(update_time).cast(\"timestamp\"))\n",
    "        arrow_schema = pa.schema(\n",
    "            [\n",
    "                pa.field(\"id\", pa.string()),\n",
    "                pa.field(\"multiplicity\", pa.int32()),\n",
    "                pa.field(\"last_modified\", pa.timestamp(\"us\")),\n",
    "                pa.field(\"$row_id\", pa.int32()),\n",
    "            ]\n",
    "        )\n",
    "        update_table = pa.table(\n",
    "            [pa.array(col) for col in zip(*df.collect())], schema=arrow_schema\n",
    "        )\n",
    "        table.update(\n",
    "            rows=update_table,\n",
    "            columns=[\"multiplicity\", \"last_modified\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with self.session.transaction() as tx:\n",
    "    table_path = \"ndb.colabfit.dev.test_co\".split(\".\")\n",
    "    table = tx.bucket(table_path[1]).schema(table_path[2]).table(table_path[3])\n",
    "    rec_batch = table.select()\n",
    "\n",
    "    rec_batch = table.select(\n",
    "        predicate=table[\"id\"].isin(id_batch),\n",
    "        columns=update_cols + [\"id\"],\n",
    "        internal_row_id=True,\n",
    "    )\n",
    "\n",
    "    rec_batch = rec_batch.read_all()\n",
    "    duplicate_df = self.spark.createDataFrame(\n",
    "        rec_batch.to_struct_array().to_pandas(), schema=spark_schema\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "\n",
    "import colabfit.tools.utilities\n",
    "import colabfit.tools.dataset\n",
    "import colabfit.tools.database\n",
    "import colabfit.tools.configuration_set\n",
    "import colabfit.tools.schema\n",
    "\n",
    "reload(colabfit.tools.utilities)\n",
    "reload(colabfit.tools.schema)\n",
    "reload(colabfit.tools.dataset)\n",
    "reload(colabfit.tools.database)\n",
    "DataManager = colabfit.tools.database.DataManager\n",
    "ConfigurationSet = colabfit.tools.configuration_set.ConfigurationSet\n",
    "Dataset = colabfit.tools.dataset.Dataset\n",
    "property_object_df_schema = colabfit.tools.schema.property_object_df_schema\n",
    "property_object_schema = colabfit.tools.schema.property_object_schema\n",
    "##############\n",
    "\n",
    "import json\n",
    "import lmdb\n",
    "import pickle\n",
    "from colabfit.tools.database import DataManager, SparkDataLoader\n",
    "\n",
    "loader = SparkDataLoader(table_prefix=\"ndb.colabfit.dev\")\n",
    "load_dotenv()\n",
    "access_key = os.getenv(\"SPARK_ID\")\n",
    "access_secret = os.getenv(\"SPARK_KEY\")\n",
    "endpoint = os.getenv(\"SPARK_ENDPOINT\")\n",
    "loader.set_vastdb_session(\n",
    "    endpoint=endpoint, access_key=access_key, access_secret=access_secret\n",
    ")\n",
    "\n",
    "with open(\"formation_energy.json\", \"r\") as f:\n",
    "    formation_energy_pd = json.load(f)\n",
    "\n",
    "carmat_config_gen = carmat_reader(Path(\"data/carolina_matdb/base/all/data.mdb\"))\n",
    "carmat_ds_id = \"DS_y7nrdsjtuw0g_0\"\n",
    "\n",
    "\n",
    "dm = DataManager(\n",
    "    nprocs=1,\n",
    "    configs=carmat_config_gen,\n",
    "    prop_defs=[formation_energy_pd],\n",
    "    prop_map=CM_PROPERTY_MAP,\n",
    "    dataset_id=carmat_ds_id,\n",
    ")\n",
    "dm.configs = carmat_reader(Path(\"data/carolina_matdb/base/all/data.mdb\"))\n",
    "\n",
    "match = [\n",
    "    (r\".*3.*\", None, \"3_configurations\", \"Carmat with 3\"),\n",
    "    (r\".*4.*\", None, \"4_configurations\", \"Carmat with 4\"),\n",
    "]\n",
    "# dm.load_co_po_to_vastdb(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vastdb.session import Session\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "vast_db_access = os.getenv(\"VAST_DB_ACCESS\")\n",
    "vast_db_secret = os.getenv(\"VAST_DB_SECRET\")\n",
    "endpoint = os.getenv(\"VAST_DB_ENDPOINT\")\n",
    "session = Session(access=vast_db_access, secret=vast_db_secret, endpoint=endpoint)\n",
    "\n",
    "with session.transaction() as tx:\n",
    "    table_split = \"ndb.colabfit-prod.prod.co_20240820\".split(\".\")\n",
    "    table = tx.bucket(table_split[1]).schema(table_split[2]).table(table_split[3])\n",
    "    print(table.projections())\n",
    "\n",
    "    sch = table.arrow_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colabfit.tools.utilities import *\n",
    "from colabfit.tools.database import *\n",
    "from colabfit.tools.schema import *\n",
    "from functools import partial\n",
    "\n",
    "self = loader\n",
    "batches = dm.gather_co_po_in_batches_no_pool()\n",
    "batch = next(batches)\n",
    "\n",
    "co_rows, po_rows = list(zip(*batch))\n",
    "codf = loader.spark.createDataFrame(co_rows, schema=config_df_schema)\n",
    "podf = loader.spark.createDataFrame(po_rows, schema=property_object_df_schema)\n",
    "df = podf\n",
    "string_cols = [f.name for f in df.schema if f.dataType.typeName() == \"array\"]\n",
    "string_col_udf = sf.udf(stringify_df_val, StringType())\n",
    "for col in string_cols:\n",
    "    df = df.withColumn(col, string_col_udf(sf.col(col)))\n",
    "\n",
    "column_name = \"atomic_forces_00\"\n",
    "\n",
    "\n",
    "####################################################################################\n",
    "def get_max_string_length(df, column_name):\n",
    "    return (\n",
    "        df.select(column_name)\n",
    "        .select(sf.length(column_name).alias(\"string_length\"))\n",
    "        .agg(sf.max(\"string_length\"))\n",
    "        .collect()[0][0]\n",
    "    )\n",
    "\n",
    "\n",
    "def split_long_string_cols(df, column_name: str, max_string_length: int = 60000):\n",
    "    \"\"\"\n",
    "    Splits a long string column into multiple columns based on a maximum string length.\n",
    "    :param df: Input DataFrame with array cols already stringified\n",
    "    :param column_name: Name of the column containing the long string\n",
    "    :param max_string_length: Maximum length for each split string\n",
    "    :return: DataFrame with the long string split across multiple columns\n",
    "    \"\"\"\n",
    "    if get_max_string_length(df, column_name) <= max_string_length:\n",
    "        print(\"no columns truncated\")\n",
    "        return df\n",
    "    print(\"columns truncated\")\n",
    "    overflow_columns = [\n",
    "        f\"{'_'.join(column_name.split('_')[:-1])}_{i+1:02}\" for i in range(19)\n",
    "    ]\n",
    "    if not all([col in df.columns for col in overflow_columns]):\n",
    "        raise ValueError(\"Overflow columns not found in target DataFrame schema\")\n",
    "    all_columns = [column_name] + overflow_columns\n",
    "    tmp_columns = [f\"{col_name}_tmp\" for col_name in all_columns]\n",
    "    df = df.withColumn(\"total_length\", sf.length(sf.col(column_name)))\n",
    "    substring_exprs = [\n",
    "        sf.when(\n",
    "            sf.length(sf.col(column_name)) - (i * max_string_length) > 0,\n",
    "            sf.substring(\n",
    "                sf.col(column_name), (i * max_string_length + 1), max_string_length\n",
    "            ),\n",
    "        )\n",
    "        .otherwise(sf.lit(None))\n",
    "        .alias(col_name)\n",
    "        for i, col_name in enumerate(tmp_columns)\n",
    "    ]\n",
    "    df = df.select(\"*\", *substring_exprs)\n",
    "    for tmp_col, col in zip(tmp_columns, all_columns):\n",
    "        df = df.drop(col).withColumnRenamed(f\"{tmp_col}\", col)\n",
    "    df = df.drop(\"total_length\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# Make this to replace the columns, not just add duplicate names\n",
    "def split_long_string(df, col_name, thresh):\n",
    "    columns = [sf.col(c) for c in df.columns]\n",
    "    num_splits = NSITES_COL_SPLITS\n",
    "    split_exprs = [\n",
    "        sf.when(\n",
    "            sf.col(col_name).substr(i * thresh + 1, thresh) != \"\",\n",
    "            sf.col(col_name).substr(i * thresh + 1, thresh),\n",
    "        ).otherwise(sf.lit(\"\"))\n",
    "        for i in range(num_splits)\n",
    "    ]\n",
    "    for i, expr in enumerate(split_exprs):\n",
    "        columns.append(expr.alias(f\"{'_'.join(col_name.split('_')[:-1])}_{i:02}\"))\n",
    "    return df.select(columns)\n",
    "\n",
    "\n",
    "df1 = split_long_string(podf, \"atomic_forces_00\", 500 // 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_filter = None\n",
    "check_length_col = \"atomic_forces_00\"\n",
    "_MAX_STRING_LEN = 2500\n",
    "if ids_filter is not None:\n",
    "    spark_df = spark_df.filter(sf.col(\"id\").isin(ids_filter))\n",
    "all_unique = self.check_unique_ids(table_name, spark_df)\n",
    "if not all_unique:\n",
    "    raise ValueError(\"Duplicate IDs found in table. Not writing.\")\n",
    "table_split = table_name.split(\".\")\n",
    "string_cols = [f.name for f in spark_df.schema if f.dataType.typeName() == \"array\"]\n",
    "string_col_udf = sf.udf(stringify_df_val, StringType())\n",
    "for col in string_cols:\n",
    "    spark_df = spark_df.withColumn(col, string_col_udf(sf.col(col)))\n",
    "if check_length_col is not None:\n",
    "    spark_df = split_long_string_cols(spark_df, check_length_col, _MAX_STRING_LEN)\n",
    "# arrow_schema = spark_schema_to_arrow_schema(spark_df.schema)\n",
    "# for field in arrow_schema:\n",
    "#     field = field.with_nullable(True)\n",
    "# if not self.spark.catalog.tableExists(table_name):\n",
    "#     print(f\"Creating table {table_name}\")\n",
    "#     with self.session.transaction() as tx:\n",
    "#         schema = tx.bucket(table_split[1]).schema(table_split[2])\n",
    "#         schema.create_table(table_split[3], arrow_schema)\n",
    "with self.session.transaction() as tx:\n",
    "    table = tx.bucket(table_split[1]).schema(table_split[2]).table(table_split[3])\n",
    "    arrow_schema = table.arrow_schema\n",
    "\n",
    "arrow_rec_batch = pa.table(\n",
    "    [pa.array(col) for col in zip(*spark_df.collect())],\n",
    "    schema=arrow_schema,\n",
    ").to_batches()\n",
    "\n",
    "with self.session.transaction() as tx:\n",
    "    table = tx.bucket(table_split[1]).schema(table_split[2]).table(table_split[3])\n",
    "\n",
    "    for rec_batch in arrow_rec_batch:\n",
    "        table.insert(rec_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This one leaves empty strings\n",
    "# substring_exprs2 = [\n",
    "#     sf.substring(\n",
    "#         df[column_name],\n",
    "#         (i * max_string_length + 1),\n",
    "#         max_string_length,\n",
    "#     ).alias(col_name)\n",
    "#     for i, col_name in enumerate(tmp_columns)\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def generate_random_float_array(n):\n",
    "    # Generate an n*3 array with random floats\n",
    "    random_array = np.random.rand(n, 3)\n",
    "    return random_array\n",
    "\n",
    "\n",
    "def split_string(s, max_length=60000):\n",
    "    if s is None:\n",
    "        return [None]\n",
    "    return [s[i : i + max_length] for i in range(0, len(s), max_length)]\n",
    "\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "x = generate_random_float_array(10000)\n",
    "xstr = \"\".join(np.array2string(np.arr(x), separator=\",\").replace(\"\\n\", \"\"))\n",
    "splx = split_string(xstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colabfit.tools.utilities import _write_value\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_write = partial(\n",
    "    _write_value,\n",
    "    \"CO/positions\",\n",
    "    \"txt\",\n",
    "    \"/scratch/gw2338/vast/data-lake-main/spark/scripts\",\n",
    "    \"positions\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_struct = StructType(\n",
    "    [\n",
    "        StructField(\"datetime\", TimestampType(), False),\n",
    "        StructField(\"datetime2\", TimestampType(), False),\n",
    "    ]\n",
    ")\n",
    "import pyarrow as pa\n",
    "import datetime\n",
    "\n",
    "timestamp_pyarrow_schema = pa.schema(\n",
    "    [\n",
    "        pa.field(\"datetime\", pa.timestamp(\"us\")),\n",
    "        pa.field(\"datetime2\", pa.timestamp(\"ns\")),\n",
    "    ]\n",
    ")\n",
    "datetimes = [datetime.datetime.now(tz=datetime.timezone.utc) for x in range(10)]\n",
    "dtdf = spark.createDataFrame(\n",
    "    datetimes,\n",
    "    schema=timestamp_struct,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sf\n",
    "from ast import literal_eval\n",
    "from colabfit.tools.schema import *\n",
    "\n",
    "\n",
    "def create_join_udf():\n",
    "    def join_cols(*cols):\n",
    "        return literal_eval(\n",
    "            \"\".join(\n",
    "                sf.col(c) for c in cols if sf.col(c) is not None and sf.col(c) != \"[]\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return sf.udf(join_cols, ArrayType(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colabfit.tools.schema import *\n",
    "\n",
    "old_ds = spark.table(\"ndb.`colabfit-prod`.prod.ds_20240820\")\n",
    "# new_ds = spark.table('ndb.`colabfit-prod`.prod.ds_20240903')\n",
    "old_schema = dataset_df_schema\n",
    "new_schema = dataset_df_schema.add(StructField(\"doi\", StringType(), True)).add(\n",
    "    StructField(\"publication_year\", StringType(), True)\n",
    ")\n",
    "# old_ds = old_ds.withColumn('doi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open(\"dois_pub_years.csv\", \"r\") as f:\n",
    "    df = pd.read_csv(f)\n",
    "\n",
    "dois_df = spark.createDataFrame(\n",
    "    df,\n",
    "    schema=StructType(\n",
    "        [\n",
    "            StructField(\"id\", StringType(), True),\n",
    "            StructField(\"doi\", StringType(), True),\n",
    "            StructField(\"publication-year\", StringType(), True),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "dois_df = dois_df.withColumnRenamed(\"colabfit-id\", \"id\")\n",
    "dois_df = dois_df.withColumnRenamed(\"publication-year\", \"publication_year\")\n",
    "dois_df.show()\n",
    "\n",
    "\n",
    "new_ds = old_ds.join(dois_df, on=\"id\", how=\"left\")\n",
    "old_ds.count()\n",
    "new_ds.count()\n",
    "import pyspark.sql.functions as sf\n",
    "\n",
    "new_ds.filter(sf.col(\"doi\").isNotNull()).count()\n",
    "new_ds.printSchema()\n",
    "new_ds.write.mode(\"overwrite\").saveAsTable(\n",
    "    \"ndb.`colabfit-prod`.prod.ds_20240903\", schema=new_schema\n",
    ")\n",
    "# new_ds.write.mode(\"overwrite\").saveAsTable(\n",
    "#     \"ndb.colabfit.dev.ds_20240903\", schema=new_schema\n",
    "# )\n",
    "cos = spark.table(\"ndb.`colabfit-prod`.prod.co_20240903\")\n",
    "cos.write.mode(\"overwrite\").saveAsTable(\"ndb.colabfit.dev.co_20240903\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dss = spark.table(\"ndb.`colabfit-prod`.prod.ds_20240903\")\n",
    "dss = dss.filter(sf.col(\"id\") != \"DS_otx1qc9f3pm4_0\")\n",
    "dss.write.mode(\"overwrite\").saveAsTable(\n",
    "    \"ndb.`colabfit-prod`.prod.ds_20240903\", schema=new_schema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_cos = spark.table(\"ndb.`colabfit-prod`.prod.co_20240820\")\n",
    "new_pos = spark.table(\"ndb.`colabfit-prod`.prod.po_20240820\")\n",
    "new_pos_oc20 = new_pos.filter(sf.col(\"dataset_id\") == \"DS_otx1qc9f3pm4_0\")\n",
    "new_pos_oc20 = new_pos_oc20.select(\"configuration_id\").withColumnRenamed(\n",
    "    \"configuration_id\", \"id\"\n",
    ")\n",
    "cos_not_in_new_pos = old_cos.filter(sf.col(\"dataset_ids\").contains(\"DS_otx1qc9f3pm4_0\"))\n",
    "cos_not_in_new_pos = cos_not_in_new_pos.join(new_pos_oc20, on=\"id\", how=\"left_anti\")\n",
    "cos_not_in_new_pos.count()\n",
    "new_cos = old_cos.join(cos_not_in_new_pos, on=\"id\", how=\"left_anti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For updating single rows in database: example of updating publication year in dataset table\n",
    "from vastdb.session import Session\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "vast_db_access = os.getenv(\"VAST_DB_ACCESS\")\n",
    "vast_db_secret = os.getenv(\"VAST_DB_SECRET\")\n",
    "endpoint = os.getenv(\"VAST_DB_ENDPOINT\")\n",
    "session = Session(access=vast_db_access, secret=vast_db_secret, endpoint=endpoint)\n",
    "\n",
    "table_split = \"ndb.`colabfit-prod`.prod.ds\".split(\".\")\n",
    "bucket_name = table_split[1].replace(\"`\", \"\")\n",
    "schema_name = table_split[2].replace(\"`\", \"\")\n",
    "table_name = table_split[3]\n",
    "\n",
    "\n",
    "with session.transaction() as tx:\n",
    "    table = tx.bucket(bucket_name).schema(schema_name).table(table_name)\n",
    "    # rec_batch = table.select(\n",
    "    #     predicate=table[\"id\"] == \"DS_otx1qc9f3pm4_0\",\n",
    "    #     columns=[\"id\", \"publication_year\"],\n",
    "    #     internal_row_id=True,\n",
    "    # )\n",
    "    # rec_batch = rec_batch.read_all()\n",
    "    pa_table = pa.table(\n",
    "        {\"id\": [\"DS_otx1qc9f3pm4_0\"], \"publication_year\": [\"2024\"], \"$row_id\": [524288]}\n",
    "    )\n",
    "    table.update(\n",
    "        rows=pa_table,\n",
    "        columns=[\"publication_year\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colabfit.tools.utilities import _empty_dict_from_schema\n",
    "\n",
    "\n",
    "def to_spark_row(self, config_df, prop_df):\n",
    "    \"\"\"\"\"\"\n",
    "    row_dict = _empty_dict_from_schema(dataset_schema)\n",
    "    row_dict[\"last_modified\"] = dateutil.parser.parse(\n",
    "        datetime.datetime.now(tz=datetime.timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    )\n",
    "    row_dict[\"nconfiguration_sets\"] = len(self.configuration_set_ids)\n",
    "    config_df = config_df.select(\n",
    "        \"id\",\n",
    "        \"elements\",\n",
    "        \"atomic_numbers\",\n",
    "        \"nsites\",\n",
    "        \"nperiodic_dimensions\",\n",
    "        \"dimension_types\",\n",
    "        # \"labels\",\n",
    "    )\n",
    "    prop_df = prop_df.select(\n",
    "        \"atomization_energy\",\n",
    "        \"atomic_forces_00\",\n",
    "        \"adsorption_energy\",\n",
    "        \"electronic_band_gap\",\n",
    "        \"cauchy_stress\",\n",
    "        \"formation_energy\",\n",
    "        \"energy\",\n",
    "    )\n",
    "    carray_cols = [\"atomic_numbers\", \"elements\", \"dimension_types\"]\n",
    "    carray_types = {\n",
    "        col.name: col.dataType for col in config_df_schema if col.name in carray_cols\n",
    "    }\n",
    "    for col in carray_cols:\n",
    "        unstr_udf = sf.udf(unstring_df_val, carray_types[col])\n",
    "        config_df = config_df.withColumn(col, unstr_udf(sf.col(col)))\n",
    "    row_dict[\"nsites\"] = config_df.agg({\"nsites\": \"sum\"}).first()[0]\n",
    "    row_dict[\"elements\"] = sorted(\n",
    "        config_df.select(\"elements\")\n",
    "        .withColumn(\"exploded_elements\", sf.explode(\"elements\"))\n",
    "        .agg(sf.collect_set(\"exploded_elements\").alias(\"exploded_elements\"))\n",
    "        .select(\"exploded_elements\")\n",
    "        .take(1)[0][0]\n",
    "    )\n",
    "    row_dict[\"nelements\"] = len(row_dict[\"elements\"])\n",
    "    atomic_ratios_df = config_df.select(\"atomic_numbers\").withColumn(\n",
    "        \"single_element\", sf.explode(\"atomic_numbers\")\n",
    "    )\n",
    "    total_elements = atomic_ratios_df.count()\n",
    "    print(total_elements, row_dict[\"nsites\"])\n",
    "    assert total_elements == row_dict[\"nsites\"]\n",
    "    atomic_ratios_df = atomic_ratios_df.groupBy(\"single_element\").count()\n",
    "    atomic_ratios_df = atomic_ratios_df.withColumn(\n",
    "        \"ratio\", sf.col(\"count\") / total_elements\n",
    "    )\n",
    "    atomic_ratios_coll = (\n",
    "        atomic_ratios_df.withColumn(\n",
    "            \"element\",\n",
    "            sf.udf(lambda x: ELEMENT_MAP[x], StringType())(sf.col(\"single_element\")),\n",
    "        )\n",
    "        .select(\"element\", \"ratio\")\n",
    "        .collect()\n",
    "    )\n",
    "    row_dict[\"total_elements_ratios\"] = [\n",
    "        x[1] for x in sorted(atomic_ratios_coll, key=lambda x: x[\"element\"])\n",
    "    ]\n",
    "    row_dict[\"nperiodic_dimensions\"] = config_df.agg(\n",
    "        sf.collect_set(\"nperiodic_dimensions\")\n",
    "    ).collect()[0][0]\n",
    "    row_dict[\"dimension_types\"] = (\n",
    "        config_df.select(\"dimension_types\")\n",
    "        .agg(sf.collect_set(\"dimension_types\"))\n",
    "        .collect()[0][0]\n",
    "    )\n",
    "    nproperty_objects = prop_df.count()\n",
    "    row_dict[\"nproperty_objects\"] = nproperty_objects\n",
    "    for prop in [\n",
    "        \"atomization_energy\",\n",
    "        \"adsorption_energy\",\n",
    "        \"electronic_band_gap\",\n",
    "        \"cauchy_stress\",\n",
    "        \"formation_energy\",\n",
    "        \"energy\",\n",
    "    ]:\n",
    "        row_dict[f\"{prop}_count\"] = (\n",
    "            prop_df.select(prop).where(f\"{prop} is not null\").count()\n",
    "        )\n",
    "    row_dict[f\"atomic_forces_count\"] = (\n",
    "        prop_df.select(\"atomic_forces_00\")\n",
    "        .filter(sf.col(\"atomic_forces_00\") != \"[]\")\n",
    "        .count()\n",
    "    )\n",
    "    prop = \"energy\"\n",
    "    row_dict[f\"{prop}_variance\"] = (\n",
    "        prop_df.select(prop).where(f\"{prop} is not null\").agg(sf.variance(prop))\n",
    "    ).first()[0]\n",
    "    row_dict[f\"{prop}_mean\"] = (\n",
    "        prop_df.select(prop).where(f\"{prop} is not null\").agg(sf.mean(prop))\n",
    "    ).first()[0]\n",
    "    return row_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from pathlib import Path\n",
    "\n",
    "DATASET_FP = Path(\n",
    "    \"/scratch/gw2338/vast/data-lake-main/spark/scripts/gw_scripts/data/s2ef_val_ood_both/\"\n",
    ")\n",
    "\n",
    "DATASET_NAME = \"OC20_S2EF_val_ood_both\"\n",
    "DATASET_ID = \"DS_889euoe7akyy_0\"\n",
    "DOI = None\n",
    "\n",
    "PUBLICATION_YEAR = \"2024\"\n",
    "AUTHORS = [\n",
    "    \"Lowik Chanussot\",\n",
    "    \"Abhishek Das\",\n",
    "    \"Siddharth Goyal\",\n",
    "    \"Thibaut Lavril\",\n",
    "    \"Muhammed Shuaibi\",\n",
    "    \"Morgane Riviere\",\n",
    "    \"Kevin Tran\",\n",
    "    \"Javier Heras-Domingo\",\n",
    "    \"Caleb Ho\",\n",
    "    \"Weihua Hu\",\n",
    "    \"Aini Palizhati\",\n",
    "    \"Anuroop Sriram\",\n",
    "    \"Brandon Wood\",\n",
    "    \"Junwoong Yoon\",\n",
    "    \"Devi Parikh\",\n",
    "    \"C. Lawrence Zitnick\",\n",
    "    \"Zachary Ulissi\",\n",
    "]\n",
    "\n",
    "LICENSE = \"CC-BY-4.0\"\n",
    "PUBLICATION = \"https://doi.org/10.1021/acscatal.0c04525\"\n",
    "DATA_LINK = \"https://fair-chem.github.io/core/datasets/oc20.html\"\n",
    "DESCRIPTION = (\n",
    "    \"OC20_S2EF_val_ood_both is the out-of-domain validation set of the OC20 \"\n",
    "    \"Structure to Energy and Forces (S2EF) dataset featuring both unseen catalyst \"\n",
    "    \"composition and unseen adsorbate. Features include energy, \"\n",
    "    \"atomic forces and data from the OC20 mappings file, including \"\n",
    "    \"adsorbate id, materials project bulk id and miller index.\"\n",
    ")\n",
    "\n",
    "self_arguments = [\n",
    "    \"configuration_set_ids\",\n",
    "    \"authors\",\n",
    "    \"description\",\n",
    "    \"data_license\",\n",
    "    \"publication_link\",\n",
    "    \"data_link\",\n",
    "    \"other_links\",\n",
    "    \"name\",\n",
    "    \"publication_year\",\n",
    "    \"doi\",\n",
    "]\n",
    "\n",
    "# Creating the namedtuple\n",
    "dataset = namedtuple(\"dataset\", self_arguments)\n",
    "\n",
    "# Example usage\n",
    "ds1 = dataset(\n",
    "    configuration_set_ids=[],\n",
    "    authors=AUTHORS,\n",
    "    description=DESCRIPTION,\n",
    "    data_license=LICENSE,\n",
    "    publication_link=PUBLICATION,\n",
    "    data_link=DATA_LINK,\n",
    "    other_links=None,\n",
    "    name=DATASET_NAME,\n",
    "    publication_year=PUBLICATION_YEAR,\n",
    "    doi=None,\n",
    ")\n",
    "\n",
    "print(ds1)\n",
    "import pyspark.sql.functions as sf\n",
    "from colabfit.tools.utilities import unstring_df_val\n",
    "from colabfit.tools.dataset import *\n",
    "from colabfit.tools.schema import *\n",
    "\n",
    "cos = spark.table(\"ndb.colabfit.dev.co_wip\").filter(\n",
    "    sf.col(\"dataset_ids\").contains(\"DS_889euoe7akyy_0\")\n",
    ")\n",
    "pos = spark.table(\"ndb.colabfit.dev.po_wip\").filter(\n",
    "    sf.col(\"dataset_id\") == \"DS_889euoe7akyy_0\"\n",
    ")\n",
    "\n",
    "# carray_cols = [f.name for f in config_df_schema if f.dataType.typeName() == \"array\"]\n",
    "\n",
    "# cschema_type_dict = {f.name: f.dataType for f in config_df_schema}\n",
    "# cstring_cols = [f.name for f in config_df_schema if f.dataType.typeName() == \"array\"]\n",
    "\n",
    "# for col in cstring_cols:\n",
    "#     string_col_udf = sf.udf(unstring_df_val, cschema_type_dict[col])\n",
    "#     cos = cos.withColumn(col, string_col_udf(sf.col(col)))\n",
    "\n",
    "\n",
    "ro = to_spark_row(ds1, cos, pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_df = pos\n",
    "config_df = cos\n",
    "from colabfit.tools.utilities import _empty_dict_from_schema\n",
    "\n",
    "self = ds1\n",
    "\n",
    "\n",
    "row_dict = _empty_dict_from_schema(dataset_schema)\n",
    "row_dict[\"last_modified\"] = dateutil.parser.parse(\n",
    "    datetime.datetime.now(tz=datetime.timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    ")\n",
    "row_dict[\"nconfiguration_sets\"] = len(self.configuration_set_ids)\n",
    "config_df = config_df.select(\n",
    "    \"id\",\n",
    "    \"elements\",\n",
    "    \"atomic_numbers\",\n",
    "    \"nsites\",\n",
    "    \"nperiodic_dimensions\",\n",
    "    \"dimension_types\",\n",
    "    # \"labels\",\n",
    ")\n",
    "row_dict[\"nsites\"] = config_df.agg({\"nsites\": \"sum\"}).first()[0]\n",
    "row_dict[\"elements\"] = sorted(\n",
    "    config_df.select(\"elements\")\n",
    "    .withColumn(\"exploded_elements\", sf.explode(\"elements\"))\n",
    "    .agg(sf.collect_set(\"exploded_elements\").alias(\"exploded_elements\"))\n",
    "    .select(\"exploded_elements\")\n",
    "    .take(1)[0][0]\n",
    ")\n",
    "row_dict[\"nelements\"] = len(row_dict[\"elements\"])\n",
    "atomic_ratios_df = config_df.select(\"atomic_numbers\").withColumn(\n",
    "    \"single_element\", sf.explode(\"atomic_numbers\")\n",
    ")\n",
    "total_elements = atomic_ratios_df.count()\n",
    "print(total_elements, row_dict[\"nsites\"])\n",
    "assert total_elements == row_dict[\"nsites\"]\n",
    "atomic_ratios_df = atomic_ratios_df.groupBy(\"single_element\").count()\n",
    "atomic_ratios_df = atomic_ratios_df.withColumn(\"ratio\", sf.col(\"count\") / total_elements)\n",
    "atomic_ratios_coll = (\n",
    "    atomic_ratios_df.withColumn(\n",
    "        \"element\",\n",
    "        sf.udf(lambda x: ELEMENT_MAP[x], StringType())(sf.col(\"single_element\")),\n",
    "    )\n",
    "    .select(\"element\", \"ratio\")\n",
    "    .collect()\n",
    ")\n",
    "row_dict[\"total_elements_ratios\"] = [\n",
    "    x[1] for x in sorted(atomic_ratios_coll, key=lambda x: x[\"element\"])\n",
    "]\n",
    "row_dict[\"nperiodic_dimensions\"] = config_df.agg(\n",
    "    sf.collect_set(\"nperiodic_dimensions\")\n",
    ").collect()[0][0]\n",
    "row_dict[\"dimension_types\"] = (\n",
    "    config_df.select(\"dimension_types\")\n",
    "    .agg(sf.collect_set(\"dimension_types\"))\n",
    "    .collect()[0][0]\n",
    ")\n",
    "\n",
    "nproperty_objects = prop_df.count()\n",
    "row_dict[\"nproperty_objects\"] = nproperty_objects\n",
    "for prop in [\n",
    "    \"atomization_energy\",\n",
    "    \"adsorption_energy\",\n",
    "    \"electronic_band_gap\",\n",
    "    \"cauchy_stress\",\n",
    "    \"formation_energy\",\n",
    "    \"energy\",\n",
    "]:\n",
    "    row_dict[f\"{prop}_count\"] = prop_df.select(prop).where(f\"{prop} is not null\").count()\n",
    "\n",
    "row_dict[f\"atomic_forces_count\"] = (\n",
    "    prop_df.select(\"atomic_forces_00\").filter(sf.col(\"atomic_forces_00\") != \"[]\").count()\n",
    ")\n",
    "prop = \"energy\"\n",
    "row_dict[f\"{prop}_variance\"] = (\n",
    "    prop_df.select(prop).where(f\"{prop} is not null\").agg(sf.variance(prop))\n",
    ").first()[0]\n",
    "row_dict[f\"{prop}_mean\"] = (\n",
    "    prop_df.select(prop).where(f\"{prop} is not null\").agg(sf.mean(prop))\n",
    ").first()[0]\n",
    "row_dict[\"nconfigurations\"] = config_df.count()\n",
    "row_dict[\"authors\"] = self.authors\n",
    "row_dict[\"description\"] = self.description\n",
    "row_dict[\"license\"] = self.data_license\n",
    "row_dict[\"links\"] = str(\n",
    "    {\n",
    "        \"source-publication\": self.publication_link,\n",
    "        \"source-data\": self.data_link,\n",
    "        \"other\": self.other_links,\n",
    "    }\n",
    ")\n",
    "row_dict[\"name\"] = self.name\n",
    "row_dict[\"publication_year\"] = self.publication_year\n",
    "row_dict[\"doi\"] = self.doi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cf-dev-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
