{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from itertools import chain, islice\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "with open(\"formation_energy.json\", \"r\") as f:\n",
    "    formation_energy_pd = json.load(f)\n",
    "import dateutil.parser\n",
    "import findspark\n",
    "import lmdb\n",
    "import numpy as np\n",
    "import pyspark\n",
    "from ase.atoms import Atoms\n",
    "\n",
    "from functools import partial\n",
    "from ase.io.cfg import read_cfg\n",
    "from dotenv import load_dotenv\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    BooleanType,\n",
    "    DoubleType,\n",
    "    FloatType,\n",
    "    IntegerType,\n",
    "    LongType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    "    TimestampType,\n",
    ")\n",
    "\n",
    "from colabfit.tools.configuration import AtomicConfiguration\n",
    "\n",
    "from colabfit.tools.database import DataManager, PGDataLoader\n",
    "from colabfit.tools.property import Property, property_object_schema\n",
    "from colabfit.tools.configuration import config_schema\n",
    "from colabfit.tools.property_definitions import (\n",
    "    atomic_forces_pd,\n",
    "    cauchy_stress_pd,\n",
    "    potential_energy_pd,\n",
    ")\n",
    "\n",
    "findspark.init()\n",
    "format = \"jdbc\"\n",
    "load_dotenv(\"./.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up MTPU and Carolina Materials readers and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_stress(keys, stress):\n",
    "    stresses = {k: s for k, s in zip(keys, stress)}\n",
    "    return [\n",
    "        [stresses[\"xx\"], stresses[\"xy\"], stresses[\"xz\"]],\n",
    "        [stresses[\"xy\"], stresses[\"yy\"], stresses[\"yz\"]],\n",
    "        [stresses[\"xz\"], stresses[\"yz\"], stresses[\"zz\"]],\n",
    "    ]\n",
    "\n",
    "\n",
    "SYMBOL_DICT = {\"0\": \"Si\", \"1\": \"O\"}\n",
    "\n",
    "\n",
    "def reader(filepath):\n",
    "    with open(filepath, \"rt\") as f:\n",
    "        energy = None\n",
    "        forces = None\n",
    "        coords = []\n",
    "        cell = []\n",
    "        symbols = []\n",
    "        config_count = 0\n",
    "        for line in f:\n",
    "            if line.strip().startswith(\"Size\"):\n",
    "                size = int(f.readline().strip())\n",
    "            elif line.strip().lower().startswith(\"supercell\"):\n",
    "                cell.append([float(x) for x in f.readline().strip().split()])\n",
    "                cell.append([float(x) for x in f.readline().strip().split()])\n",
    "                cell.append([float(x) for x in f.readline().strip().split()])\n",
    "            elif line.strip().startswith(\"Energy\"):\n",
    "                energy = float(f.readline().strip())\n",
    "            elif line.strip().startswith(\"PlusStress\"):\n",
    "                stress_keys = line.strip().split()[-6:]\n",
    "                stress = [float(x) for x in f.readline().strip().split()]\n",
    "                stress = convert_stress(stress_keys, stress)\n",
    "            elif line.strip().startswith(\"AtomData:\"):\n",
    "                keys = line.strip().split()[1:]\n",
    "                if \"fx\" in keys:\n",
    "                    forces = []\n",
    "                for i in range(size):\n",
    "                    li = {\n",
    "                        key: val for key, val in zip(keys, f.readline().strip().split())\n",
    "                    }\n",
    "                    symbols.append(SYMBOL_DICT[li[\"type\"]])\n",
    "                    if \"cartes_x\" in keys:\n",
    "                        coords.append(\n",
    "                            [\n",
    "                                float(c)\n",
    "                                for c in [\n",
    "                                    li[\"cartes_x\"],\n",
    "                                    li[\"cartes_y\"],\n",
    "                                    li[\"cartes_z\"],\n",
    "                                ]\n",
    "                            ]\n",
    "                        )\n",
    "                    elif \"direct_x\" in keys:\n",
    "                        coords.append(\n",
    "                            [\n",
    "                                float(c)\n",
    "                                for c in [\n",
    "                                    li[\"direct_x\"],\n",
    "                                    li[\"direct_y\"],\n",
    "                                    li[\"direct_z\"],\n",
    "                                ]\n",
    "                            ]\n",
    "                        )\n",
    "\n",
    "                    if \"fx\" in keys:\n",
    "                        forces.append(\n",
    "                            [float(f) for f in [li[\"fx\"], li[\"fy\"], li[\"fz\"]]]\n",
    "                        )\n",
    "\n",
    "            elif line.startswith(\"END_CFG\"):\n",
    "                if \"cartes_x\" in keys:\n",
    "                    config = AtomicConfiguration(\n",
    "                        positions=coords, symbols=symbols, cell=cell\n",
    "                    )\n",
    "                elif \"direct_x\" in keys:\n",
    "                    config = AtomicConfiguration(\n",
    "                        scaled_positions=coords, symbols=symbols, cell=cell\n",
    "                    )\n",
    "                config.info[\"energy\"] = energy\n",
    "                if forces:\n",
    "                    config.info[\"forces\"] = forces\n",
    "                config.info[\"stress\"] = stress\n",
    "\n",
    "                if \"Si\" in symbols and \"O\" in symbols:\n",
    "                    config.info[\"input\"] = {\n",
    "                        \"kpoint-scheme\": \"Monkhorst-Pack\",\n",
    "                        \"kpoints\": \"11x11x11\",\n",
    "                        \"kinetic-energy-cutoff\": {\n",
    "                            \"val\": 1224,\n",
    "                            \"units\": \"eV\",\n",
    "                        },\n",
    "                    }\n",
    "                    config.info[\"_name\"] = f\"{filepath.stem}_SiO2_{config_count}\"\n",
    "                elif \"Si\" in symbols:\n",
    "                    config.info[\"input\"] = {\n",
    "                        \"kpoint-scheme\": \"Monkhorst-Pack\",\n",
    "                        \"kpoints\": \"8x8x8\",\n",
    "                        \"kinetic-energy-cutoff\": {\n",
    "                            \"val\": 884,\n",
    "                            \"units\": \"eV\",\n",
    "                        },\n",
    "                    }\n",
    "                    config.info[\"_name\"] = f\"{filepath.stem}_Si_{config_count}\"\n",
    "                elif \"O\" in symbols:\n",
    "                    config.info[\"input\"] = {\n",
    "                        \"kpoint-scheme\": \"Monkhorst-Pack\",\n",
    "                        \"kpoints\": \"gamma-point\",\n",
    "                        \"kinetic-energy-cutoff\": {\n",
    "                            \"val\": 1224,\n",
    "                            \"units\": \"eV\",\n",
    "                        },\n",
    "                    }\n",
    "                    config.info[\"_name\"] = f\"{filepath.stem}_O_{config_count}\"\n",
    "                config_count += 1\n",
    "                yield config\n",
    "                forces = None\n",
    "                stress = []\n",
    "                coords = []\n",
    "                cell = []\n",
    "                symbols = []\n",
    "                energy = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nsites': 4,\n",
       " 'elements': ['Si'],\n",
       " 'nelements': 1,\n",
       " 'elements_ratios': [1.0],\n",
       " 'chemical_formula_anonymous': 'A',\n",
       " 'chemical_formula_reduced': 'Si',\n",
       " 'chemical_formula_hill': 'Si4',\n",
       " 'dimension_types': [0, 0, 0],\n",
       " 'nperiodic_dimensions': 0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtpu_configs = reader(Path(\"data/mtpu_2023/Unified_training_set.cfg\"))\n",
    "data = [x for x in mtpu_configs]\n",
    "data[0].configuration_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AtomicConfiguration(symbols='Si4', pbc=False, cell=[[3.85085, 0.0, 0.077017], [-1.925425, 3.334933, -0.038508], [0.127258, 0.0, 6.362934]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOFTWARE = \"VASP\"\n",
    "METHODS = \"DFT-PBE\"\n",
    "CM_PI_METADATA = {\n",
    "    \"software\": {\"value\": SOFTWARE},\n",
    "    \"method\": {\"value\": METHODS},\n",
    "    \"input\": {\"value\": {\"IBRION\": 6, \"NFREE\": 4}},\n",
    "}\n",
    "\n",
    "CM_PROPERTY_MAP = {\n",
    "    \"formation-energy\": [\n",
    "        {\n",
    "            \"energy\": {\"field\": \"energy\", \"units\": \"eV\"},\n",
    "            \"per-atom\": {\"value\": False, \"units\": None},\n",
    "        }\n",
    "    ],\n",
    "    \"_metadata\": CM_PI_METADATA,\n",
    "}\n",
    "CO_MD = {\n",
    "    key: {\"field\": key}\n",
    "    for key in [\n",
    "        \"_symmetry_space_group_name_H-M\",\n",
    "        \"_symmetry_Int_Tables_number\",\n",
    "        \"_chemical_formula_structural\",\n",
    "        \"_chemical_formula_sum\",\n",
    "        \"_cell_volume\",\n",
    "        \"_cell_formula_units_Z\",\n",
    "        \"symmetry_dict\",\n",
    "        \"formula_pretty\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "def load_row(txn, row):\n",
    "    try:\n",
    "        data = pickle.loads(txn.get(f\"{row}\".encode(\"ascii\")))\n",
    "        return data\n",
    "    except TypeError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def config_from_row(row: dict, row_num: int):\n",
    "    coords = row.pop(\"cart_coords\")\n",
    "    a_num = row.pop(\"atomic_numbers\")\n",
    "    cell = [\n",
    "        row.pop(x)\n",
    "        for x in [\n",
    "            \"_cell_length_a\",\n",
    "            \"_cell_length_b\",\n",
    "            \"_cell_length_c\",\n",
    "            \"_cell_angle_alpha\",\n",
    "            \"_cell_angle_beta\",\n",
    "            \"_cell_angle_gamma\",\n",
    "        ]\n",
    "    ]\n",
    "    config = AtomicConfiguration(scaled_positions=coords, numbers=a_num, cell=cell)\n",
    "    symmetry_dict = {str(key): val for key, val in row.pop(\"symmetry_dict\").items()}\n",
    "    for key in symmetry_dict:\n",
    "        key = str(key)\n",
    "    config.info = row\n",
    "    config.info[\"symmetry_dict\"] = symmetry_dict\n",
    "    config.info[\"_name\"] = f\"carolina_materials_{row_num}\"\n",
    "    return config\n",
    "    # return AtomicConfiguration.from_ase(config)\n",
    "\n",
    "\n",
    "def carmat_reader(fp: Path):\n",
    "    parent = fp.parent\n",
    "    env = lmdb.open(str(parent))\n",
    "    txn = env.begin()\n",
    "    row_num = 0\n",
    "    rows = []\n",
    "    while row_num <= 10000:\n",
    "        row = load_row(txn, row_num)\n",
    "        if row is False:\n",
    "            env.close()\n",
    "            break\n",
    "        rows.append(row)\n",
    "        yield config_from_row(row, row_num)\n",
    "        row_num += 1\n",
    "    env.close()\n",
    "    return False\n",
    "    # return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = carmat_reader(Path(\"data/carolina_matdb/base/all/data.mdb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "carmat_configs = list(carmat_reader(Path(\"data/carolina_matdb/base/all/data.mdb\")))\n",
    "carmat_config_gen = carmat_reader(Path(\"data/carolina_matdb/base/all/data.mdb\"))\n",
    "PI_METADATA = {\n",
    "    \"software\": {\"value\": \"Quantum ESPRESSO\"},\n",
    "    \"method\": {\"value\": \"DFT-PBE\"},\n",
    "    \"input\": {\"field\": \"input\"},\n",
    "}\n",
    "\n",
    "PROPERTY_MAP = {\n",
    "    \"potential-energy\": [\n",
    "        {\n",
    "            \"energy\": {\"field\": \"energy\", \"units\": \"eV\"},\n",
    "            \"per-atom\": {\"value\": False, \"units\": None},\n",
    "            # \"_metadata\": PI_METADATA,\n",
    "        }\n",
    "    ],\n",
    "    \"atomic-forces\": [\n",
    "        {\n",
    "            \"forces\": {\"field\": \"forces\", \"units\": \"eV/angstrom\"},\n",
    "            # \"_metadata\": PI_METADATA,\n",
    "        },\n",
    "    ],\n",
    "    \"cauchy-stress\": [\n",
    "        {\n",
    "            \"stress\": {\"field\": \"stress\", \"units\": \"GPa\"},\n",
    "            \"volume-normalized\": {\"value\": True, \"units\": None},\n",
    "        }\n",
    "    ],\n",
    "    \"_metadata\": PI_METADATA,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AtomicConfiguration(name=carolina_materials_0, symbols='BrCaH6Rh2', pbc=False, cell=[[5.39874426, 0.0, 0.0], [2.6993721300000004, 4.67544967769542, 0.0], [2.6993721300000004, 1.5584832258984738, 4.4080562295931855]])\n"
     ]
    }
   ],
   "source": [
    "print(carmat_configs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colabfit.tools.configuration import AtomicConfigurationOld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carmat_aconfigs = [\n",
    "#     AtomicConfiguration(co_md_map=CO_MD, names=\"test_name\").from_ase(c)\n",
    "#     for c in carmat_configs\n",
    "# ]\n",
    "# carmat_oconfigs = [\n",
    "#     AtomicConfigurationOld(names=f\"{i}\").from_ase(c)\n",
    "#     for i, c in enumerate(carmat_configs)\n",
    "# ]\n",
    "# for c in carmat_aconfigs:\n",
    "#     c.set_metadata(CO_MD)\n",
    "# len(carmat_aconfigs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to DB and run loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/17 16:17:44 WARN Utils: Your hostname, arktos resolves to a loopback address: 127.0.1.1; using 172.24.21.25 instead (on interface enp5s0)\n",
      "24/04/17 16:17:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/04/17 16:17:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/17 16:17:45 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "JARFILE = os.environ.get(\"CLASSPATH\")\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"PostgreSQL Connection with PySpark\")\n",
    "    .config(\"spark.jars\", JARFILE)\n",
    "    .getOrCreate()\n",
    ")\n",
    "url = \"jdbc:postgresql://localhost:5432/colabfit\"\n",
    "user = os.environ.get(\"PGS_USER\")\n",
    "password = os.environ.get(\"PGS_PASS\")\n",
    "properties = {\n",
    "    \"user\": user,\n",
    "    \"password\": password,\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "}\n",
    "loader = PGDataLoader(appname=\"colabfit\", env=\"./.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DataManager(\n",
    "    nprocs=4,\n",
    "    configs=carmat_config_gen,\n",
    "    prop_defs=[formation_energy_pd],\n",
    "    prop_map=CM_PROPERTY_MAP,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched(configs, n):\n",
    "    \"Batch data into tuples of length n. The last batch may be shorter.\"\n",
    "    if not isinstance(configs, GeneratorType):\n",
    "        configs = iter(configs)\n",
    "    while True:\n",
    "        batch = list(islice(configs, n))\n",
    "        if len(batch) == 0:\n",
    "            break\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import GeneratorType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "carmat_config_gen = carmat_reader(Path(\"data/carolina_matdb/base/all/data.mdb\"))\n",
    "chunk_size = 1000\n",
    "nprocs = 4\n",
    "config_chunks = batched(carmat_config_gen, chunk_size)\n",
    "ls = list(islice(config_chunks, nprocs))\n",
    "ls2 = list(islice(carmat_config_gen, nprocs))\n",
    "ls3 = list(islice(carmat_config_gen, nprocs))\n",
    "# b = batched(carmat_config_gen, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_po = dm.gather_co_po_in_batches()\n",
    "# co_po0 = co_po[0]\n",
    "# len(co_po0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object DataManager.gather_co_po_in_batches at 0x79ad84d2e020>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co_po"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type config chunks <class 'generator'>\n",
      "number of chunks 2\n",
      "10000\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot pickle 'generator' object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data_to_pg_in_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/colabfit-tools/colabfit/tools/database.py:227\u001b[0m, in \u001b[0;36mDataManager.load_data_to_pg_in_batches\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load data to PostgreSQL database in batches.\"\"\"\u001b[39;00m\n\u001b[1;32m    225\u001b[0m co_po_rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_co_po_in_batches()\n\u001b[0;32m--> 227\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m co_po_batch \u001b[38;5;129;01min\u001b[39;00m co_po_rows:\n\u001b[1;32m    228\u001b[0m     co_rows, po_rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mco_po_batch))\n\u001b[1;32m    229\u001b[0m     loader\u001b[38;5;241m.\u001b[39mwrite_table(\n\u001b[1;32m    230\u001b[0m         co_rows,\n\u001b[1;32m    231\u001b[0m         _CONFIGS_COLLECTION,\n\u001b[1;32m    232\u001b[0m         config_schema,\n\u001b[1;32m    233\u001b[0m     )\n",
      "File \u001b[0;32m~/colabfit-tools/colabfit/tools/database.py:221\u001b[0m, in \u001b[0;36mDataManager.gather_co_po_in_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather_co_po_rows_pool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpool\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/colabfit-tools/colabfit/tools/database.py:192\u001b[0m, in \u001b[0;36mDataManager.gather_co_po_rows_pool\u001b[0;34m(self, config_chunks, pool)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(config_chunks[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m    187\u001b[0m part_gather \u001b[38;5;241m=\u001b[39m partial(\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gather_co_po_rows,\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprop_defs,\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprop_map,\n\u001b[1;32m    191\u001b[0m )\n\u001b[0;32m--> 192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mfrom_iterable(\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpart_gather\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_chunks\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/multiprocessing/pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    363\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/multiprocessing/pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 774\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/multiprocessing/pool.py:540\u001b[0m, in \u001b[0;36mPool._handle_tasks\u001b[0;34m(taskqueue, put, outqueue, pool, cache)\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 540\u001b[0m     \u001b[43mput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    542\u001b[0m     job, idx \u001b[38;5;241m=\u001b[39m task[:\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/multiprocessing/connection.py:205\u001b[0m, in \u001b[0;36m_ConnectionBase.send\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_writable()\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_bytes(\u001b[43m_ForkingPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/multiprocessing/reduction.py:51\u001b[0m, in \u001b[0;36mForkingPickler.dumps\u001b[0;34m(cls, obj, protocol)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdumps\u001b[39m(\u001b[38;5;28mcls\u001b[39m, obj, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     50\u001b[0m     buf \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO()\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buf\u001b[38;5;241m.\u001b[39mgetbuffer()\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot pickle 'generator' object"
     ]
    }
   ],
   "source": [
    "dm.load_data_to_pg_in_batches(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cos, pos = zip(*co_po0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint(cos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint(pos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dateutil.parser.parse(\n",
    "#     datetime.datetime.now(tz=datetime.timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# outer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Can we make the configuration and the property instance/data object at the same time?\n",
    "In this way, we would only have to pass through the data one time.\n",
    "\n",
    "Workflow:\n",
    "create database access object\n",
    "create data reader as function? of the database access object\n",
    "reader returns ase.Atoms-style objects (AtomicConfiguration)\n",
    "DOs and PIs are now one object\n",
    "These DOs point to a configuration\n",
    "The configuration may already exist in the database, so we keep track of the hash added to the DO\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = json.load(Path(\"sample_db/co_ds1.json\").open(\"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(\"sample_db/co_ds1.json\"), \"r\") as f:\n",
    "    co_json = spark.sparkContext.parallelize(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = co_json.map(_parse_config).map(stringify_lists)\n",
    "co_df = spark.createDataFrame(co, config_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_configs(co_path, spark):\n",
    "    with open(co_path, \"r\") as f:\n",
    "        co_json = spark.sparkContext.parallelize(json.load(f))\n",
    "    co = co_json.map(_parse_config).map(stringify_lists)\n",
    "    co_df = spark.createDataFrame(co, config_schema)\n",
    "    return co_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_configs(\"sample_db/co_ds1.json\", spark).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"co\"\n",
    "\n",
    "mode = \"append\"\n",
    "url = \"jdbc:postgresql://localhost:5432/colabfit\"\n",
    "properties = {\"user\": user, \"password\": password, \"driver\": \"org.postgresql.Driver\"}\n",
    "co_df.write.jdbc(url=url, table=table_name, mode=mode, properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# co_df.write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cf-dev-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
