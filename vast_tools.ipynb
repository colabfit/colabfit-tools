{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import json\n",
    "from time import time\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from functools import partial\n",
    "# from itertools import chain, islice\n",
    "# from multiprocessing import Pool, cpu_count\n",
    "from pathlib import Path\n",
    "\n",
    "# from pprint import pprint\n",
    "\n",
    "import dateutil.parser\n",
    "import findspark\n",
    "import lmdb\n",
    "import numpy as np\n",
    "import psycopg\n",
    "import pyspark.sql.functions as sf\n",
    "from ase.atoms import Atoms\n",
    "from ase.io.cfg import read_cfg\n",
    "from dotenv import load_dotenv\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    ArrayType,\n",
    "    BooleanType,\n",
    "    DoubleType,\n",
    "    FloatType,\n",
    "    IntegerType,\n",
    "    LongType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    "    TimestampType,\n",
    ")\n",
    "from colabfit.tools.schema import (\n",
    "    property_object_schema,\n",
    "    config_df_schema,\n",
    "    config_schema,\n",
    "    property_object_df_schema,\n",
    ")\n",
    "from colabfit.tools.configuration import AtomicConfiguration, config_schema\n",
    "from colabfit.tools.database import DataManager, PGDataLoader\n",
    "from colabfit.tools.dataset import Dataset, dataset_schema\n",
    "from colabfit.tools.property import Property, property_object_schema\n",
    "from colabfit.tools.property_definitions import (\n",
    "    atomic_forces_pd,\n",
    "    cauchy_stress_pd,\n",
    "    potential_energy_pd,\n",
    ")\n",
    "from colabfit.tools.schema import configuration_set_schema\n",
    "import pyarrow as pa\n",
    "\n",
    "with open(\"formation_energy.json\", \"r\") as f:\n",
    "    formation_energy_pd = json.load(f)\n",
    "findspark.init()\n",
    "format = \"jdbc\"\n",
    "load_dotenv(\"./.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up MTPU and Carolina Materials readers and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MTPU data\n",
    "\n",
    "\n",
    "def convert_stress(keys, stress):\n",
    "    stresses = {k: s for k, s in zip(keys, stress)}\n",
    "    return [\n",
    "        [stresses[\"xx\"], stresses[\"xy\"], stresses[\"xz\"]],\n",
    "        [stresses[\"xy\"], stresses[\"yy\"], stresses[\"yz\"]],\n",
    "        [stresses[\"xz\"], stresses[\"yz\"], stresses[\"zz\"]],\n",
    "    ]\n",
    "\n",
    "\n",
    "SYMBOL_DICT = {\"0\": \"Si\", \"1\": \"O\"}\n",
    "\n",
    "\n",
    "def mtpu_reader(filepath):\n",
    "    with open(filepath, \"rt\") as f:\n",
    "        energy = None\n",
    "        forces = None\n",
    "        coords = []\n",
    "        cell = []\n",
    "        symbols = []\n",
    "        config_count = 0\n",
    "        info = dict()\n",
    "        for line in f:\n",
    "            if line.strip().startswith(\"Size\"):\n",
    "                size = int(f.readline().strip())\n",
    "            elif line.strip().lower().startswith(\"supercell\"):\n",
    "                cell.append([float(x) for x in f.readline().strip().split()])\n",
    "                cell.append([float(x) for x in f.readline().strip().split()])\n",
    "                cell.append([float(x) for x in f.readline().strip().split()])\n",
    "            elif line.strip().startswith(\"Energy\"):\n",
    "                energy = float(f.readline().strip())\n",
    "            elif line.strip().startswith(\"PlusStress\"):\n",
    "                stress_keys = line.strip().split()[-6:]\n",
    "                stress = [float(x) for x in f.readline().strip().split()]\n",
    "                stress = convert_stress(stress_keys, stress)\n",
    "            elif line.strip().startswith(\"AtomData:\"):\n",
    "                keys = line.strip().split()[1:]\n",
    "                if \"fx\" in keys:\n",
    "                    forces = []\n",
    "                for i in range(size):\n",
    "                    li = {\n",
    "                        key: val for key, val in zip(keys, f.readline().strip().split())\n",
    "                    }\n",
    "                    symbols.append(SYMBOL_DICT[li[\"type\"]])\n",
    "                    if \"cartes_x\" in keys:\n",
    "                        coords.append(\n",
    "                            [\n",
    "                                float(c)\n",
    "                                for c in [\n",
    "                                    li[\"cartes_x\"],\n",
    "                                    li[\"cartes_y\"],\n",
    "                                    li[\"cartes_z\"],\n",
    "                                ]\n",
    "                            ]\n",
    "                        )\n",
    "                    elif \"direct_x\" in keys:\n",
    "                        coords.append(\n",
    "                            [\n",
    "                                float(c)\n",
    "                                for c in [\n",
    "                                    li[\"direct_x\"],\n",
    "                                    li[\"direct_y\"],\n",
    "                                    li[\"direct_z\"],\n",
    "                                ]\n",
    "                            ]\n",
    "                        )\n",
    "\n",
    "                    if \"fx\" in keys:\n",
    "                        forces.append(\n",
    "                            [float(f) for f in [li[\"fx\"], li[\"fy\"], li[\"fz\"]]]\n",
    "                        )\n",
    "\n",
    "            elif line.startswith(\"END_CFG\"):\n",
    "\n",
    "                info[\"energy\"] = energy\n",
    "                if forces:\n",
    "                    info[\"forces\"] = forces\n",
    "                info[\"stress\"] = stress\n",
    "\n",
    "                if \"Si\" in symbols and \"O\" in symbols:\n",
    "                    info[\"input\"] = {\n",
    "                        \"kpoint-scheme\": \"Monkhorst-Pack\",\n",
    "                        \"kpoints\": \"11x11x11\",\n",
    "                        \"kinetic-energy-cutoff\": {\n",
    "                            \"val\": 1224,\n",
    "                            \"units\": \"eV\",\n",
    "                        },\n",
    "                    }\n",
    "                    info[\"_name\"] = f\"{filepath.stem}_SiO2_{config_count}\"\n",
    "                elif \"Si\" in symbols:\n",
    "                    info[\"input\"] = {\n",
    "                        \"kpoint-scheme\": \"Monkhorst-Pack\",\n",
    "                        \"kpoints\": \"8x8x8\",\n",
    "                        \"kinetic-energy-cutoff\": {\n",
    "                            \"val\": 884,\n",
    "                            \"units\": \"eV\",\n",
    "                        },\n",
    "                    }\n",
    "                    info[\"_name\"] = f\"{filepath.stem}_Si_{config_count}\"\n",
    "                elif \"O\" in symbols:\n",
    "                    info[\"input\"] = {\n",
    "                        \"kpoint-scheme\": \"Monkhorst-Pack\",\n",
    "                        \"kpoints\": \"gamma-point\",\n",
    "                        \"kinetic-energy-cutoff\": {\n",
    "                            \"val\": 1224,\n",
    "                            \"units\": \"eV\",\n",
    "                        },\n",
    "                    }\n",
    "                    info[\"_name\"] = f\"{filepath.stem}_O_{config_count}\"\n",
    "                if \"cartes_x\" in keys:\n",
    "                    config = AtomicConfiguration(\n",
    "                        positions=coords, symbols=symbols, cell=cell, info=info\n",
    "                    )\n",
    "                elif \"direct_x\" in keys:\n",
    "                    config = AtomicConfiguration(\n",
    "                        scaled_positions=coords, symbols=symbols, cell=cell, info=info\n",
    "                    )\n",
    "                config_count += 1\n",
    "                yield config\n",
    "                forces = None\n",
    "                stress = []\n",
    "                coords = []\n",
    "                cell = []\n",
    "                symbols = []\n",
    "                energy = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtpu_configs = mtpu_reader(Path(\"data/mtpu_2023/Unified_training_set.cfg\"))\n",
    "data = list(mtpu_configs)\n",
    "# data = [x for x in mtpu_configs]\n",
    "# data[0].configuration_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import colabfit.tools.configuration\n",
    "from importlib import reload\n",
    "\n",
    "reload(colabfit.tools.configuration)\n",
    "AtomicConfiguration = colabfit.tools.configuration.AtomicConfiguration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carolina Materials data\n",
    "\n",
    "SOFTWARE = \"VASP\"\n",
    "METHODS = \"DFT-PBE\"\n",
    "CM_PI_METADATA = {\n",
    "    \"software\": {\"value\": SOFTWARE},\n",
    "    \"method\": {\"value\": METHODS},\n",
    "    \"input\": {\"value\": {\"IBRION\": 6, \"NFREE\": 4}},\n",
    "}\n",
    "\n",
    "CM_PROPERTY_MAP = {\n",
    "    \"formation-energy\": [\n",
    "        {\n",
    "            \"energy\": {\"field\": \"energy\", \"units\": \"eV\"},\n",
    "            \"per-atom\": {\"value\": False, \"units\": None},\n",
    "        }\n",
    "    ],\n",
    "    \"_metadata\": CM_PI_METADATA,\n",
    "}\n",
    "CO_MD = {\n",
    "    key: {\"field\": key}\n",
    "    for key in [\n",
    "        \"_symmetry_space_group_name_H-M\",\n",
    "        \"_symmetry_Int_Tables_number\",\n",
    "        \"_chemical_formula_structural\",\n",
    "        \"_chemical_formula_sum\",\n",
    "        \"_cell_volume\",\n",
    "        \"_cell_formula_units_Z\",\n",
    "        \"symmetry_dict\",\n",
    "        \"formula_pretty\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "def load_row(txn, row):\n",
    "    try:\n",
    "        data = pickle.loads(txn.get(f\"{row}\".encode(\"ascii\")))\n",
    "        return data\n",
    "    except TypeError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def config_from_row(row: dict, row_num: int):\n",
    "    coords = row.pop(\"cart_coords\")\n",
    "    a_num = row.pop(\"atomic_numbers\")\n",
    "    cell = [\n",
    "        row.pop(x)\n",
    "        for x in [\n",
    "            \"_cell_length_a\",\n",
    "            \"_cell_length_b\",\n",
    "            \"_cell_length_c\",\n",
    "            \"_cell_angle_alpha\",\n",
    "            \"_cell_angle_beta\",\n",
    "            \"_cell_angle_gamma\",\n",
    "        ]\n",
    "    ]\n",
    "    symmetry_dict = {str(key): val for key, val in row.pop(\"symmetry_dict\").items()}\n",
    "    for key in symmetry_dict:\n",
    "        key = str(key)\n",
    "    info = {}\n",
    "    info = row\n",
    "    info[\"symmetry_dict\"] = symmetry_dict\n",
    "    info[\"_name\"] = f\"carolina_materials_{row_num}\"\n",
    "    if row_num % 10 == 0:\n",
    "        info[\"_labels\"] = [row_num % 10, \"bcc\"]\n",
    "    else:\n",
    "        info[\"_labels\"] = [row_num % 10, \"fcc\"]\n",
    "    config = AtomicConfiguration(\n",
    "        scaled_positions=coords,\n",
    "        numbers=a_num,\n",
    "        cell=cell,\n",
    "        info=info,\n",
    "    )\n",
    "    return config\n",
    "    # return AtomicConfiguration.from_ase(config)\n",
    "\n",
    "\n",
    "def carmat_reader(fp: Path):\n",
    "    parent = fp.parent\n",
    "    env = lmdb.open(str(parent))\n",
    "    txn = env.begin()\n",
    "    row_num = 0\n",
    "    rows = []\n",
    "    while row_num <= 100000:\n",
    "        row = load_row(txn, row_num)\n",
    "        if row is False:\n",
    "            env.close()\n",
    "            break\n",
    "        rows.append(row)\n",
    "        yield config_from_row(row, row_num)\n",
    "        row_num += 1\n",
    "    env.close()\n",
    "    return False\n",
    "    # return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "PI_METADATA = {\n",
    "    \"software\": {\"value\": \"Quantum ESPRESSO\"},\n",
    "    \"method\": {\"value\": \"DFT-PBE\"},\n",
    "    \"input\": {\"field\": \"input\"},\n",
    "}\n",
    "PROPERTY_MAP = {\n",
    "    \"potential-energy\": [\n",
    "        {\n",
    "            \"energy\": {\"field\": \"energy\", \"units\": \"eV\"},\n",
    "            \"per-atom\": {\"value\": False, \"units\": None},\n",
    "            # \"_metadata\": PI_METADATA,\n",
    "        }\n",
    "    ],\n",
    "    \"atomic-forces\": [\n",
    "        {\n",
    "            \"forces\": {\"field\": \"forces\", \"units\": \"eV/angstrom\"},\n",
    "            # \"_metadata\": PI_METADATA,\n",
    "        },\n",
    "    ],\n",
    "    \"cauchy-stress\": [\n",
    "        {\n",
    "            \"stress\": {\"field\": \"stress\", \"units\": \"GPa\"},\n",
    "            \"volume-normalized\": {\"value\": True, \"units\": None},\n",
    "        }\n",
    "    ],\n",
    "    \"_metadata\": PI_METADATA,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to DB and run loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/21 14:49:53 WARN Utils: Your hostname, arktos resolves to a loopback address: 127.0.1.1; using 172.24.21.25 instead (on interface enp5s0)\n",
      "24/05/21 14:49:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/05/21 14:49:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/21 14:49:55 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "JARFILE = os.environ.get(\"CLASSPATH\")\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"PostgreSQL Connection with PySpark\")\n",
    "    .config(\"spark.jars\", JARFILE)\n",
    "    .getOrCreate()\n",
    ")\n",
    "url = \"jdbc:postgresql://localhost:5432/colabfit\"\n",
    "user = os.environ.get(\"PGS_USER\")\n",
    "password = os.environ.get(\"PGS_PASS\")\n",
    "properties = {\n",
    "    \"user\": user,\n",
    "    \"password\": password,\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "}\n",
    "loader = PGDataLoader(appname=\"colabfit\", env=\"./.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/20 17:14:41 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AtomicConfiguration(name=Unified_training_set_SiO2_1061, symbols='Si4', pbc=False, cell=[[3.85085, 0.0, 0.077017], [-1.925425, 3.334933, -0.038508], [0.127258, 0.0, 6.362934]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1062/1062 [00:00<00:00, 2495.36it/s]\n"
     ]
    }
   ],
   "source": [
    "mtpu_configs = mtpu_reader(Path(\"data/mtpu_2023/Unified_training_set.cfg\"))\n",
    "\n",
    "PI_METADATA = {\n",
    "    \"software\": {\"value\": \"Quantum ESPRESSO\"},\n",
    "    \"method\": {\"value\": \"DFT-PBE\"},\n",
    "    \"input\": {\"field\": \"input\"},\n",
    "}\n",
    "PROPERTY_MAP = {\n",
    "    \"potential-energy\": [\n",
    "        {\n",
    "            \"energy\": {\"field\": \"energy\", \"units\": \"eV\"},\n",
    "            \"per-atom\": {\"value\": False, \"units\": None},\n",
    "            # \"_metadata\": PI_METADATA,\n",
    "        }\n",
    "    ],\n",
    "    \"atomic-forces\": [\n",
    "        {\n",
    "            \"forces\": {\"field\": \"forces\", \"units\": \"eV/angstrom\"},\n",
    "            # \"_metadata\": PI_METADATA,\n",
    "        },\n",
    "    ],\n",
    "    \"cauchy-stress\": [\n",
    "        {\n",
    "            \"stress\": {\"field\": \"stress\", \"units\": \"GPa\"},\n",
    "            \"volume-normalized\": {\"value\": True, \"units\": None},\n",
    "        }\n",
    "    ],\n",
    "    \"_metadata\": PI_METADATA,\n",
    "}\n",
    "spark = SparkSession.builder.appName(\"ColabfitIngestData\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "# loader = SparkDataLoader(table_prefix=\"ndb.colabfit.dev\")\n",
    "# print(loader.spark)\n",
    "mtpu_ds_id = \"DS_y7nrdsjtuwom_0\"\n",
    "mtpu_configs = list(mtpu_configs)\n",
    "print(mtpu_configs[0])\n",
    "co_po_rows = []\n",
    "for config in tqdm(mtpu_configs):\n",
    "    config.set_dataset_id(mtpu_ds_id)\n",
    "    co_po_rows.append(\n",
    "        (\n",
    "            config.spark_row,\n",
    "            Property.from_definition(\n",
    "                [potential_energy_pd, atomic_forces_pd, cauchy_stress_pd],\n",
    "                configuration=config,\n",
    "                property_map=PROPERTY_MAP,\n",
    "            ).spark_row,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_df = loader.spark.read.jdbc(\n",
    "    url=url, table=\"configurations\", properties=properties\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colabfit.tools.database import batched\n",
    "\n",
    "\n",
    "def gather_co_po_in_batches_no_pool(self):\n",
    "    chunk_size = 10000\n",
    "    config_chunks = batched(self.configs, chunk_size)\n",
    "    for chunk in config_chunks:\n",
    "        yield list(\n",
    "            self._gather_co_po_rows(\n",
    "                self.prop_defs, self.prop_map, self.dataset_id, chunk\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colabfit.tools.utilities import (\n",
    "    add_elem_to_row_dict,\n",
    "    unstringify_row_dict,\n",
    "    stringify_row_dict,\n",
    "    get_spark_field_type,\n",
    "    spark_schema_to_arrow_schema,\n",
    "    arrow_record_batch_to_rdd,\n",
    ")\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def append_ith_element_to_rdd(row_elem):\n",
    "    \"\"\"\n",
    "    row_elem: tuple created by joining two RDD.zipWithIndex\n",
    "    new_co_ids: list of configuration ids\n",
    "    \"\"\"\n",
    "    (index, (po_row, new_co_ids)) = row_elem\n",
    "    val = po_row.get(\"configuration_ids\")\n",
    "    if val is None:\n",
    "        val = new_co_ids\n",
    "    else:\n",
    "        val.extend(new_co_ids)\n",
    "        val = list(set(val))\n",
    "    po_row[\"configuration_ids\"] = val\n",
    "    return po_row\n",
    "\n",
    "\n",
    "def find_existing_pos_append_elems(\n",
    "    self,\n",
    "    rdd,\n",
    "    table_name: str,\n",
    "    ids: list[str],\n",
    "    cols: list[str],\n",
    "    elems: list[str],\n",
    "    write_schema: StructType,\n",
    "):\n",
    "    if isinstance(cols, str):\n",
    "        cols = [cols]\n",
    "    if isinstance(elems, str):\n",
    "        elems = [elems]\n",
    "    col_types = {\"id\": StringType(), \"$row_id\": IntegerType()}\n",
    "    for col in cols:\n",
    "        col_types[col] = get_spark_field_type(write_schema, col)\n",
    "    update_cols = [col for col in col_types if col != \"id\"]\n",
    "    query_schema = StructType(\n",
    "        [\n",
    "            StructField(col, col_types[col], False)\n",
    "            for i, col in enumerate(cols + [\"id\", \"$row_id\"])\n",
    "        ]\n",
    "    )\n",
    "    partial_batch_to_rdd = partial(arrow_record_batch_to_rdd, query_schema)\n",
    "    batched_ids = batched(ids, 10000)\n",
    "    new_ids = []\n",
    "    existing_ids = []\n",
    "    for id_batch in batched_ids:\n",
    "        id_batch = list(set(id_batch))\n",
    "        # We only have to use vastdb-sdk here bc we need the '$row_id' column\n",
    "        with self.session.transaction() as tx:\n",
    "            # string would be 'ndb.colabfit.dev.[table name]'\n",
    "            table_path = table_name.split(\".\")\n",
    "            table = tx.bucket(table_path[1]).schema(table_path[2]).table(table_path[3])\n",
    "            rec_batch = table.select(\n",
    "                predicate=table[\"id\"].isin(id_batch),\n",
    "                columns=cols + [\"id\"],\n",
    "                internal_row_id=True,\n",
    "            )\n",
    "            rec_batch = rec_batch.read_all()\n",
    "            rdd = self.spark.sparkContext.parallelize(\n",
    "                list(partial_batch_to_rdd(rec_batch))\n",
    "            )\n",
    "            print(f\"length of rdd: {rdd.count()}\")\n",
    "        rdd = rdd.map(unstringify_row_dict)\n",
    "        for col, elem in zip(cols, elems):\n",
    "            # Add 'labels' to this?\n",
    "            if col == \"configuration_ids\":\n",
    "                po_co_id_map = rdd.map(lambda x: {x[\"id\"]: x[col][0]}).collect()\n",
    "                rdd = rdd.zipWithIndex()\n",
    "                co_ids = rdd.map(lambda x: po_co_id_map[x[\"id\"]]).zipWithIndex()\n",
    "                rdd = rdd.join(co_ids).map(append_ith_element_to_rdd)\n",
    "\n",
    "            else:\n",
    "                partial_add = partial(add_elem_to_row_dict, col, elem)\n",
    "                rdd = rdd.map(partial_add)\n",
    "        existing_ids_batch = rdd.map(lambda x: x[\"id\"]).collect()\n",
    "        new_ids_batch = [id for id in id_batch if id not in existing_ids_batch]\n",
    "        rdd = rdd.map(stringify_row_dict)\n",
    "        rdd_collect = rdd.map(lambda x: [x[col] for col in update_cols]).collect()\n",
    "        update_schema = StructType(\n",
    "            [StructField(col, col_types[col], False) for col in update_cols]\n",
    "        )\n",
    "        arrow_schema = spark_schema_to_arrow_schema(update_schema)\n",
    "        update_table = pa.table(\n",
    "            [pa.array(col) for col in zip(*rdd_collect)], schema=arrow_schema\n",
    "        )\n",
    "        with self.session.transaction() as tx:\n",
    "            table = tx.bucket(table_path[1]).schema(table_path[2]).table(table_path[3])\n",
    "            table.update(rows=update_table)\n",
    "        new_ids.extend(new_ids_batch)\n",
    "        existing_ids.extend(existing_ids_batch)\n",
    "\n",
    "    return (new_ids, list(set(existing_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_po_rdd(po_rdd):\n",
    "    po_co_ids = (\n",
    "        po_rdd.map(lambda x: (x[\"id\"], x[\"configuration_ids\"][0]))\n",
    "        .groupByKey()\n",
    "        .mapValues(list)\n",
    "    )\n",
    "    po_id_map = po_co_ids.collectAsMap()\n",
    "    broadcast_map = spark.sparkContext.broadcast(po_id_map)\n",
    "\n",
    "    def replace_id_val(row):\n",
    "        row[\"configuration_ids\"] = broadcast_map.value[row[\"id\"]]\n",
    "        return row\n",
    "\n",
    "    po_rdd = po_rdd.map(replace_id_val)\n",
    "    po_rdd = (\n",
    "        po_rdd.map(lambda x: (x[\"id\"], x))\n",
    "        .reduceByKey(lambda a, b: a)\n",
    "        .map(lambda x: x[1])\n",
    "    )\n",
    "    return po_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atomic_ratios_df = (\n",
    "    config_df.select(\"atomic_numbers\")\n",
    "    .withColumn(\"exploded_atom\", sf.explode(\"atomic_numbers\"))\n",
    "    .groupBy(sf.col(\"exploded_atom\").alias(\"atomic_number\"))\n",
    "    .show()\n",
    "    .count()\n",
    "    .withColumn(\"ratio\", sf.col(\"count\") / row_dict[\"nsites\"])\n",
    "    .select(\"ratio\", \"atomic_number\")\n",
    "    .withColumn(\n",
    "        \"element\",\n",
    "        sf.udf(lambda x: ELEMENT_MAP[x], StringType())(sf.col(\"atomic_number\")),\n",
    "    )\n",
    "    .select(\"element\", \"ratio\")\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colabfit.tools.utilities import unstringify\n",
    "\n",
    "batches = dm.gather_co_po_in_batches_no_pool()\n",
    "batch1 = next(batches)\n",
    "cos, pos = zip(*batch1)\n",
    "pos_rdd = spark.sparkContext.parallelize(pos)\n",
    "print(pos_rdd.count())\n",
    "pos_rdd_reduced = reduce_po_rdd(pos_rdd)\n",
    "print(pos_rdd_reduced.count())\n",
    "\n",
    "po = loader.read_table(loader.prop_object_table)\n",
    "po_rdd = po.rdd.map(unstringify).map(lambda x: x.asDict())\n",
    "po_co_id_map = (\n",
    "    po_rdd.map(lambda x: (x[\"id\"], x[\"configuration_ids\"][0]))\n",
    "    .groupByKey()\n",
    "    .mapValues(list)\n",
    "    .collect()\n",
    ")\n",
    "po_co_id_map = dict(po_co_id_map)\n",
    "co_ids = po_rdd.map(lambda x: po_co_id_map[x[\"id\"]]).zipWithIndex()\n",
    "co_ids = co_ids.map(lambda x: (x[1], x[0]))\n",
    "rdd = po_rdd.zipWithIndex()\n",
    "rdd = rdd.map(lambda x: (x[1], x[0]))\n",
    "joined_rdd = rdd.join(co_ids).map(append_ith_element_of_list_to_spark_rdd_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "\n",
    "import colabfit.tools.utilities\n",
    "import colabfit.tools.dataset\n",
    "import colabfit.tools.database\n",
    "import colabfit.tools.configuration_set\n",
    "\n",
    "reload(colabfit.tools.utilities)\n",
    "reload(colabfit.tools.dataset)\n",
    "reload(colabfit.tools.database)\n",
    "DataManager = colabfit.tools.database.DataManager\n",
    "ConfigurationSet = colabfit.tools.configuration_set.ConfigurationSet\n",
    "Dataset = colabfit.tools.dataset.Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.rdd  \n",
    "you can only parallelize one time so don't try to do a dataframe select from an rdd  \n",
    "updating to sdk 5.1 in a couple weeks  \n",
    "boto3 and s3 are the amazon file system interactions, mostly for adding metadata TO FILES (not to the database) and interacting with the files as FileExistsError. \n",
    "Make sure to spark.stop() at end of  python file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'carmat_reader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m carmat_config_gen \u001b[38;5;241m=\u001b[39m \u001b[43mcarmat_reader\u001b[49m(Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/carolina_matdb/base/all/data.mdb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      2\u001b[0m carmat_ds_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDS_y7nrdsjtuw0g_0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# carmat_ds_id2 = \"duplicate_ds_id\"\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'carmat_reader' is not defined"
     ]
    }
   ],
   "source": [
    "carmat_config_gen = carmat_reader(Path(\"data/carolina_matdb/base/all/data.mdb\"))\n",
    "carmat_ds_id = \"DS_y7nrdsjtuw0g_0\"\n",
    "dm = DataManager(\n",
    "    nprocs=4,\n",
    "    configs=carmat_config_gen,\n",
    "    prop_defs=[formation_energy_pd],\n",
    "    prop_map=CM_PROPERTY_MAP,\n",
    "    dataset_id=carmat_ds_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ID: DS_y7nrdsjtuwom_0\n"
     ]
    }
   ],
   "source": [
    "mtpu_ds_id = \"DS_y7nrdsjtuwom_0\"\n",
    "mtpu_configs = mtpu_reader(Path(\"data/mtpu_2023/Unified_training_set.cfg\"))\n",
    "dm2 = DataManager(\n",
    "    nprocs=4,\n",
    "    configs=mtpu_configs,\n",
    "    prop_defs=[potential_energy_pd, atomic_forces_pd, cauchy_stress_pd],\n",
    "    prop_map=PROPERTY_MAP,\n",
    "    dataset_id=mtpu_ds_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(dm.gather_co_po_in_batches())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_co_rows_cs_id(self, co_ids: list[str], cs_id: str):\n",
    "    with psycopg.connect(\n",
    "        \"\"\"dbname=colabfit user=%s password=%s host=localhost port=5432\"\"\"\n",
    "        % (\n",
    "            user,\n",
    "            password,\n",
    "        )\n",
    "    ) as conn:\n",
    "        # dbname=self.database_name,\n",
    "        # user=self.properties[\"user\"],\n",
    "        # password=self.properties[\"password\"],\n",
    "        # host=\"localhost\",\n",
    "        # port=\"5432\",\n",
    "        cur = conn.execute(\n",
    "            \"\"\"UPDATE configurations\n",
    "                SET configuration_set_ids = \n",
    "            \"\"\"\n",
    "        )\n",
    "        cur = conn.execute(\n",
    "            \"\"\"UPDATE configurations\n",
    "                SET configuration_set_ids = concat(%s::text, \n",
    "                rtrim(ltrim(replace(configuration_set_ids,%s,''), \n",
    "                \n",
    "                '['),']') || ', ', %s::text)\n",
    "            WHERE id = ANY(%s)\"\"\",\n",
    "            (\"[\", f\"{cs_id}\", f\"{cs_id}]\", co_ids),\n",
    "            # (\"[\", f\", {cs_id}\", f\", {cs_id}]\"),\n",
    "        )\n",
    "        # cur.fetchall()\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You were trying to get  postgresql to recognize the WHERE id = ANY() array syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "with psycopg.connect(\n",
    "    dbname=\"colabfit\",\n",
    "    user=os.environ.get(\"PGS_USER\"),\n",
    "    password=os.environ.get(\"PGS_PASS\"),\n",
    "    host=\"localhost\",\n",
    ") as conn:\n",
    "    with conn.cursor() as cur:\n",
    "\n",
    "        # cur.execute(\n",
    "        #     \"UPDATE configurations SET configuration_set_ids = configuration_set_ids || %(cs_id)s WHERE id = ANY(%(co_ids)s)\",\n",
    "        #     {\"cs_id\": cs[\"id\"], \"co_ids\": co_ids},\n",
    "        # )\n",
    "        # data = cur.fetchall()\n",
    "        cur.execute(\n",
    "            \"SELECT * FROM public.configurations WHERE id = ANY(%s)\",\n",
    "            [co_ids],\n",
    "        )\n",
    "        data2 = cur.fetchall()\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upsert appears to be this for postgres:\n",
    "```\n",
    "update the_table\n",
    "    set id = id || array[5,6]\n",
    "where id = 4;\n",
    "```\n",
    "* ~~Check for upsert function from pyspark to concatenate lists of relationships instead of primary key id collision~~\n",
    "* There is no pyspark-upsert function. Will have to manage this possibly through a different sql-based library\n",
    "* Written: find duplicates, but convert to access database, not download full dataframe\n",
    "* I see this being used with batches of hashes during upload: something like\n",
    "    ``` for batch in batches:\n",
    "            hash_duplicates = find_duplicates(batch, loader/database)\n",
    "            hash_duplicates.make_change_to_append_dataset-ids\n",
    "            hash_duplicates.write-to-database\n",
    "* Where would be the best place to catch duplicates? Keeping in mind that this might be a bulk operation (i.e. on the order of millions, like with ANI1/ANI2x variations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/30 09:52:06 WARN Utils: Your hostname, arktos resolves to a loopback address: 127.0.1.1; using 172.24.21.25 instead (on interface enp5s0)\n",
      "24/05/30 09:52:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/05/30 09:52:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/30 09:52:08 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "JARFILE = os.environ.get(\"CLASSPATH\")\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"PostgreSQL Connection with PySpark\")\n",
    "    .config(\"spark.jars\", JARFILE)\n",
    "    .getOrCreate()\n",
    ")\n",
    "url = \"jdbc:postgresql://localhost:5432/colabfit\"\n",
    "user = os.environ.get(\"PGS_USER\")\n",
    "password = os.environ.get(\"PGS_PASS\")\n",
    "properties = {\n",
    "    \"user\": user,\n",
    "    \"password\": password,\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "}\n",
    "loader = PGDataLoader(appname=\"colabfit\", env=\"./.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtpu_ds_id = \"DS_y7nrdsjtuwom_0\"\n",
    "mtpu_configs = mtpu_reader(Path(\"data/mtpu_2023/Unified_training_set.cfg\"))\n",
    "dm2 = DataManager(\n",
    "    nprocs=4,\n",
    "    configs=mtpu_configs,\n",
    "    prop_defs=[potential_energy_pd, atomic_forces_pd, cauchy_stress_pd],\n",
    "    prop_map=PROPERTY_MAP,\n",
    "    dataset_id=mtpu_ds_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "\n",
    "def write_value_to_file(path_prefix, extension, BUCKET_DIR, write_column, row):\n",
    "    \"\"\"i.e.: partial(_write_value(\n",
    "    'CO/positions',\n",
    "    'txt',\n",
    "    '/save/here'\n",
    "    'positions',\n",
    "    )\n",
    "    \"\"\"\n",
    "    id = row[\"id\"]\n",
    "    value = row[write_column]\n",
    "    row_dict = row.copy()\n",
    "    split = id[-4:]\n",
    "    filename = f\"{id}.{extension}\"\n",
    "    full_path = Path(BUCKET_DIR) / path_prefix / split / filename\n",
    "    full_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    full_path.write_text(str(value))\n",
    "    # row_dict = row.asDict()\n",
    "    row_dict[write_column] = str(full_path)\n",
    "    return Row(**row_dict)\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "part_write = partial(\n",
    "    write_value_to_file,\n",
    "    \"CO/positions\",\n",
    "    \"txt\",\n",
    "    \"/scratch/gw2338/vast/data-lake-main/spark/scripts\",\n",
    "    \"positions\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = mtpu_reader(Path(\"data/mtpu_2023/Unified_training_set.cfg\"))\n",
    "co_rows = [x.spark_row for x in configs]\n",
    "rdd = sc.parallelize(co_rows)\n",
    "rdd.foreachPartition(part_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_list = list(mtpu_reader(Path(\"data/mtpu_2023/Unified_training_set.cfg\")))\n",
    "dm2.configs = config_list[:50]\n",
    "dm2.load_co_po_to_vastdb(loader)\n",
    "dm2.configs = config_list[25:]\n",
    "dm2.load_co_po_to_vastdb(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "\n",
    "import colabfit.tools.utilities\n",
    "import colabfit.tools.dataset\n",
    "import colabfit.tools.database\n",
    "import colabfit.tools.configuration_set\n",
    "\n",
    "reload(colabfit.tools.utilities)\n",
    "reload(colabfit.tools.dataset)\n",
    "reload(colabfit.tools.database)\n",
    "DataManager = colabfit.tools.database.DataManager\n",
    "ConfigurationSet = colabfit.tools.configuration_set.ConfigurationSet\n",
    "Dataset = colabfit.tools.dataset.Dataset\n",
    "##############\n",
    "\n",
    "import json\n",
    "import lmdb\n",
    "import pickle\n",
    "from colabfit.tools.database import DataManager, SparkDataLoader\n",
    "\n",
    "loader = SparkDataLoader(table_prefix=\"ndb.colabfit.dev\")\n",
    "load_dotenv()\n",
    "access_key = os.getenv(\"SPARK_ID\")\n",
    "access_secret = os.getenv(\"SPARK_KEY\")\n",
    "endpoint = os.getenv(\"SPARK_ENDPOINT\")\n",
    "loader.set_vastdb_session(\n",
    "    endpoint=endpoint, access_key=access_key, access_secret=access_secret\n",
    ")\n",
    "\n",
    "with open(\"formation_energy.json\", \"r\") as f:\n",
    "    formation_energy_pd = json.load(f)\n",
    "\n",
    "carmat_config_gen = carmat_reader(Path(\"data/carolina_matdb/base/all/data.mdb\"))\n",
    "carmat_ds_id = \"DS_y7nrdsjtuw0g_0\"\n",
    "\n",
    "\n",
    "dm = DataManager(\n",
    "    nprocs=1,\n",
    "    configs=carmat_config_gen,\n",
    "    prop_defs=[formation_energy_pd],\n",
    "    prop_map=CM_PROPERTY_MAP,\n",
    "    dataset_id=carmat_ds_id,\n",
    ")\n",
    "dm.configs = carmat_reader(Path(\"data/carolina_matdb/base/all/data.mdb\"))\n",
    "\n",
    "match = [\n",
    "    (r\".*3.*\", None, \"3_configurations\", \"Carmat with 3\"),\n",
    "    (r\".*4.*\", None, \"4_configurations\", \"Carmat with 4\"),\n",
    "]\n",
    "# dm.load_co_po_to_vastdb(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = dm2.gather_co_po_in_batches()\n",
    "batch = next(batches)\n",
    "cos, pos = zip(*batch)\n",
    "rdd = loader.spark.sparkContext.parallelize(cos)\n",
    "ids_coll = rdd.map(lambda x: x[\"id\"]).collect()\n",
    "loader.spark.read.table(loader.config_table).select(sf.col(\"id\")).filter(\n",
    "    sf.col(\"id\").isin(broadcast_ids.value)\n",
    ").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colabfit.tools.utilities import unstringify\n",
    "from colabfit.tools.schema import (\n",
    "    configuration_set_df_schema,\n",
    "    dataset_df_schema,\n",
    "    property_object_df_schema,\n",
    "    config_df_schema,\n",
    ")\n",
    "\n",
    "\n",
    "def read_table(self, table_name: str, unstring: bool = False):\n",
    "    \"\"\"\n",
    "    Include self.table_prefix in the table name when passed to this function.\n",
    "    Ex: loader.read_table(loader.config_table, unstring=True)\n",
    "    Arguments:\n",
    "        table_name {str} -- Name of the table to read from database\n",
    "    Keyword Arguments:\n",
    "        unstring {bool} -- Convert stringified lists to lists (default: {False})\n",
    "    Returns:\n",
    "        DataFrame -- Spark DataFrame\n",
    "    \"\"\"\n",
    "    schema_dict = {\n",
    "        self.config_table: config_df_schema,\n",
    "        self.config_set_table: configuration_set_df_schema,\n",
    "        self.dataset_table: dataset_df_schema,\n",
    "        self.prop_object_table: property_object_df_schema,\n",
    "    }\n",
    "    if unstring:\n",
    "        df = self.spark.read.table(table_name)\n",
    "        return df.rdd.map(unstringify).toDF(schema_dict[table_name])\n",
    "    else:\n",
    "        return self.spark.read.table(table_name)\n",
    "\n",
    "\n",
    "def get_pos_cos_by_filter(self, filter_conditions):\n",
    "    po_df = self.read_table(self.prop_object_table, unstring=True).withColumnRenamed(\n",
    "        \"id\", \"po_id\"\n",
    "    )\n",
    "    po_df = po_df.withColumn(\n",
    "        \"configuration_id\", sf.explode(sf.col(\"configuration_ids\"))\n",
    "    ).drop(\"configuration_ids\")\n",
    "    co_df = self.read_table(self.config_table, unstring=True).withColumnRenamed(\n",
    "        \"id\", \"co_id\"\n",
    "    )\n",
    "    for i, (column, operand, condition) in enumerate(filter_conditions):\n",
    "        if operand == \"in\":\n",
    "            po_df = po_df.filter(sf.col(column).isin(condition))\n",
    "        elif operand == \"like\":\n",
    "            po_df = po_df.filter(sf.col(column).like(condition))\n",
    "        elif operand == \"rlike\":\n",
    "            po_df = po_df.filter(sf.col(column).rlike(condition))\n",
    "        elif operand == \"==\":\n",
    "            po_df = po_df.filter(sf.col(column) == condition)\n",
    "        elif operand == \"array_contains\":\n",
    "            po_df = po_df.filter(sf.array_contains(sf.col(column), condition))\n",
    "        elif operand == \">\":\n",
    "            po_df = po_df.filter(sf.col(column) > condition)\n",
    "        elif operand == \"<\":\n",
    "            po_df = po_df.filter(sf.col(column) < condition)\n",
    "        elif operand == \">=\":\n",
    "            po_df = po_df.filter(sf.col(column) >= condition)\n",
    "        elif operand == \"<=\":\n",
    "            po_df = po_df.filter(sf.col(column) <= condition)\n",
    "        else:\n",
    "            raise ValueError(f\"Operand {operand} not implemented in get_pos_cos_filter\")\n",
    "    co_po_df = co_df.join(po_df, co_df[\"co_id\"] == po_df[\"configuration_id\"], \"inner\")\n",
    "    return co_po_df\n",
    "\n",
    "\n",
    "get_pos_cos_by_filter(\n",
    "    loader, [(\"dataset_ids\", \"array_contains\", mtpu_ds_id), (\"method\", \"like\", \"DFT%\")]\n",
    ")\n",
    "df = get_pos_cos_by_filter(\n",
    "    loader,\n",
    "    [\n",
    "        (\"dataset_ids\", \"array_contains\", mtpu_ds_id),\n",
    "        (\"method\", \"like\", \"DFT%\"),\n",
    "        (\"potential_energy\", \"<\", -56729.0),\n",
    "    ],\n",
    "    [(\"nsites\", \">=\", 63)],\n",
    ")\n",
    "\n",
    "\n",
    "def get_pos_cos_by_filter(\n",
    "    self,\n",
    "    po_filter_conditions: list[tuple],\n",
    "    co_filter_conditions: list[tuple] = None,\n",
    "):\n",
    "    po_df = self.read_table(self.prop_object_table, unstring=True).withColumnRenamed(\n",
    "        \"id\", \"po_id\"\n",
    "    )\n",
    "    po_df = po_df.withColumn(\n",
    "        \"configuration_id\", sf.explode(sf.col(\"configuration_ids\"))\n",
    "    ).drop(\"configuration_ids\")\n",
    "    co_df = self.read_table(self.config_table, unstring=True).withColumnRenamed(\n",
    "        \"id\", \"co_id\"\n",
    "    )\n",
    "    po_df = get_filtered_table(self, po_df, po_filter_conditions)\n",
    "    if co_filter_conditions is not None:\n",
    "        co_df = get_filtered_table(self, co_df, co_filter_conditions)\n",
    "    co_po_df = co_df.join(po_df, co_df[\"co_id\"] == po_df[\"configuration_id\"], \"inner\")\n",
    "    return co_po_df\n",
    "\n",
    "\n",
    "def get_filtered_table(\n",
    "    self, df: DataFrame, filter_conditions: list[tuple[str, str, str]]\n",
    "):\n",
    "    for i, (column, operand, condition) in enumerate(filter_conditions):\n",
    "        if operand == \"in\":\n",
    "            df = df.filter(sf.col(column).isin(condition))\n",
    "        elif operand == \"like\":\n",
    "            df = df.filter(sf.col(column).like(condition))\n",
    "        elif operand == \"rlike\":\n",
    "            df = df.filter(sf.col(column).rlike(condition))\n",
    "        elif operand == \"==\":\n",
    "            df = df.filter(sf.col(column) == condition)\n",
    "        elif operand == \"array_contains\":\n",
    "            df = df.filter(sf.array_contains(sf.col(column), condition))\n",
    "        elif operand == \">\":\n",
    "            df = df.filter(sf.col(column) > condition)\n",
    "        elif operand == \"<\":\n",
    "            df = df.filter(sf.col(column) < condition)\n",
    "        elif operand == \">=\":\n",
    "            df = df.filter(sf.col(column) >= condition)\n",
    "        elif operand == \"<=\":\n",
    "            df = df.filter(sf.col(column) <= condition)\n",
    "        else:\n",
    "            raise ValueError(f\"Operand {operand} not implemented in get_pos_cos_filter\")\n",
    "    return df\n",
    "\n",
    "\n",
    "df1 = read_filter_table(loader, [(\"id\", \"==\", \"CO_47706510123393079\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = mtpu_ds_id\n",
    "name_label_match = [\n",
    "    (\".*Si.*3.*\", None, \"All_si_with_zero\", \"All Si with zero description\"),\n",
    "    (\".*Si.*4.*\", None, \"All_si_with_two\", \"All Si with two description\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin = time()\n",
    "dm.create_configuration_sets(loader, match)\n",
    "end = time()\n",
    "print(f\"Time elapsed: {end - begin}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_configuration_sets(\n",
    "    self,\n",
    "    loader,\n",
    "    # below args in order:\n",
    "    # [config-name-regex-pattern], [config-label-regex-pattern], \\\n",
    "    # [config-set-name], [config-set-description]\n",
    "    name_label_match: list[tuple],\n",
    "):\n",
    "    config_set_rows = []\n",
    "    # Load unstrung dataframe of configs, filter for just includes ds-id\n",
    "    config_df = loader.read_table(table_name=loader.config_table, unstring=True)\n",
    "    config_df = config_df.filter(\n",
    "        sf.array_contains(sf.col(\"dataset_ids\"), self.dataset_id)\n",
    "    )\n",
    "    for i, (names_match, label_match, cs_name, cs_desc) in tqdm(\n",
    "        enumerate(name_label_match), desc=\"Creating Configuration Sets\"\n",
    "    ):\n",
    "        print(\n",
    "            f\"names match: {names_match}, label {label_match}, cs_name {cs_name}, cs_desc {cs_desc}\"\n",
    "        )\n",
    "        if names_match:\n",
    "            config_set_query = config_df.withColumn(\n",
    "                \"names_exploded\", sf.explode(sf.col(\"names\"))\n",
    "            ).filter(sf.col(\"names_exploded\").rlike(names_match))\n",
    "        # Currently an AND operation on labels: labels col contains x AND y\n",
    "        if label_match is not None:\n",
    "            if isinstance(label_match, str):\n",
    "                label_match = [label_match]\n",
    "            for label in label_match:\n",
    "                config_set_query = config_set_query.filter(\n",
    "                    sf.array_contains(sf.col(\"labels\"), label)\n",
    "                )\n",
    "        co_ids = [x[\"id\"] for x in config_set_query.select(\"id\").distinct().collect()]\n",
    "        loader.find_existing_rows_append_elem(\n",
    "            table_name=loader.config_table,\n",
    "            ids=co_ids,\n",
    "            cols=\"configuration_set_ids\",\n",
    "            elems=cs_name,\n",
    "            edit_schema=config_df_schema,\n",
    "            write_schema=config_schema,\n",
    "        )\n",
    "        config_set = ConfigurationSet(\n",
    "            name=cs_name,\n",
    "            description=cs_desc,\n",
    "            config_df=config_set_query,\n",
    "            dataset_id=self.dataset_id,\n",
    "        )\n",
    "        config_set_rows.append(config_set.spark_row)\n",
    "    loader.write_table(\n",
    "        config_set_rows, loader.config_set_table, schema=configuration_set_schema\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colabfit.tools.utilities import _write_value\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_write = partial(\n",
    "    _write_value,\n",
    "    \"CO/positions\",\n",
    "    \"txt\",\n",
    "    \"/scratch/gw2338/vast/data-lake-main/spark/scripts\",\n",
    "    \"positions\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from schema import dataset_schema\n",
    "from colabfit.tools.dataset import Dataset\n",
    "\n",
    "\n",
    "def create_dataset(\n",
    "    self,\n",
    "    loader,\n",
    "    name: str,\n",
    "    authors: list[str],\n",
    "    publication_link: str,\n",
    "    data_link: str,\n",
    "    description: str,\n",
    "    other_links: list[str] = None,\n",
    "    dataset_id: str = None,\n",
    "    labels: list[str] = None,\n",
    "    data_license: str = \"CC-BY-ND-4.0\",\n",
    "):\n",
    "    cs_ids = loader.read_table(loader.config_set_table).select(\"id\").collect()\n",
    "    if len(cs_ids) == 0:\n",
    "        cs_ids = None\n",
    "    else:\n",
    "        cs_ids = [x[\"id\"] for x in cs_ids]\n",
    "    config_df = loader.read_table(loader.config_table, unstring=True)\n",
    "    config_df = config_df.filter(sf.array_contains(sf.col(\"dataset_ids\"), dataset_id))\n",
    "    prop_df = loader.read_table(loader.prop_object_table, unstring=True)\n",
    "    prop_df = prop_df.filter(sf.array_contains(sf.col(\"dataset_ids\"), dataset_id))\n",
    "    ds = Dataset(\n",
    "        name=name,\n",
    "        authors=authors,\n",
    "        config_df=config_df,\n",
    "        prop_df=prop_df,\n",
    "        publication_link=publication_link,\n",
    "        data_link=data_link,\n",
    "        description=description,\n",
    "        other_links=other_links,\n",
    "        dataset_id=dataset_id,\n",
    "        labels=labels,\n",
    "        data_license=data_license,\n",
    "        configuration_set_ids=cs_ids,\n",
    "    )\n",
    "    loader.write_table([ds.spark_row], loader.dataset_table, schema=dataset_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "create_dataset(\n",
    "    dm,\n",
    "    loader,\n",
    "    \"carolina_materials\",\n",
    "    [\"author one\", \"author two\"],\n",
    "    \"https://www.carolina_materials.com\",\n",
    "    \"https://www.carolina_materials.com/data\",\n",
    "    \"Carolina Materials is a ... description\",\n",
    "    dataset_id=dm.dataset_id,\n",
    ")\n",
    "print(f\"Time elapsed: {time() - t0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "\n",
    "import colabfit.tools.dataset\n",
    "import colabfit.tools.database\n",
    "import colabfit.tools.configuration_set\n",
    "import colabfit.tools.schema\n",
    "\n",
    "reload(colabfit.tools.configuration_set)\n",
    "reload(colabfit.tools.dataset)\n",
    "reload(colabfit.tools.database)\n",
    "reload(colabfit.tools.schema)\n",
    "configuration_set_schema = colabfit.tools.schema.configuration_set_schema\n",
    "DataManager = colabfit.tools.database.DataManager\n",
    "ConfigurationSet = colabfit.tools.configuration_set.ConfigurationSet\n",
    "Dataset = colabfit.tools.dataset.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_duplicate_hash(spark_rows: dict, loader):\n",
    "    # hashes = loader.spark.createDataFrame([x[\"hash\"] for x in spark_rows])\n",
    "    hashes = [x[\"hash\"] for x in spark_rows]\n",
    "    duplicates = loader.spark.read.jdbc(\n",
    "        url=url,\n",
    "        table=\"configurations\",\n",
    "        properties=properties,\n",
    "    ).filter(sf.col(\"hash\").isin(hashes))\n",
    "    # dupl_hashes = df.filter(df.hash.isin(hashes)).select(\"hash\").collect()\n",
    "    return duplicates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cf-dev-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
