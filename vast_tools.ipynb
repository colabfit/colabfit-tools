{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import json\n",
    "from time import time\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from functools import partial\n",
    "# from itertools import chain, islice\n",
    "# from multiprocessing import Pool, cpu_count\n",
    "from pathlib import Path\n",
    "\n",
    "# from pprint import pprint\n",
    "\n",
    "import dateutil.parser\n",
    "import findspark\n",
    "import lmdb\n",
    "import numpy as np\n",
    "import psycopg\n",
    "import pyspark.sql.functions as sf\n",
    "from ase.atoms import Atoms\n",
    "from ase.io.cfg import read_cfg\n",
    "from dotenv import load_dotenv\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    ArrayType,\n",
    "    BooleanType,\n",
    "    DoubleType,\n",
    "    FloatType,\n",
    "    IntegerType,\n",
    "    LongType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    "    TimestampType,\n",
    ")\n",
    "from colabfit.tools.schema import (\n",
    "    property_object_schema,\n",
    "    config_df_schema,\n",
    "    config_schema,\n",
    "    property_object_df_schema,\n",
    ")\n",
    "from colabfit.tools.configuration import AtomicConfiguration, config_schema\n",
    "from colabfit.tools.database import DataManager, PGDataLoader\n",
    "from colabfit.tools.dataset import Dataset, dataset_schema\n",
    "from colabfit.tools.property import Property, property_object_schema\n",
    "from colabfit.tools.property_definitions import (\n",
    "    atomic_forces_pd,\n",
    "    cauchy_stress_pd,\n",
    "    potential_energy_pd,\n",
    ")\n",
    "from colabfit.tools.schema import configuration_set_schema\n",
    "import pyarrow as pa\n",
    "\n",
    "with open(\"formation_energy.json\", \"r\") as f:\n",
    "    formation_energy_pd = json.load(f)\n",
    "findspark.init()\n",
    "format = \"jdbc\"\n",
    "load_dotenv(\"./.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import colabfit.tools.configuration\n",
    "from importlib import reload\n",
    "\n",
    "reload(colabfit.tools.configuration)\n",
    "AtomicConfiguration = colabfit.tools.configuration.AtomicConfiguration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to DB and run loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/21 14:49:53 WARN Utils: Your hostname, arktos resolves to a loopback address: 127.0.1.1; using 172.24.21.25 instead (on interface enp5s0)\n",
      "24/05/21 14:49:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/05/21 14:49:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/21 14:49:55 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "JARFILE = os.environ.get(\"CLASSPATH\")\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"PostgreSQL Connection with PySpark\")\n",
    "    .config(\"spark.jars\", JARFILE)\n",
    "    .getOrCreate()\n",
    ")\n",
    "url = \"jdbc:postgresql://localhost:5432/colabfit\"\n",
    "user = os.environ.get(\"PGS_USER\")\n",
    "password = os.environ.get(\"PGS_PASS\")\n",
    "properties = {\n",
    "    \"user\": user,\n",
    "    \"password\": password,\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "}\n",
    "loader = PGDataLoader(appname=\"colabfit\", env=\"./.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "from ase.io import iread\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "from colabfit.tools.configuration import AtomicConfiguration\n",
    "from colabfit.tools.database import DataManager, SparkDataLoader\n",
    "from colabfit.tools.property_definitions import (\n",
    "    atomic_forces_pd,\n",
    "    free_energy_pd,\n",
    "    potential_energy_pd,\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "loader = SparkDataLoader(table_prefix=\"ndb.colabfit.dev\")\n",
    "access_key = os.getenv(\"SPARK_ID\")\n",
    "access_secret = os.getenv(\"SPARK_KEY\")\n",
    "endpoint = os.getenv(\"SPARK_ENDPOINT\")\n",
    "# loader.set_vastdb_session(\n",
    "PKL_FP = Path(\"data/oc20_data_mapping.pkl\")\n",
    "with open(PKL_FP, \"rb\") as f:\n",
    "    OC20_MAP = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colabfit.tools.schema import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('atomic_forces_11', StringType(), True), StructField('atomic_forces_12', StringType(), True), StructField('atomic_forces_13', StringType(), True), StructField('atomic_forces_14', StringType(), True), StructField('atomic_forces_15', StringType(), True), StructField('atomic_forces_16', StringType(), True), StructField('atomic_forces_17', StringType(), True), StructField('atomic_forces_18', StringType(), True), StructField('atomic_forces_19', StringType(), True), StructField('atomic_forces_unit', StringType(), True), StructField('atomic_forces_property_id', StringType(), True), StructField('cauchy_stress', StringType(), True), StructField('cauchy_stress_unit', StringType(), True), StructField('cauchy_stress_volume_normalized', BooleanType(), True), StructField('cauchy_stress_property_id', StringType(), True), StructField('electronic_free_energy', DoubleType(), True), StructField('electronic_free_energy_unit', StringType(), True), StructField('electronic_free_energy_per_atom', BooleanType(), True), StructField('electronic_free_energy_reference', DoubleType(), True), StructField('electronic_free_energy_reference_unit', StringType(), True)])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema[40:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'string'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema[41].dataType.typeName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['atomic_forces_00', 'atomic_forces_01', 'atomic_forces_02', 'atomic_forces_03', 'atomic_forces_04', 'atomic_forces_05', 'atomic_forces_06', 'atomic_forces_07', 'atomic_forces_08', 'atomic_forces_09', 'atomic_forces_10', 'atomic_forces_11', 'atomic_forces_12', 'atomic_forces_13', 'atomic_forces_14', 'atomic_forces_15', 'atomic_forces_16', 'atomic_forces_17', 'atomic_forces_18', 'atomic_forces_19', 'cauchy_stress']\n"
     ]
    }
   ],
   "source": [
    "schema = property_object_df_schema\n",
    "string_cols = [f.name for f in schema if f.dataType.typeName() == \"array\"]\n",
    "print(string_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "\n",
    "import colabfit.tools.utilities\n",
    "import colabfit.tools.dataset\n",
    "import colabfit.tools.database\n",
    "import colabfit.tools.configuration_set\n",
    "import colabfit.tools.property\n",
    "\n",
    "reload(colabfit.tools.utilities)\n",
    "reload(colabfit.tools.dataset)\n",
    "reload(colabfit.tools.database)\n",
    "reload(colabfit.tools.property)\n",
    "DataManager = colabfit.tools.database.DataManager\n",
    "ConfigurationSet = colabfit.tools.configuration_set.ConfigurationSet\n",
    "Dataset = colabfit.tools.dataset.Dataset\n",
    "Property = colabfit.tools.property.Property"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.rdd  \n",
    "you can only parallelize one time so don't try to do a dataframe select from an rdd  \n",
    "updating to sdk 5.1 in a couple weeks  \n",
    "boto3 and s3 are the amazon file system interactions, mostly for adding metadata TO FILES (not to the database) and interacting with the files as FileExistsError. \n",
    "Make sure to spark.stop() at end of  python file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_co_rows_cs_id(self, co_ids: list[str], cs_id: str):\n",
    "    with psycopg.connect(\n",
    "        \"\"\"dbname=colabfit user=%s password=%s host=localhost port=5432\"\"\"\n",
    "        % (\n",
    "            user,\n",
    "            password,\n",
    "        )\n",
    "    ) as conn:\n",
    "        # dbname=self.database_name,\n",
    "        # user=self.properties[\"user\"],\n",
    "        # password=self.properties[\"password\"],\n",
    "        # host=\"localhost\",\n",
    "        # port=\"5432\",\n",
    "        cur = conn.execute(\n",
    "            \"\"\"UPDATE configurations\n",
    "                SET configuration_set_ids = \n",
    "            \"\"\"\n",
    "        )\n",
    "        cur = conn.execute(\n",
    "            \"\"\"UPDATE configurations\n",
    "                SET configuration_set_ids = concat(%s::text, \n",
    "                rtrim(ltrim(replace(configuration_set_ids,%s,''), \n",
    "                \n",
    "                '['),']') || ', ', %s::text)\n",
    "            WHERE id = ANY(%s)\"\"\",\n",
    "            (\"[\", f\"{cs_id}\", f\"{cs_id}]\", co_ids),\n",
    "            # (\"[\", f\", {cs_id}\", f\", {cs_id}]\"),\n",
    "        )\n",
    "        # cur.fetchall()\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You were trying to get  postgresql to recognize the WHERE id = ANY() array syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "with psycopg.connect(\n",
    "    dbname=\"colabfit\",\n",
    "    user=os.environ.get(\"PGS_USER\"),\n",
    "    password=os.environ.get(\"PGS_PASS\"),\n",
    "    host=\"localhost\",\n",
    ") as conn:\n",
    "    with conn.cursor() as cur:\n",
    "\n",
    "        # cur.execute(\n",
    "        #     \"UPDATE configurations SET configuration_set_ids = configuration_set_ids || %(cs_id)s WHERE id = ANY(%(co_ids)s)\",\n",
    "        #     {\"cs_id\": cs[\"id\"], \"co_ids\": co_ids},\n",
    "        # )\n",
    "        # data = cur.fetchall()\n",
    "        cur.execute(\n",
    "            \"SELECT * FROM public.configurations WHERE id = ANY(%s)\",\n",
    "            [co_ids],\n",
    "        )\n",
    "        data2 = cur.fetchall()\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upsert appears to be this for postgres:\n",
    "```\n",
    "update the_table\n",
    "    set id = id || array[5,6]\n",
    "where id = 4;\n",
    "```\n",
    "* ~~Check for upsert function from pyspark to concatenate lists of relationships instead of primary key id collision~~\n",
    "* There is no pyspark-upsert function. Will have to manage this possibly through a different sql-based library\n",
    "* Written: find duplicates, but convert to access database, not download full dataframe\n",
    "* I see this being used with batches of hashes during upload: something like\n",
    "    ``` for batch in batches:\n",
    "            hash_duplicates = find_duplicates(batch, loader/database)\n",
    "            hash_duplicates.make_change_to_append_dataset-ids\n",
    "            hash_duplicates.write-to-database\n",
    "* Where would be the best place to catch duplicates? Keeping in mind that this might be a bulk operation (i.e. on the order of millions, like with ANI1/ANI2x variations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/30 09:52:06 WARN Utils: Your hostname, arktos resolves to a loopback address: 127.0.1.1; using 172.24.21.25 instead (on interface enp5s0)\n",
      "24/05/30 09:52:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/05/30 09:52:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/30 09:52:08 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "JARFILE = os.environ.get(\"CLASSPATH\")\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"PostgreSQL Connection with PySpark\")\n",
    "    .config(\"spark.jars\", JARFILE)\n",
    "    .getOrCreate()\n",
    ")\n",
    "url = \"jdbc:postgresql://localhost:5432/colabfit\"\n",
    "user = os.environ.get(\"PGS_USER\")\n",
    "password = os.environ.get(\"PGS_PASS\")\n",
    "properties = {\n",
    "    \"user\": user,\n",
    "    \"password\": password,\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "}\n",
    "loader = PGDataLoader(appname=\"colabfit\", env=\"./.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtpu_ds_id = \"DS_y7nrdsjtuwom_0\"\n",
    "mtpu_configs = mtpu_reader(Path(\"data/mtpu_2023/Unified_training_set.cfg\"))\n",
    "dm2 = DataManager(\n",
    "    nprocs=4,\n",
    "    configs=mtpu_configs,\n",
    "    prop_defs=[potential_energy_pd, atomic_forces_pd, cauchy_stress_pd],\n",
    "    prop_map=PROPERTY_MAP,\n",
    "    dataset_id=mtpu_ds_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "\n",
    "def write_value_to_file(path_prefix, extension, BUCKET_DIR, write_column, row):\n",
    "    \"\"\"i.e.: partial(_write_value(\n",
    "    'CO/positions',\n",
    "    'txt',\n",
    "    '/save/here'\n",
    "    'positions',\n",
    "    )\n",
    "    \"\"\"\n",
    "    id = row[\"id\"]\n",
    "    value = row[write_column]\n",
    "    row_dict = row.copy()\n",
    "    split = id[-4:]\n",
    "    filename = f\"{id}.{extension}\"\n",
    "    full_path = Path(BUCKET_DIR) / path_prefix / split / filename\n",
    "    full_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    full_path.write_text(str(value))\n",
    "    # row_dict = row.asDict()\n",
    "    row_dict[write_column] = str(full_path)\n",
    "    return Row(**row_dict)\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "part_write = partial(\n",
    "    write_value_to_file,\n",
    "    \"CO/positions\",\n",
    "    \"txt\",\n",
    "    \"/scratch/gw2338/vast/data-lake-main/spark/scripts\",\n",
    "    \"positions\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = mtpu_reader(Path(\"data/mtpu_2023/Unified_training_set.cfg\"))\n",
    "co_rows = [x.spark_row for x in configs]\n",
    "rdd = sc.parallelize(co_rows)\n",
    "rdd.foreachPartition(part_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_list = list(mtpu_reader(Path(\"data/mtpu_2023/Unified_training_set.cfg\")))\n",
    "dm2.configs = config_list[:50]\n",
    "dm2.load_co_po_to_vastdb(loader)\n",
    "dm2.configs = config_list[25:]\n",
    "dm2.load_co_po_to_vastdb(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "\n",
    "import colabfit.tools.utilities\n",
    "import colabfit.tools.dataset\n",
    "import colabfit.tools.database\n",
    "import colabfit.tools.configuration_set\n",
    "import colabfit.tools.schema\n",
    "\n",
    "reload(colabfit.tools.utilities)\n",
    "reload(colabfit.tools.schema)\n",
    "reload(colabfit.tools.dataset)\n",
    "reload(colabfit.tools.database)\n",
    "DataManager = colabfit.tools.database.DataManager\n",
    "ConfigurationSet = colabfit.tools.configuration_set.ConfigurationSet\n",
    "Dataset = colabfit.tools.dataset.Dataset\n",
    "property_object_df_schema = colabfit.tools.schema.property_object_df_schema\n",
    "property_object_schema = colabfit.tools.schema.property_object_schema\n",
    "##############\n",
    "\n",
    "import json\n",
    "import lmdb\n",
    "import pickle\n",
    "from colabfit.tools.database import DataManager, SparkDataLoader\n",
    "\n",
    "loader = SparkDataLoader(table_prefix=\"ndb.colabfit.dev\")\n",
    "load_dotenv()\n",
    "access_key = os.getenv(\"SPARK_ID\")\n",
    "access_secret = os.getenv(\"SPARK_KEY\")\n",
    "endpoint = os.getenv(\"SPARK_ENDPOINT\")\n",
    "loader.set_vastdb_session(\n",
    "    endpoint=endpoint, access_key=access_key, access_secret=access_secret\n",
    ")\n",
    "\n",
    "with open(\"formation_energy.json\", \"r\") as f:\n",
    "    formation_energy_pd = json.load(f)\n",
    "\n",
    "carmat_config_gen = carmat_reader(Path(\"data/carolina_matdb/base/all/data.mdb\"))\n",
    "carmat_ds_id = \"DS_y7nrdsjtuw0g_0\"\n",
    "\n",
    "\n",
    "dm = DataManager(\n",
    "    nprocs=1,\n",
    "    configs=carmat_config_gen,\n",
    "    prop_defs=[formation_energy_pd],\n",
    "    prop_map=CM_PROPERTY_MAP,\n",
    "    dataset_id=carmat_ds_id,\n",
    ")\n",
    "dm.configs = carmat_reader(Path(\"data/carolina_matdb/base/all/data.mdb\"))\n",
    "\n",
    "match = [\n",
    "    (r\".*3.*\", None, \"3_configurations\", \"Carmat with 3\"),\n",
    "    (r\".*4.*\", None, \"4_configurations\", \"Carmat with 4\"),\n",
    "]\n",
    "# dm.load_co_po_to_vastdb(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase import units\n",
    "from colabfit.tools.property_definitions import *\n",
    "import numpy as np\n",
    "from pyspark.sql import DataFrame\n",
    "import pyspark.sql.functions as sf\n",
    "from pyspark.sql.types import FloatType, ArrayType\n",
    "from ase.units import create_units\n",
    "\n",
    "UNITS = create_units(\"2014\")\n",
    "# (energy/band gap eV, position A, force eV/A, stress/pressure eV/A^3, temperature K, charge e\n",
    "# Make GPa the base unit\n",
    "# UNITS[\"bar\"] = 1e-4  # bar to GPa\n",
    "# UNITS[\"kilobar\"] = 1e-1  # kilobar to GPa\n",
    "# UNITS[\"pascal\"] = 1e-9  # pascal to GPa\n",
    "# UNITS[\"GPa\"] = 1\n",
    "\n",
    "UNITS[\"angstrom\"] = UNITS[\"Ang\"]\n",
    "UNITS[\"bohr\"] = UNITS[\"Bohr\"]\n",
    "UNITS[\"hartree\"] = UNITS[\"Hartree\"]\n",
    "\n",
    "OPENKIM_PROPERTY_UNITS = {\n",
    "    \"energy\": \"eV\",\n",
    "    \"forces\": \"eV/angstrom\",\n",
    "    \"stress\": \"GPa\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNITS = create_units(\"2014\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UNITS.eV / UNITS.angstrom**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "prop_info = namedtuple(\"prop_info\", [\"key\", \"unit\", \"dtype\"])\n",
    "energy_info = prop_info(\"energy\", \"eV\", float)\n",
    "force_info = prop_info(\"forces\", \"eV/angstrom\", list)\n",
    "stress_info = prop_info(\"stress\", \"eV/angstrom^3\", list)\n",
    "main_key_map = {\n",
    "    \"energy-conjugate-with-atomic-forces\": energy_info,\n",
    "    \"atomic-forces\": force_info,\n",
    "    \"cauchy-stress\": stress_info,\n",
    "    \"atomization-energy\": energy_info,\n",
    "    \"formation-energy\": energy_info,\n",
    "    \"band-gap\": energy_info,\n",
    "}\n",
    "\n",
    "prop_name = \"\"\n",
    "val = p.instance[prop_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_defs = [energy_conjugate_pd, atomic_forces_pd, cauchy_stress_pd]\n",
    "\n",
    "for prop_name, val in self.instance.items():\n",
    "    if prop_name in [\"method\", \"software\", \"configuration_id\"]:\n",
    "        continue\n",
    "    p_info = main_key_map[prop_name]\n",
    "    print(p_info)\n",
    "    print(val)\n",
    "    print(val[p_info.key])\n",
    "    units = val[p_info.key][\"source-unit\"]\n",
    "    if units == p_info.unit:\n",
    "        continue\n",
    "    split_units = list(\n",
    "        itertools.chain.from_iterable([sp.split(\"/\") for sp in units.split(\"*\")])\n",
    "    )\n",
    "    print(p_info.key, \" :  \", units)\n",
    "    print(split_units)\n",
    "    if p_info.dtype == list:\n",
    "        prop_val = np.array(val[p_info.key][\"source-value\"], dtype=np.float64)\n",
    "    prop_val *= float(UNITS[split_units[0]])\n",
    "\n",
    "    for u in split_units[1:]:\n",
    "        print(units.find(u) - 1)\n",
    "        if units[units.find(u) - 1] == \"*\":\n",
    "            prop_val *= UNITS[u]\n",
    "        elif units[units.find(u) - 1] == \"/\":\n",
    "            prop_val /= UNITS[u]\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                \"There may be something wrong with the units: {}\".format(u)\n",
    "            )\n",
    "    if p_info.dtype == list:\n",
    "        prop_val = prop_val.tolist()\n",
    "    self.instance[prop_name][p_info.key] = {\n",
    "        \"source-value\": prop_val,\n",
    "        \"source-unit\": p_info.unit,\n",
    "    }\n",
    "    self.property_map[prop_name][p_info.key][\"source-unit\"] = p_info.unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "def convert_units(self):\n",
    "    \"\"\"\n",
    "    For each key in :attr:`self.property_map`, convert :attr:`self.edn[key]`\n",
    "    from its original units to the expected ColabFit-compliant units.\n",
    "    \"\"\"\n",
    "\n",
    "    for key, val in self.spark_row.items():\n",
    "        edn_key = EDN_KEY_MAP.get(key, key)\n",
    "\n",
    "        units = val[\"units\"]\n",
    "\n",
    "        split_units = list(\n",
    "            itertools.chain.from_iterable([sp.split(\"/\") for sp in units.split(\"*\")])\n",
    "        )\n",
    "\n",
    "        if edn_key not in self.instance:\n",
    "            continue\n",
    "\n",
    "        val = np.array(self.instance[edn_key][\"source-value\"], dtype=np.float64)\n",
    "\n",
    "        val *= float(UNITS[split_units[0]])\n",
    "\n",
    "        for u in split_units[1:]:\n",
    "            if units[units.find(u) - 1] == \"*\":\n",
    "                val *= UNITS[u]\n",
    "            elif units[units.find(u) - 1] == \"/\":\n",
    "                val /= UNITS[u]\n",
    "            else:\n",
    "                raise RuntimeError(\n",
    "                    \"There may be something wrong with the units: {}\".format(u)\n",
    "                )\n",
    "\n",
    "        self.instance[edn_key] = {\n",
    "            \"source-value\": val.tolist(),\n",
    "            \"source-unit\": OPENKIM_PROPERTY_UNITS[key],\n",
    "        }\n",
    "\n",
    "        self.property_map[key][\"units\"] = self.instance[edn_key][\"source-unit\"]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gather_co_po_rows(\n",
    "    prop_defs: list[dict],\n",
    "    prop_map: dict,\n",
    "    dataset_id,\n",
    "    configs: list[AtomicConfiguration],\n",
    "):\n",
    "    \"\"\"Convert COs and DOs to Spark rows.\"\"\"\n",
    "    co_po_rows = []\n",
    "    for config in configs:\n",
    "        config.set_dataset_id(dataset_id)\n",
    "        property = Property.from_definition(\n",
    "            definitions=prop_defs,\n",
    "            configuration=config,\n",
    "            property_map=prop_map,\n",
    "            standardize_energy=True,\n",
    "        )\n",
    "        yield property\n",
    "\n",
    "\n",
    "from colabfit.tools.property import Property\n",
    "from colabfit.tools.property_definitions import *\n",
    "\n",
    "\n",
    "propgen = _gather_co_po_rows(dm.prop_defs, dm.prop_map, dm.dataset_id, dm.configs)\n",
    "p = next(propgen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_cols = [f.name for f in dataset_df_schema if f.dataType.typeName() == \"array\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "\n",
    "import colabfit.tools.utilities\n",
    "import colabfit.tools.dataset\n",
    "import colabfit.tools.database\n",
    "import colabfit.tools.configuration\n",
    "import colabfit.tools.configuration_set\n",
    "import colabfit.tools.property\n",
    "import colabfit.tools.schema\n",
    "\n",
    "reload(colabfit.tools.utilities)\n",
    "reload(colabfit.tools.dataset)\n",
    "reload(colabfit.tools.configuration)\n",
    "reload(colabfit.tools.database)\n",
    "reload(colabfit.tools.property)\n",
    "reload(colabfit.tools.schema)\n",
    "AtomicConfiguration = colabfit.tools.configuration.AtomicConfiguration\n",
    "DataManager = colabfit.tools.database.DataManager\n",
    "SparkDataLoader = colabfit.tools.database.SparkDataLoader\n",
    "ConfigurationSet = colabfit.tools.configuration_set.ConfigurationSet\n",
    "Dataset = colabfit.tools.dataset.Dataset\n",
    "Property = colabfit.tools.property.Property\n",
    "dataset_df_schema = colabfit.tools.schema.dataset_df_schema\n",
    "dataset_schema = colabfit.tools.schema.dataset_schema\n",
    "\n",
    "#################################################\n",
    "\n",
    "batches = dm.gather_co_po_in_batches()\n",
    "batch = next(batches)\n",
    "cos, pos = zip(*batch)\n",
    "from colabfit.tools.schema import *\n",
    "import pyspark.sql.functions as sf\n",
    "\n",
    "podf = spark.createDataFrame(pos, schema=property_object_df_schema).limit(100)\n",
    "from colabfit.tools.utilities import stringify_df_val\n",
    "\n",
    "schema = property_object_df_schema\n",
    "string_cols = [f.name for f in schema if f.dataType.typeName() == \"array\"]\n",
    "string_col_udf = sf.udf(stringify_df_val, StringType())\n",
    "for col in string_cols:\n",
    "    podf = podf.withColumn(col, string_col_udf(sf.col(col)))\n",
    "podf2 = split_long_string(podf)\n",
    "\n",
    "codf = spark.createDataFrame(cos, schema=config_df_schema)\n",
    "combindf = codf.withColumnRenamed(\"id\", \"configuration_id\").join(\n",
    "    podf, on=\"configuration_id\", how=\"inner\"\n",
    ")\n",
    "combindf.select(\"id\", \"configuration_id\", \"nsites\", \"atomic_forces_00\").limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colabfit.tools.schema import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'string'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_schema[0].dataType.typeName()\n",
    "sorted([(field.name, field.typeName) for field in df.schema]) == sorted(\n",
    "    [(field.name, field.typeName) for field in config_schema]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3863094619420121725777758568264852161557642315873240643857946951802225712257921945039652182889650012603442118527748985013505998175398661746010802427376410"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hashlib import sha512\n",
    "\n",
    "\n",
    "def __hash__(self):\n",
    "    sha = sha512()\n",
    "    sha.update(self.encode(\"utf-8\"))\n",
    "    return int(sha.hexdigest(), 16)\n",
    "\n",
    "\n",
    "__hash__(\"DS_28dfhgfa_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as sf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "NSITES_COL_SPLITS = 20\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import substring, length, col, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colabfit.tools.utilities import get_spark_field_type\n",
    "from colabfit.tools.schema import *\n",
    "\n",
    "col_types = {}\n",
    "cols = [\n",
    "    \"last_modified\",\n",
    "    \"dataset_ids\",\n",
    "    \"id\",\n",
    "]\n",
    "arr_cols = []\n",
    "for col in cols:\n",
    "    col_types[col] = get_spark_field_type(config_schema, col)\n",
    "    is_arr = get_spark_field_type(config_df_schema, col)\n",
    "    print(is_arr.typeName())\n",
    "    if is_arr.typeName == \"array\":\n",
    "        arr_cols.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_long_string(\n",
    "#     df, column_name: str = \"atomic_forces_00\", max_string_length: int = 100\n",
    "# ):\n",
    "\"\"\"\n",
    "Splits a long string column into multiple columns based on a maximum string length.\n",
    ":param df: Input DataFrame\n",
    ":param column_name: Name of the column containing the long string\n",
    ":param max_string_length: Maximum length for each split string\n",
    ":return: DataFrame with the long string split across multiple columns\n",
    "\"\"\"\n",
    "overflow_columns = [\n",
    "    f\"{'_'.join(column_name.split('_')[:-1])}_{i+1:02}\" for i in range(19)\n",
    "]\n",
    "all_columns = [column_name] + overflow_columns\n",
    "tmp_columns = [f\"{col_name}_tmp\" for col_name in all_columns]\n",
    "df = df.withColumn(\"total_length\", sf.length(sf.col(column_name)))\n",
    "substring_exprs = [\n",
    "    sf.when(\n",
    "        sf.length(sf.col(column_name)) - (i * max_string_length) > 0,\n",
    "        sf.substring(\n",
    "            sf.col(column_name), (i * max_string_length + 1), max_string_length\n",
    "        ),\n",
    "    )\n",
    "    .otherwise(sf.lit(None))\n",
    "    .alias(col_name)\n",
    "    for i, col_name in enumerate(tmp_columns)\n",
    "]\n",
    "\n",
    "substring_exprs = [\n",
    "    sf.substring(\n",
    "        df[column_name],\n",
    "        (i * max_string_length + 1),\n",
    "        max_string_length,\n",
    "    ).alias(col_name)\n",
    "    for i, col_name in enumerate(tmp_columns)\n",
    "]\n",
    "df1 = df.select(\"*\", *substring_exprs)\n",
    "for tmp_col, col in zip(tmp_columns, all_columns):\n",
    "    df1 = df1.drop(col).withColumnRenamed(f\"{tmp_col}\", col)\n",
    "df = df.drop(\"total_length\")\n",
    "# return df\n",
    "\n",
    "\n",
    "# Make this to replace the columns, not just add duplicate names\n",
    "def split_long_string(df, col_name, thresh):\n",
    "    columns = [sf.col(c) for c in df.columns]\n",
    "    num_splits = NSITES_COL_SPLITS\n",
    "    split_exprs = [\n",
    "        sf.when(\n",
    "            sf.col(col_name).substr(i * thresh + 1, thresh) != \"\",\n",
    "            sf.col(col_name).substr(i * thresh + 1, thresh),\n",
    "        ).otherwise(sf.lit(\"\"))\n",
    "        for i in range(num_splits)\n",
    "    ]\n",
    "    for i, expr in enumerate(split_exprs):\n",
    "        columns.append(expr.alias(f\"{'_'.join(col_name.split('_')[:-1])}_{i:02}\"))\n",
    "    return df.select(columns)\n",
    "\n",
    "\n",
    "df1 = split_long_string(podf, \"atomic_forces_00\", 500 // 19)\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder.appName(\"split_long_string\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (1, \"a\" * 150),  # Long string\n",
    "    (2, \"short\"),  # Short string\n",
    "    (3, \"b\" * 250),  # Longer string\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"id\", \"stringcol_00\"])\n",
    "\n",
    "# Apply the split_long_string function\n",
    "result_df = split_long_string(df, \"stringcol_00\", 100, 3)\n",
    "\n",
    "# Show the result\n",
    "result_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colabfit.tools.schema import *\n",
    "\n",
    "batches = dm.gather_co_po_in_batches()\n",
    "batch = next(batches)\n",
    "cos, pos = zip(*batch)\n",
    "co_df = spark.createDataFrame(cos, schema=config_df_schema)\n",
    "po_df = spark.createDataFrame(pos, schema=property_object_df_schema)\n",
    "ids = [x[\"id\"] for x in po_df.select(\"id\").collect()]\n",
    "# new, old = loader.find_existing_po_rows_append_elem(ids, podf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colabfit.tools.database import batched\n",
    "from colabfit.tools.utilities import *\n",
    "from colabfit.tools.schema import *\n",
    "from colabfit.tools.database import *\n",
    "from colabfit.tools.configuration import *\n",
    "\n",
    "import pyarrow as pa\n",
    "\n",
    "self = loader\n",
    "import dateutil.parser\n",
    "import datetime\n",
    "import pyspark.sql.functions as sf\n",
    "\n",
    "\n",
    "cols = [\"dataset_ids\"]\n",
    "elems = [\"DS_y7nrdsjtuw0g_1\"]\n",
    "\n",
    "\n",
    "if isinstance(cols, str):\n",
    "    cols = [cols]\n",
    "\n",
    "\n",
    "if isinstance(elems, str):\n",
    "    elems = [elems]\n",
    "\n",
    "\n",
    "col_types = {\n",
    "    \"id\": StringType(),\n",
    "    \"last_modified\": TimestampType(),\n",
    "    \"$row_id\": IntegerType(),\n",
    "}\n",
    "\n",
    "\n",
    "arr_cols = []\n",
    "for col in cols:\n",
    "    col_types[col] = get_spark_field_type(config_schema, col)\n",
    "    is_arr = get_spark_field_type(config_df_schema, col)\n",
    "    if is_arr.typeName() == \"array\":\n",
    "        arr_cols.append(col)\n",
    "\n",
    "\n",
    "update_cols = [col for col in col_types if col not in [\"id\", \"$row_id\"]]\n",
    "\n",
    "\n",
    "total_write_cols = update_cols + [\"$row_id\"]\n",
    "ids = [x[\"id\"] for x in co_df.select(\"id\").collect()]\n",
    "batched_ids = batched(ids, 10000)\n",
    "new_ids = []\n",
    "existing_ids = []\n",
    "\n",
    "\n",
    "id_batch = next(batched_ids)\n",
    "\n",
    "\n",
    "id_batch = list(set(id_batch))\n",
    "\n",
    "\n",
    "# We only have to use vastdb-sdk here bc we need the '$row_id' column\n",
    "with self.session.transaction() as tx:\n",
    "    table_path = self.config_table.split(\".\")\n",
    "    table = tx.bucket(table_path[1]).schema(table_path[2]).table(table_path[3])\n",
    "    rec_batch = table.select(\n",
    "        predicate=table[\"id\"].isin(id_batch),\n",
    "        columns=update_cols + [\"id\"],\n",
    "        internal_row_id=True,\n",
    "    )\n",
    "    spark_schema = StructType(\n",
    "        [StructField(col, col_types[col], True) for i, col in enumerate(update_cols)]\n",
    "        + [\n",
    "            StructField(\"id\", StringType(), False),\n",
    "            StructField(\"$row_id\", IntegerType(), False),\n",
    "        ]\n",
    "    )\n",
    "    rec_batch = rec_batch.read_all()\n",
    "    duplicate_df = self.spark.createDataFrame(\n",
    "        rec_batch.to_pylist(), schema=spark_schema\n",
    "    )\n",
    "    print(f\"length of df: {duplicate_df.count()}\")\n",
    "\n",
    "\n",
    "unstring_udf = sf.udf(unstring_df_val, ArrayType(StringType()))\n",
    "\n",
    "\n",
    "for col_name, col_type in col_types.items():\n",
    "    if col_name in arr_cols:\n",
    "        duplicate_df = duplicate_df.withColumn(col_name, unstring_udf(sf.col(col_name)))\n",
    "\n",
    "\n",
    "for col, elem in zip(cols, elems):\n",
    "    if col == \"labels\":\n",
    "        co_df_labels = co_df.select(\"id\", \"labels\").collect()\n",
    "        duplicate_df.withColumnRenamed(\"labels\", \"labels_dup\").join(\n",
    "            co_df_labels.withColumnRenamed(\"labels\", \"labels_co_df\"),\n",
    "            on=\"id\",\n",
    "        ).withColumn(\n",
    "            \"labels\",\n",
    "            sf.array_distinct(sf.array_union(\"labels_dup\", \"labels_co_df\")),\n",
    "        )\n",
    "    else:\n",
    "        duplicate_df = duplicate_df.withColumn(\n",
    "            col,\n",
    "            sf.array_distinct(sf.array_union(sf.col(col), sf.array(sf.lit(elem)))),\n",
    "        )\n",
    "\n",
    "\n",
    "existing_ids_batch = [x[\"id\"] for x in duplicate_df.select(\"id\").collect()]\n",
    "\n",
    "new_ids_batch = [id for id in id_batch if id not in existing_ids_batch]\n",
    "string_udf = sf.udf(stringify_df_val, StringType())\n",
    "print(arr_cols)\n",
    "\n",
    "\n",
    "for col_name in duplicate_df.columns:\n",
    "    print(\"stringifying column: \", col_name)\n",
    "    if col_name in arr_cols:\n",
    "        duplicate_df = duplicate_df.withColumn(col_name, string_udf(sf.col(col_name)))\n",
    "\n",
    "\n",
    "update_time = dateutil.parser.parse(\n",
    "    datetime.datetime.now(tz=datetime.timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    ")\n",
    "\n",
    "duplicate_df = duplicate_df.withColumn(\n",
    "    \"last_modified\", sf.lit(update_time).cast(\"timestamp\")\n",
    ")\n",
    "\n",
    "update_schema = StructType(\n",
    "    [StructField(col, col_types[col], False) for col in total_write_cols]\n",
    ")\n",
    "\n",
    "arrow_schema = spark_schema_to_arrow_schema(update_schema)\n",
    "update_table = pa.table(\n",
    "    [pa.array(col) for col in zip(*duplicate_df.select(total_write_cols).collect())],\n",
    "    schema=arrow_schema,\n",
    ")\n",
    "\n",
    "with self.session.transaction() as tx:\n",
    "    table = tx.bucket(table_path[1]).schema(table_path[2]).table(table_path[3])\n",
    "    table.update(\n",
    "        rows=update_table,\n",
    "        columns=update_cols,\n",
    "    )\n",
    "new_ids.extend(new_ids_batch)\n",
    "existing_ids.extend(existing_ids_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_DIR = \"/vast/gpwolfe/METADATA\"\n",
    "\n",
    "\n",
    "def _sort_dict(dictionary):\n",
    "    keys = list(dictionary.keys())\n",
    "    keys.sort()\n",
    "    return {k: dictionary[k] for k in keys}\n",
    "\n",
    "\n",
    "def _parse_unstructured_metadata(md_json):\n",
    "    md = {}\n",
    "    for key, val in md_json.items():\n",
    "        if key in [\"_id\", \"hash\", \"colabfit-id\", \"last_modified\", \"software\", \"method\"]:\n",
    "            continue\n",
    "        if \"source-value\" in val.keys():\n",
    "            source_value = val[\"source-value\"]\n",
    "        else:\n",
    "            source_value = val\n",
    "        if isinstance(source_value, list) and len(source_value) == 1:\n",
    "            source_value = source_value[0]\n",
    "        if isinstance(source_value, dict):\n",
    "            source_value = _sort_dict(source_value)\n",
    "        if isinstance(source_value, bytes):\n",
    "            source_value = source_value.decode(\"utf-8\")\n",
    "        md[key] = source_value\n",
    "    md = _sort_dict(md)\n",
    "    md_hash = _hash(md, md.keys(), include_keys_in_hash=True)\n",
    "    md[\"hash\"] = md_hash\n",
    "    md[\"id\"] = f\"MD_{md_hash[:25]}\"\n",
    "    split = md[\"id\"][-4:]\n",
    "    filename = f\"{md['id']}.json\"\n",
    "    full_path = os.path.join(BUCKET_DIR, \"MD\", split, filename)\n",
    "    if not os.path.isfile(full_path):\n",
    "        # Write iff the ID is new and unique\n",
    "        os.makedirs(os.path.dirname(full_path), exist_ok=True)\n",
    "        with open(full_path, \"w\") as f:\n",
    "            json.dump(md, f)\n",
    "    return {\n",
    "        \"metadata_id\": md[\"id\"],\n",
    "        \"metadata_path\": full_path,\n",
    "        \"metadata_size\": sys.getsizeof(json.dumps(md)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show tables in ndb.colabfit.dev\").show()\n",
    "spark.sql(\"drop table ndb.colabfit.dev.gpw_test_configs\")\n",
    "spark.sql(\"drop table ndb.colabfit.dev.gpw_test_propobjects\")\n",
    "spark.sql(\"drop table ndb.colabfit.dev.gpw_test_config_sets\")\n",
    "spark.sql(\"drop table ndb.colabfit.dev.gpw_test_datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show tables in ndb.colabfit.dev\").show()\n",
    "spark.sql(\"drop table ndb.colabfit.dev.sample_configs\")\n",
    "spark.sql(\"drop table ndb.colabfit.dev.sample_prop_objects\")\n",
    "spark.sql(\"drop table ndb.colabfit.dev.sample_config_sets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colabfit.tools.utilities import *\n",
    "from colabfit.tools.database import *\n",
    "from colabfit.tools.schema import *\n",
    "from functools import partial\n",
    "\n",
    "self = loader\n",
    "batches = dm.gather_co_po_in_batches()\n",
    "batch = next(batches)\n",
    "\n",
    "co_rows, po_rows = list(zip(*batch))\n",
    "co_rdd = loader.spark.sparkContext.parallelize(co_rows)\n",
    "po_rdd = loader.spark.sparkContext.parallelize(po_rows)\n",
    "co_ids = co_rdd.map(lambda x: x[\"id\"]).collect()\n",
    "co_rdd2 = (\n",
    "    co_rdd.map(lambda x: (x[\"id\"], x)).reduceByKey(lambda a, b: a).map(lambda x: x[1])\n",
    ")\n",
    "\n",
    "\n",
    "po_ids = po_rdd.map(lambda x: x[\"id\"]).collect()\n",
    "if len(set(po_ids)) < len(po_ids):\n",
    "    print(f\"{len(po_ids) - len(set(po_ids))} duplicates found in PO RDD\")\n",
    "    po_rdd = loader.reduce_po_rdd(po_rdd)\n",
    "co_ids = set(co_ids)\n",
    "all_unique_co = loader.check_unique_ids(loader.config_table, co_ids)\n",
    "all_unique_po = loader.check_unique_ids(loader.prop_object_table, po_ids)\n",
    "\n",
    "new_po_ids, update_po_ids = loader.find_existing_po_rows_append_elem(\n",
    "    po_rdd=po_rdd,\n",
    "    ids=po_ids,\n",
    ")\n",
    "loader.write_table(\n",
    "    po_rdd,\n",
    "    loader.prop_object_table,\n",
    "    property_object_schema,\n",
    "    ids_filter=new_po_ids,\n",
    ")\n",
    "\n",
    "cos = list(cos)\n",
    "cos.extend([cos[0], cos[1]])\n",
    "co_rdd = sc.parallelize(cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "idc = Counter(ids)\n",
    "ids = prop_df.select(\"configuration_id\").collect()\n",
    "ids = [x[\"configuration_id\"] for x in ids]\n",
    "\n",
    "\n",
    "ids = [x[\"configuration_id\"] for x in ids]\n",
    "co_po_df.filter(sf.col(\"configuration_id\") == \"CO_1077499488010994550\").select(\n",
    "    \"potential_energy\", \"potential_energy_reference\", \"atomic_forces\", \"cauchy_stress\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def generate_random_float_array(n):\n",
    "    # Generate an n*3 array with random floats\n",
    "    random_array = np.random.rand(n, 3)\n",
    "    return random_array\n",
    "\n",
    "\n",
    "def split_string(s, max_length=60000):\n",
    "    if s is None:\n",
    "        return [None]\n",
    "    return [s[i : i + max_length] for i in range(0, len(s), max_length)]\n",
    "\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "x = generate_random_float_array(10000)\n",
    "xstr = \"\".join(np.array2string(np.arr(x), separator=\",\").replace(\"\\n\", \"\"))\n",
    "splx = split_string(xstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colabfit.tools.utilities import _write_value\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_write = partial(\n",
    "    _write_value,\n",
    "    \"CO/positions\",\n",
    "    \"txt\",\n",
    "    \"/scratch/gw2338/vast/data-lake-main/spark/scripts\",\n",
    "    \"positions\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "\n",
    "import colabfit.tools.dataset\n",
    "import colabfit.tools.database\n",
    "import colabfit.tools.configuration_set\n",
    "import colabfit.tools.schema\n",
    "\n",
    "reload(colabfit.tools.configuration_set)\n",
    "reload(colabfit.tools.dataset)\n",
    "reload(colabfit.tools.database)\n",
    "reload(colabfit.tools.schema)\n",
    "configuration_set_schema = colabfit.tools.schema.configuration_set_schema\n",
    "DataManager = colabfit.tools.database.DataManager\n",
    "ConfigurationSet = colabfit.tools.configuration_set.ConfigurationSet\n",
    "Dataset = colabfit.tools.dataset.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase import units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cf-dev-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
